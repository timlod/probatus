{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to probatus documentation! \u00b6 Probatus is a Python library that allows to analyse binary classification models as well as the data used to develop them. The main features assess the metric stability and analyse differences between two data samples e.g. shift between train and test splits. Installation \u00b6 In order to install Probatus you need to use Python 3.6 or higher. Install probatus via pip with: pip install probatus Alternatively you can fork/clone and run: git clone https://gitlab.com/ing_rpaa/probatus.git cd probatus pip install . Licence \u00b6 Probatus is created under MIT License, see more in LICENCE file .","title":"Home"},{"location":"index.html#welcome-to-probatus-documentation","text":"Probatus is a Python library that allows to analyse binary classification models as well as the data used to develop them. The main features assess the metric stability and analyse differences between two data samples e.g. shift between train and test splits.","title":"Welcome to probatus documentation!"},{"location":"index.html#installation","text":"In order to install Probatus you need to use Python 3.6 or higher. Install probatus via pip with: pip install probatus Alternatively you can fork/clone and run: git clone https://gitlab.com/ing_rpaa/probatus.git cd probatus pip install .","title":"Installation"},{"location":"index.html#licence","text":"Probatus is created under MIT License, see more in LICENCE file .","title":"Licence"},{"location":"api/feature_elimination.html","text":"Features Elimination \u00b6 This module focuses on feature elimination and it contains two classes: ShapRFECV : Perform Backwards Recursive Feature Elimination, using SHAP feature importance. It supports binary classification models and hyperparameter optimization at every feature elimination step. EarlyStoppingShapRFECV : adds support to early stopping of the model fitting process. It can be an alternative regularization technique to hyperparameter optimization of the number of base trees in gradient boosted tree models. Particularly useful when dealing with large datasets. EarlyStoppingShapRFECV \u00b6 This class performs Backwards Recursive Feature Elimination, using SHAP feature importance. This is a child of ShapRFECV which allows early stopping of the training step, available in models such as XGBoost and LightGBM. If you are not using early stopping, you should use the parent class, ShapRFECV, instead of EarlyStoppingShapRFECV. Early stopping is a type of regularization technique in which the model is trained until the scoring metric, measured on a validation set, stops improving after a number of early_stopping_rounds. In boosted tree models, this technique can increase the training speed, by skipping the training of trees that do not improve the scoring metric any further, which is particularly useful when the training dataset is large. Note that if the classifier is a hyperparameter search model is used, the early stopping parameter is passed only to the fit method of the model duiring the Shapley values estimation step, and not for the hyperparameter search step. Early stopping can be seen as a type of regularization of the optimal number of trees. Therefore you can use it directly with a LightGBM or XGBoost model, as an alternative to a hyperparameter search model. At each round, for a given feature set, starting from all available features, the following steps are applied: (Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g. GridSearchCV , RandomizedSearchCV , or BayesSearchCV . Note that during this step the model does not use early stopping. Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV iteration, the model is fitted on the train folds, and applied on the validation fold to estimate SHAP feature importance. The model is trained until the scoring metric eval_metric, measured on the validation fold, stops improving after a number of early_stopping_rounds. Remove step lowest SHAP importance features from the dataset. At the end of the process, the user can plot the performance of the model for each iteration, and select the optimal number of features and the features set. We recommend using LGBMClassifier , because by default it handles missing values and categorical features. In case of other models, make sure to handle these issues for your dataset and consider impact it might have on features importance. Examples: from lightgbm import LGBMClassifier import pandas as pd from probatus.feature_elimination import EarlyStoppingShapRFECV from sklearn.datasets import make_classification feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 200 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) # Prepare model clf = LGBMClassifier ( n_estimators = 200 , max_depth = 3 ) # Run feature elimination shap_elimination = EarlyStoppingShapRFECV ( clf = clf , step = 0.2 , cv = 10 , scoring = 'roc_auc' , early_stopping_rounds = 10 , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) # Make plots performance_plot = shap_elimination . plot () # Get final feature set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 3 ) __init__ ( self , clf , step = 1 , min_features_to_select = 1 , cv = None , scoring = 'roc_auc' , n_jobs =- 1 , verbose = 0 , random_state = None , early_stopping_rounds = 5 , eval_metric = 'auc' ) special \u00b6 This method initializes the class. Parameters: Name Type Description Default clf binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV A model that will be optimized and trained at each round of features elimination. The model must support early stopping of training, which is the case for XGBoost and LightGBM, for example. The recommended model is LGBMClassifier , because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV . Note that if a hyperparemeter search model is used, the hyperparameters are tuned without early stopping. Early stopping is applied only during the Shapley values estimation for feature elimination. We recommend simply passing the model without hyperparameter optimization, or using ShapRFECV without early stopping. required step int or float Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. 1 min_features_to_select int Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. 1 cv int, cross-validation generator or an iterable Determines the cross-validation splitting strategy. Compatible with sklearn cv parameter . If None, then cv of 5 is used. None scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' n_jobs int Number of cores to run in parallel while fitting across folds. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. -1 verbose int Controls verbosity of the output: 0 - nether prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None early_stopping_rounds int Number of rounds with constant performance after which the model fitting stops. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as XGBoost and LightGBM. 5 eval_metric str Metric for scoring fitting rounds and activating early stopping. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as XGBoost and LightGBM . Note that eval_metric is an argument of the model's fit method and it is different from scoring . 'auc' Source code in probatus/feature_elimination/feature_elimination.py def __init__ ( self , clf , step = 1 , min_features_to_select = 1 , cv = None , scoring = \"roc_auc\" , n_jobs =- 1 , verbose = 0 , random_state = None , early_stopping_rounds = 5 , eval_metric = \"auc\" , ): \"\"\" This method initializes the class. Args: clf (binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV): A model that will be optimized and trained at each round of features elimination. The model must support early stopping of training, which is the case for XGBoost and LightGBM, for example. The recommended model is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html), because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV). Note that if a hyperparemeter search model is used, the hyperparameters are tuned without early stopping. Early stopping is applied only during the Shapley values estimation for feature elimination. We recommend simply passing the model without hyperparameter optimization, or using ShapRFECV without early stopping. step (int or float, optional): Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. min_features_to_select (int, optional): Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. cv (int, cross-validation generator or an iterable, optional): Determines the cross-validation splitting strategy. Compatible with sklearn [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html). If None, then cv of 5 is used. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. n_jobs (int, optional): Number of cores to run in parallel while fitting across folds. None means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. verbose (int, optional): Controls verbosity of the output: - 0 - nether prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. early_stopping_rounds (int, optional): Number of rounds with constant performance after which the model fitting stops. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as XGBoost and LightGBM. eval_metric (str, optional): Metric for scoring fitting rounds and activating early stopping. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as [XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters) and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters). Note that `eval_metric` is an argument of the model's fit method and it is different from `scoring`. \"\"\" # noqa super ( EarlyStoppingShapRFECV , self ) . __init__ ( clf , step = step , min_features_to_select = min_features_to_select , cv = cv , scoring = scoring , n_jobs = n_jobs , verbose = verbose , random_state = random_state , ) if self . search_clf : if self . verbose > 0 : warnings . warn ( \"Early stopping will be used only during Shapley value estimation step, and not for hyperparameter\" \"optimization.\" ) if isinstance ( early_stopping_rounds , int ) and early_stopping_rounds > 0 : self . early_stopping_rounds = early_stopping_rounds else : raise ( ValueError ( f \"The current value of early_stopping_rounds = { early_stopping_rounds } is not allowed. \" f \"It needs to be a positive integer.\" ) ) self . eval_metric = eval_metric compute ( self ) inherited \u00b6 Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: Type Description (pd.DataFrame) DataFrame with results of feature elimination for each round. Source code in probatus/feature_elimination/feature_elimination.py def compute ( self ): \"\"\" Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: (pd.DataFrame): DataFrame with results of feature elimination for each round. \"\"\" self . _check_if_fitted () return self . report_df fit ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ) inherited \u00b6 Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (ShapRFECV) Fitted object. Source code in probatus/feature_elimination/feature_elimination.py def fit ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (ShapRFECV): Fitted object. \"\"\" # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # If to columns_to_keep is not provided, then initialise it by an empty string. # If provided check if all the elements in columns_to_keep are of type string. if columns_to_keep is None : len_columns_to_keep = 0 else : if all ( isinstance ( x , str ) for x in columns_to_keep ): len_columns_to_keep = len ( columns_to_keep ) else : raise ( ValueError ( \"The current values of columns_to_keep are not allowed.All the elements should be strings.\" ) ) # If the columns_to_keep parameter is provided, check if they match the column names in the X. if column_names is not None : if all ( x in column_names for x in list ( X . columns )): pass else : raise ( ValueError ( \"The column names in parameter columns_to_keep and column_names are not macthing.\" )) # Check that the total number of columns to select is less than total number of columns in the data. # only when both parameters are provided. if column_names is not None and columns_to_keep is not None : if ( self . min_features_to_select + len_columns_to_keep ) > len ( self . column_names ): raise ValueError ( \"Minimum features to select is greater than number of features.\" \"Lower the value for min_features_to_select or number of columns in columns_to_keep\" ) self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) self . cv = check_cv ( self . cv , self . y , classifier = is_classifier ( self . clf )) remaining_features = current_features_set = self . column_names round_number = 0 # Stop when stopping criteria is met. stopping_criteria = np . max ([ self . min_features_to_select , len_columns_to_keep ]) # Setting up the min_features_to_select parameter. if columns_to_keep is None : pass else : self . min_features_to_select = 0 # This ensures that, if columns_to_keep is provided , # the last features remaining are only the columns_to_keep. if self . verbose > 50 : warnings . warn ( f \"Minimum features to select : { stopping_criteria } \" ) while len ( current_features_set ) > stopping_criteria : round_number += 1 # Get current dataset info current_features_set = remaining_features if columns_to_keep is None : remaining_removeable_features = list ( set ( current_features_set )) else : remaining_removeable_features = list ( set ( current_features_set ) | set ( columns_to_keep )) current_X = self . X [ remaining_removeable_features ] # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # Optimize parameters if self . search_clf : current_search_clf = clone ( self . clf ) . fit ( current_X , self . y ) current_clf = current_search_clf . estimator . set_params ( ** current_search_clf . best_params_ ) else : current_clf = clone ( self . clf ) # Perform CV to estimate feature importance with SHAP results_per_fold = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _get_feature_shap_values_per_fold )( X = current_X , y = self . y , clf = current_clf , train_index = train_index , val_index = val_index , ** shap_kwargs , ) for train_index , val_index in self . cv . split ( current_X , self . y ) ) shap_values = np . vstack ([ current_result [ 0 ] for current_result in results_per_fold ]) scores_train = [ current_result [ 1 ] for current_result in results_per_fold ] scores_val = [ current_result [ 2 ] for current_result in results_per_fold ] # Calculate the shap features with remaining features and features to keep. shap_importance_df = calculate_shap_importance ( shap_values , remaining_removeable_features ) # Get features to remove features_to_remove = self . _get_current_features_to_remove ( shap_importance_df , columns_to_keep = columns_to_keep ) remaining_features = list ( set ( current_features_set ) - set ( features_to_remove )) # Report results self . _report_current_results ( round_number = round_number , current_features_set = current_features_set , features_to_remove = features_to_remove , train_metric_mean = np . round ( np . mean ( scores_train ), 3 ), train_metric_std = np . round ( np . std ( scores_train ), 3 ), val_metric_mean = np . round ( np . mean ( scores_val ), 3 ), val_metric_std = np . round ( np . std ( scores_val ), 3 ), ) if self . verbose > 50 : print ( f \"Round: { round_number } , Current number of features: { len ( current_features_set ) } , \" f 'Current performance: Train { self . report_df . loc [ round_number ][ \"train_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"train_metric_std\" ] } , CV Validation ' f ' { self . report_df . loc [ round_number ][ \"val_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"val_metric_std\" ] } . \\n ' f \"Features left: { remaining_features } . \" f \"Removed features at the end of the round: { features_to_remove } \" ) self . fitted = True return self fit_compute ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ) inherited \u00b6 Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of columns to keep. If given, these columns will not be eliminated. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame) DataFrame containing results of feature elimination from each iteration. Source code in probatus/feature_elimination/feature_elimination.py def fit_compute ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of columns to keep. If given, these columns will not be eliminated. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame): DataFrame containing results of feature elimination from each iteration. \"\"\" self . fit ( X , y , columns_to_keep = columns_to_keep , column_names = column_names , ** shap_kwargs ) return self . compute () get_reduced_features_set ( self , num_features ) inherited \u00b6 Gets the features set after the feature elimination process, for a given number of features. Parameters: Name Type Description Default num_features int Number of features in the reduced features set. required Returns: Type Description (list of str) Reduced features set. Source code in probatus/feature_elimination/feature_elimination.py def get_reduced_features_set ( self , num_features ): \"\"\" Gets the features set after the feature elimination process, for a given number of features. Args: num_features (int): Number of features in the reduced features set. Returns: (list of str): Reduced features set. \"\"\" self . _check_if_fitted () if num_features not in self . report_df . num_features . tolist (): raise ( ValueError ( f \"The provided number of features has not been achieved at any stage of the process. \" f \"You can select one of the following: { self . report_df . num_features . tolist () } \" ) ) else : return self . report_df [ self . report_df . num_features == num_features ][ \"features_set\" ] . values [ 0 ] plot ( self , show = True , ** figure_kwargs ) inherited \u00b6 Generates plot of the model performance for each iteration of feature elimination. Parameters: Name Type Description Default show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description (plt.axis) Axis containing the performance plot. Source code in probatus/feature_elimination/feature_elimination.py def plot ( self , show = True , ** figure_kwargs ): \"\"\" Generates plot of the model performance for each iteration of feature elimination. Args: show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: (plt.axis): Axis containing the performance plot. \"\"\" x_ticks = list ( reversed ( self . report_df [ \"num_features\" ] . tolist ())) plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"train_metric_mean\" ], label = \"Train Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"train_metric_mean\" ] - self . report_df [ \"train_metric_std\" ], self . report_df [ \"train_metric_mean\" ] + self . report_df [ \"train_metric_std\" ], alpha = 0.3 , ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"val_metric_mean\" ], label = \"Validation Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"val_metric_mean\" ] - self . report_df [ \"val_metric_std\" ], self . report_df [ \"val_metric_mean\" ] + self . report_df [ \"val_metric_std\" ], alpha = 0.3 , ) plt . xlabel ( \"Number of features\" ) plt . ylabel ( f \"Performance { self . scorer . metric_name } \" ) plt . title ( \"Backwards Feature Elimination using SHAP & CV\" ) plt . legend ( loc = \"lower left\" ) ax = plt . gca () ax . invert_xaxis () ax . set_xticks ( x_ticks ) if show : plt . show () else : plt . close () return ax ShapRFECV \u00b6 This class performs Backwards Recursive Feature Elimination, using SHAP feature importance. At each round, for a given feature set, starting from all available features, the following steps are applied: (Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g. GridSearchCV , RandomizedSearchCV , or BayesSearchCV , Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV iteration, the model is fitted on the train folds, and applied on the validation fold to estimate SHAP feature importance. Remove step lowest SHAP importance features from the dataset. At the end of the process, the user can plot the performance of the model for each iteration, and select the optimal number of features and the features set. The functionality is similar to RFECV . The main difference is removing the lowest importance features based on SHAP features importance. It also supports the use of sklearn compatible search CV for hyperparameter optimization e.g. GridSearchCV , RandomizedSearchCV , or BayesSearchCV , which needs to be passed as the clf . Thanks to this you can perform hyperparameter optimization at each step of the feature elimination. Lastly, it supports categorical features (object and category dtype) and missing values in the data, as long as the model supports them. We recommend using LGBMClassifier , because by default it handles missing values and categorical features. In case of other models, make sure to handle these issues for your dataset and consider impact it might have on features importance. Examples: import numpy as np import pandas as pd from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import RandomizedSearchCV feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 200 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) # Prepare model and parameter search space clf = RandomForestClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'min_samples_leaf' : [ 3 , 5 , 7 , 10 ], } search = RandomizedSearchCV ( clf , param_grid ) # Run feature elimination shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) # Make plots performance_plot = shap_elimination . plot () # Get final feature set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 3 ) __init__ ( self , clf , step = 1 , min_features_to_select = 1 , cv = None , scoring = 'roc_auc' , n_jobs =- 1 , verbose = 0 , random_state = None ) special \u00b6 This method initializes the class. Parameters: Name Type Description Default clf binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV A model that will be optimized and trained at each round of feature elimination. The recommended model is LGBMClassifier , because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV . required step int or float Number of lowest importance features removed each round. If it is an int, then each round such a number of features are discarded. If float, such a percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise with fewer features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. 1 min_features_to_select int Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. 1 cv int, cross-validation generator or an iterable Determines the cross-validation splitting strategy. Compatible with sklearn cv parameter . If None, then cv of 5 is used. None scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' n_jobs int Number of cores to run in parallel while fitting across folds. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. -1 verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to an integer. None Source code in probatus/feature_elimination/feature_elimination.py def __init__ ( self , clf , step = 1 , min_features_to_select = 1 , cv = None , scoring = \"roc_auc\" , n_jobs =- 1 , verbose = 0 , random_state = None , ): \"\"\" This method initializes the class. Args: clf (binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV): A model that will be optimized and trained at each round of feature elimination. The recommended model is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html), because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV). step (int or float, optional): Number of lowest importance features removed each round. If it is an int, then each round such a number of features are discarded. If float, such a percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise with fewer features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. min_features_to_select (int, optional): Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. cv (int, cross-validation generator or an iterable, optional): Determines the cross-validation splitting strategy. Compatible with sklearn [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html). If None, then cv of 5 is used. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. n_jobs (int, optional): Number of cores to run in parallel while fitting across folds. None means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to an integer. \"\"\" # noqa self . clf = clf if isinstance ( self . clf , BaseSearchCV ): self . search_clf = True else : self . search_clf = False if ( isinstance ( step , int ) or isinstance ( step , float )) and step > 0 : self . step = step else : raise ( ValueError ( f \"The current value of step = { step } is not allowed. \" f \"It needs to be a positive integer or positive float.\" ) ) if isinstance ( min_features_to_select , int ) and min_features_to_select > 0 : self . min_features_to_select = min_features_to_select else : raise ( ValueError ( f \"The current value of min_features_to_select = { min_features_to_select } is not allowed. \" f \"It needs to be a greater than or equal to 0.\" ) ) self . cv = cv self . scorer = get_single_scorer ( scoring ) self . random_state = random_state self . n_jobs = n_jobs self . report_df = pd . DataFrame ([]) self . verbose = verbose compute ( self ) \u00b6 Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: Type Description (pd.DataFrame) DataFrame with results of feature elimination for each round. Source code in probatus/feature_elimination/feature_elimination.py def compute ( self ): \"\"\" Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: (pd.DataFrame): DataFrame with results of feature elimination for each round. \"\"\" self . _check_if_fitted () return self . report_df fit ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ) \u00b6 Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (ShapRFECV) Fitted object. Source code in probatus/feature_elimination/feature_elimination.py def fit ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (ShapRFECV): Fitted object. \"\"\" # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # If to columns_to_keep is not provided, then initialise it by an empty string. # If provided check if all the elements in columns_to_keep are of type string. if columns_to_keep is None : len_columns_to_keep = 0 else : if all ( isinstance ( x , str ) for x in columns_to_keep ): len_columns_to_keep = len ( columns_to_keep ) else : raise ( ValueError ( \"The current values of columns_to_keep are not allowed.All the elements should be strings.\" ) ) # If the columns_to_keep parameter is provided, check if they match the column names in the X. if column_names is not None : if all ( x in column_names for x in list ( X . columns )): pass else : raise ( ValueError ( \"The column names in parameter columns_to_keep and column_names are not macthing.\" )) # Check that the total number of columns to select is less than total number of columns in the data. # only when both parameters are provided. if column_names is not None and columns_to_keep is not None : if ( self . min_features_to_select + len_columns_to_keep ) > len ( self . column_names ): raise ValueError ( \"Minimum features to select is greater than number of features.\" \"Lower the value for min_features_to_select or number of columns in columns_to_keep\" ) self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) self . cv = check_cv ( self . cv , self . y , classifier = is_classifier ( self . clf )) remaining_features = current_features_set = self . column_names round_number = 0 # Stop when stopping criteria is met. stopping_criteria = np . max ([ self . min_features_to_select , len_columns_to_keep ]) # Setting up the min_features_to_select parameter. if columns_to_keep is None : pass else : self . min_features_to_select = 0 # This ensures that, if columns_to_keep is provided , # the last features remaining are only the columns_to_keep. if self . verbose > 50 : warnings . warn ( f \"Minimum features to select : { stopping_criteria } \" ) while len ( current_features_set ) > stopping_criteria : round_number += 1 # Get current dataset info current_features_set = remaining_features if columns_to_keep is None : remaining_removeable_features = list ( set ( current_features_set )) else : remaining_removeable_features = list ( set ( current_features_set ) | set ( columns_to_keep )) current_X = self . X [ remaining_removeable_features ] # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # Optimize parameters if self . search_clf : current_search_clf = clone ( self . clf ) . fit ( current_X , self . y ) current_clf = current_search_clf . estimator . set_params ( ** current_search_clf . best_params_ ) else : current_clf = clone ( self . clf ) # Perform CV to estimate feature importance with SHAP results_per_fold = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _get_feature_shap_values_per_fold )( X = current_X , y = self . y , clf = current_clf , train_index = train_index , val_index = val_index , ** shap_kwargs , ) for train_index , val_index in self . cv . split ( current_X , self . y ) ) shap_values = np . vstack ([ current_result [ 0 ] for current_result in results_per_fold ]) scores_train = [ current_result [ 1 ] for current_result in results_per_fold ] scores_val = [ current_result [ 2 ] for current_result in results_per_fold ] # Calculate the shap features with remaining features and features to keep. shap_importance_df = calculate_shap_importance ( shap_values , remaining_removeable_features ) # Get features to remove features_to_remove = self . _get_current_features_to_remove ( shap_importance_df , columns_to_keep = columns_to_keep ) remaining_features = list ( set ( current_features_set ) - set ( features_to_remove )) # Report results self . _report_current_results ( round_number = round_number , current_features_set = current_features_set , features_to_remove = features_to_remove , train_metric_mean = np . round ( np . mean ( scores_train ), 3 ), train_metric_std = np . round ( np . std ( scores_train ), 3 ), val_metric_mean = np . round ( np . mean ( scores_val ), 3 ), val_metric_std = np . round ( np . std ( scores_val ), 3 ), ) if self . verbose > 50 : print ( f \"Round: { round_number } , Current number of features: { len ( current_features_set ) } , \" f 'Current performance: Train { self . report_df . loc [ round_number ][ \"train_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"train_metric_std\" ] } , CV Validation ' f ' { self . report_df . loc [ round_number ][ \"val_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"val_metric_std\" ] } . \\n ' f \"Features left: { remaining_features } . \" f \"Removed features at the end of the round: { features_to_remove } \" ) self . fitted = True return self fit_compute ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ) \u00b6 Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of columns to keep. If given, these columns will not be eliminated. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame) DataFrame containing results of feature elimination from each iteration. Source code in probatus/feature_elimination/feature_elimination.py def fit_compute ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of columns to keep. If given, these columns will not be eliminated. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame): DataFrame containing results of feature elimination from each iteration. \"\"\" self . fit ( X , y , columns_to_keep = columns_to_keep , column_names = column_names , ** shap_kwargs ) return self . compute () get_reduced_features_set ( self , num_features ) \u00b6 Gets the features set after the feature elimination process, for a given number of features. Parameters: Name Type Description Default num_features int Number of features in the reduced features set. required Returns: Type Description (list of str) Reduced features set. Source code in probatus/feature_elimination/feature_elimination.py def get_reduced_features_set ( self , num_features ): \"\"\" Gets the features set after the feature elimination process, for a given number of features. Args: num_features (int): Number of features in the reduced features set. Returns: (list of str): Reduced features set. \"\"\" self . _check_if_fitted () if num_features not in self . report_df . num_features . tolist (): raise ( ValueError ( f \"The provided number of features has not been achieved at any stage of the process. \" f \"You can select one of the following: { self . report_df . num_features . tolist () } \" ) ) else : return self . report_df [ self . report_df . num_features == num_features ][ \"features_set\" ] . values [ 0 ] plot ( self , show = True , ** figure_kwargs ) \u00b6 Generates plot of the model performance for each iteration of feature elimination. Parameters: Name Type Description Default show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description (plt.axis) Axis containing the performance plot. Source code in probatus/feature_elimination/feature_elimination.py def plot ( self , show = True , ** figure_kwargs ): \"\"\" Generates plot of the model performance for each iteration of feature elimination. Args: show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: (plt.axis): Axis containing the performance plot. \"\"\" x_ticks = list ( reversed ( self . report_df [ \"num_features\" ] . tolist ())) plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"train_metric_mean\" ], label = \"Train Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"train_metric_mean\" ] - self . report_df [ \"train_metric_std\" ], self . report_df [ \"train_metric_mean\" ] + self . report_df [ \"train_metric_std\" ], alpha = 0.3 , ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"val_metric_mean\" ], label = \"Validation Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"val_metric_mean\" ] - self . report_df [ \"val_metric_std\" ], self . report_df [ \"val_metric_mean\" ] + self . report_df [ \"val_metric_std\" ], alpha = 0.3 , ) plt . xlabel ( \"Number of features\" ) plt . ylabel ( f \"Performance { self . scorer . metric_name } \" ) plt . title ( \"Backwards Feature Elimination using SHAP & CV\" ) plt . legend ( loc = \"lower left\" ) ax = plt . gca () ax . invert_xaxis () ax . set_xticks ( x_ticks ) if show : plt . show () else : plt . close () return ax","title":"probatus.feature_elimination"},{"location":"api/feature_elimination.html#features-elimination","text":"This module focuses on feature elimination and it contains two classes: ShapRFECV : Perform Backwards Recursive Feature Elimination, using SHAP feature importance. It supports binary classification models and hyperparameter optimization at every feature elimination step. EarlyStoppingShapRFECV : adds support to early stopping of the model fitting process. It can be an alternative regularization technique to hyperparameter optimization of the number of base trees in gradient boosted tree models. Particularly useful when dealing with large datasets.","title":"Features Elimination"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV","text":"This class performs Backwards Recursive Feature Elimination, using SHAP feature importance. This is a child of ShapRFECV which allows early stopping of the training step, available in models such as XGBoost and LightGBM. If you are not using early stopping, you should use the parent class, ShapRFECV, instead of EarlyStoppingShapRFECV. Early stopping is a type of regularization technique in which the model is trained until the scoring metric, measured on a validation set, stops improving after a number of early_stopping_rounds. In boosted tree models, this technique can increase the training speed, by skipping the training of trees that do not improve the scoring metric any further, which is particularly useful when the training dataset is large. Note that if the classifier is a hyperparameter search model is used, the early stopping parameter is passed only to the fit method of the model duiring the Shapley values estimation step, and not for the hyperparameter search step. Early stopping can be seen as a type of regularization of the optimal number of trees. Therefore you can use it directly with a LightGBM or XGBoost model, as an alternative to a hyperparameter search model. At each round, for a given feature set, starting from all available features, the following steps are applied: (Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g. GridSearchCV , RandomizedSearchCV , or BayesSearchCV . Note that during this step the model does not use early stopping. Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV iteration, the model is fitted on the train folds, and applied on the validation fold to estimate SHAP feature importance. The model is trained until the scoring metric eval_metric, measured on the validation fold, stops improving after a number of early_stopping_rounds. Remove step lowest SHAP importance features from the dataset. At the end of the process, the user can plot the performance of the model for each iteration, and select the optimal number of features and the features set. We recommend using LGBMClassifier , because by default it handles missing values and categorical features. In case of other models, make sure to handle these issues for your dataset and consider impact it might have on features importance. Examples: from lightgbm import LGBMClassifier import pandas as pd from probatus.feature_elimination import EarlyStoppingShapRFECV from sklearn.datasets import make_classification feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 200 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) # Prepare model clf = LGBMClassifier ( n_estimators = 200 , max_depth = 3 ) # Run feature elimination shap_elimination = EarlyStoppingShapRFECV ( clf = clf , step = 0.2 , cv = 10 , scoring = 'roc_auc' , early_stopping_rounds = 10 , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) # Make plots performance_plot = shap_elimination . plot () # Get final feature set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 3 )","title":"EarlyStoppingShapRFECV"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.__init__","text":"This method initializes the class. Parameters: Name Type Description Default clf binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV A model that will be optimized and trained at each round of features elimination. The model must support early stopping of training, which is the case for XGBoost and LightGBM, for example. The recommended model is LGBMClassifier , because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV . Note that if a hyperparemeter search model is used, the hyperparameters are tuned without early stopping. Early stopping is applied only during the Shapley values estimation for feature elimination. We recommend simply passing the model without hyperparameter optimization, or using ShapRFECV without early stopping. required step int or float Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. 1 min_features_to_select int Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. 1 cv int, cross-validation generator or an iterable Determines the cross-validation splitting strategy. Compatible with sklearn cv parameter . If None, then cv of 5 is used. None scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' n_jobs int Number of cores to run in parallel while fitting across folds. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. -1 verbose int Controls verbosity of the output: 0 - nether prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None early_stopping_rounds int Number of rounds with constant performance after which the model fitting stops. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as XGBoost and LightGBM. 5 eval_metric str Metric for scoring fitting rounds and activating early stopping. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as XGBoost and LightGBM . Note that eval_metric is an argument of the model's fit method and it is different from scoring . 'auc' Source code in probatus/feature_elimination/feature_elimination.py def __init__ ( self , clf , step = 1 , min_features_to_select = 1 , cv = None , scoring = \"roc_auc\" , n_jobs =- 1 , verbose = 0 , random_state = None , early_stopping_rounds = 5 , eval_metric = \"auc\" , ): \"\"\" This method initializes the class. Args: clf (binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV): A model that will be optimized and trained at each round of features elimination. The model must support early stopping of training, which is the case for XGBoost and LightGBM, for example. The recommended model is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html), because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV). Note that if a hyperparemeter search model is used, the hyperparameters are tuned without early stopping. Early stopping is applied only during the Shapley values estimation for feature elimination. We recommend simply passing the model without hyperparameter optimization, or using ShapRFECV without early stopping. step (int or float, optional): Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. min_features_to_select (int, optional): Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. cv (int, cross-validation generator or an iterable, optional): Determines the cross-validation splitting strategy. Compatible with sklearn [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html). If None, then cv of 5 is used. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. n_jobs (int, optional): Number of cores to run in parallel while fitting across folds. None means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. verbose (int, optional): Controls verbosity of the output: - 0 - nether prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. early_stopping_rounds (int, optional): Number of rounds with constant performance after which the model fitting stops. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as XGBoost and LightGBM. eval_metric (str, optional): Metric for scoring fitting rounds and activating early stopping. This is passed to the fit method of the model for Shapley values estimation, but not for hyperparameter search. Only supported by some models, such as [XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters) and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters). Note that `eval_metric` is an argument of the model's fit method and it is different from `scoring`. \"\"\" # noqa super ( EarlyStoppingShapRFECV , self ) . __init__ ( clf , step = step , min_features_to_select = min_features_to_select , cv = cv , scoring = scoring , n_jobs = n_jobs , verbose = verbose , random_state = random_state , ) if self . search_clf : if self . verbose > 0 : warnings . warn ( \"Early stopping will be used only during Shapley value estimation step, and not for hyperparameter\" \"optimization.\" ) if isinstance ( early_stopping_rounds , int ) and early_stopping_rounds > 0 : self . early_stopping_rounds = early_stopping_rounds else : raise ( ValueError ( f \"The current value of early_stopping_rounds = { early_stopping_rounds } is not allowed. \" f \"It needs to be a positive integer.\" ) ) self . eval_metric = eval_metric","title":"__init__()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.compute","text":"Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: Type Description (pd.DataFrame) DataFrame with results of feature elimination for each round. Source code in probatus/feature_elimination/feature_elimination.py def compute ( self ): \"\"\" Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: (pd.DataFrame): DataFrame with results of feature elimination for each round. \"\"\" self . _check_if_fitted () return self . report_df","title":"compute()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit","text":"Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (ShapRFECV) Fitted object. Source code in probatus/feature_elimination/feature_elimination.py def fit ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (ShapRFECV): Fitted object. \"\"\" # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # If to columns_to_keep is not provided, then initialise it by an empty string. # If provided check if all the elements in columns_to_keep are of type string. if columns_to_keep is None : len_columns_to_keep = 0 else : if all ( isinstance ( x , str ) for x in columns_to_keep ): len_columns_to_keep = len ( columns_to_keep ) else : raise ( ValueError ( \"The current values of columns_to_keep are not allowed.All the elements should be strings.\" ) ) # If the columns_to_keep parameter is provided, check if they match the column names in the X. if column_names is not None : if all ( x in column_names for x in list ( X . columns )): pass else : raise ( ValueError ( \"The column names in parameter columns_to_keep and column_names are not macthing.\" )) # Check that the total number of columns to select is less than total number of columns in the data. # only when both parameters are provided. if column_names is not None and columns_to_keep is not None : if ( self . min_features_to_select + len_columns_to_keep ) > len ( self . column_names ): raise ValueError ( \"Minimum features to select is greater than number of features.\" \"Lower the value for min_features_to_select or number of columns in columns_to_keep\" ) self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) self . cv = check_cv ( self . cv , self . y , classifier = is_classifier ( self . clf )) remaining_features = current_features_set = self . column_names round_number = 0 # Stop when stopping criteria is met. stopping_criteria = np . max ([ self . min_features_to_select , len_columns_to_keep ]) # Setting up the min_features_to_select parameter. if columns_to_keep is None : pass else : self . min_features_to_select = 0 # This ensures that, if columns_to_keep is provided , # the last features remaining are only the columns_to_keep. if self . verbose > 50 : warnings . warn ( f \"Minimum features to select : { stopping_criteria } \" ) while len ( current_features_set ) > stopping_criteria : round_number += 1 # Get current dataset info current_features_set = remaining_features if columns_to_keep is None : remaining_removeable_features = list ( set ( current_features_set )) else : remaining_removeable_features = list ( set ( current_features_set ) | set ( columns_to_keep )) current_X = self . X [ remaining_removeable_features ] # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # Optimize parameters if self . search_clf : current_search_clf = clone ( self . clf ) . fit ( current_X , self . y ) current_clf = current_search_clf . estimator . set_params ( ** current_search_clf . best_params_ ) else : current_clf = clone ( self . clf ) # Perform CV to estimate feature importance with SHAP results_per_fold = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _get_feature_shap_values_per_fold )( X = current_X , y = self . y , clf = current_clf , train_index = train_index , val_index = val_index , ** shap_kwargs , ) for train_index , val_index in self . cv . split ( current_X , self . y ) ) shap_values = np . vstack ([ current_result [ 0 ] for current_result in results_per_fold ]) scores_train = [ current_result [ 1 ] for current_result in results_per_fold ] scores_val = [ current_result [ 2 ] for current_result in results_per_fold ] # Calculate the shap features with remaining features and features to keep. shap_importance_df = calculate_shap_importance ( shap_values , remaining_removeable_features ) # Get features to remove features_to_remove = self . _get_current_features_to_remove ( shap_importance_df , columns_to_keep = columns_to_keep ) remaining_features = list ( set ( current_features_set ) - set ( features_to_remove )) # Report results self . _report_current_results ( round_number = round_number , current_features_set = current_features_set , features_to_remove = features_to_remove , train_metric_mean = np . round ( np . mean ( scores_train ), 3 ), train_metric_std = np . round ( np . std ( scores_train ), 3 ), val_metric_mean = np . round ( np . mean ( scores_val ), 3 ), val_metric_std = np . round ( np . std ( scores_val ), 3 ), ) if self . verbose > 50 : print ( f \"Round: { round_number } , Current number of features: { len ( current_features_set ) } , \" f 'Current performance: Train { self . report_df . loc [ round_number ][ \"train_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"train_metric_std\" ] } , CV Validation ' f ' { self . report_df . loc [ round_number ][ \"val_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"val_metric_std\" ] } . \\n ' f \"Features left: { remaining_features } . \" f \"Removed features at the end of the round: { features_to_remove } \" ) self . fitted = True return self","title":"fit()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit_compute","text":"Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of columns to keep. If given, these columns will not be eliminated. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame) DataFrame containing results of feature elimination from each iteration. Source code in probatus/feature_elimination/feature_elimination.py def fit_compute ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of columns to keep. If given, these columns will not be eliminated. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame): DataFrame containing results of feature elimination from each iteration. \"\"\" self . fit ( X , y , columns_to_keep = columns_to_keep , column_names = column_names , ** shap_kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.get_reduced_features_set","text":"Gets the features set after the feature elimination process, for a given number of features. Parameters: Name Type Description Default num_features int Number of features in the reduced features set. required Returns: Type Description (list of str) Reduced features set. Source code in probatus/feature_elimination/feature_elimination.py def get_reduced_features_set ( self , num_features ): \"\"\" Gets the features set after the feature elimination process, for a given number of features. Args: num_features (int): Number of features in the reduced features set. Returns: (list of str): Reduced features set. \"\"\" self . _check_if_fitted () if num_features not in self . report_df . num_features . tolist (): raise ( ValueError ( f \"The provided number of features has not been achieved at any stage of the process. \" f \"You can select one of the following: { self . report_df . num_features . tolist () } \" ) ) else : return self . report_df [ self . report_df . num_features == num_features ][ \"features_set\" ] . values [ 0 ]","title":"get_reduced_features_set()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.plot","text":"Generates plot of the model performance for each iteration of feature elimination. Parameters: Name Type Description Default show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description (plt.axis) Axis containing the performance plot. Source code in probatus/feature_elimination/feature_elimination.py def plot ( self , show = True , ** figure_kwargs ): \"\"\" Generates plot of the model performance for each iteration of feature elimination. Args: show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: (plt.axis): Axis containing the performance plot. \"\"\" x_ticks = list ( reversed ( self . report_df [ \"num_features\" ] . tolist ())) plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"train_metric_mean\" ], label = \"Train Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"train_metric_mean\" ] - self . report_df [ \"train_metric_std\" ], self . report_df [ \"train_metric_mean\" ] + self . report_df [ \"train_metric_std\" ], alpha = 0.3 , ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"val_metric_mean\" ], label = \"Validation Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"val_metric_mean\" ] - self . report_df [ \"val_metric_std\" ], self . report_df [ \"val_metric_mean\" ] + self . report_df [ \"val_metric_std\" ], alpha = 0.3 , ) plt . xlabel ( \"Number of features\" ) plt . ylabel ( f \"Performance { self . scorer . metric_name } \" ) plt . title ( \"Backwards Feature Elimination using SHAP & CV\" ) plt . legend ( loc = \"lower left\" ) ax = plt . gca () ax . invert_xaxis () ax . set_xticks ( x_ticks ) if show : plt . show () else : plt . close () return ax","title":"plot()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV","text":"This class performs Backwards Recursive Feature Elimination, using SHAP feature importance. At each round, for a given feature set, starting from all available features, the following steps are applied: (Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g. GridSearchCV , RandomizedSearchCV , or BayesSearchCV , Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV iteration, the model is fitted on the train folds, and applied on the validation fold to estimate SHAP feature importance. Remove step lowest SHAP importance features from the dataset. At the end of the process, the user can plot the performance of the model for each iteration, and select the optimal number of features and the features set. The functionality is similar to RFECV . The main difference is removing the lowest importance features based on SHAP features importance. It also supports the use of sklearn compatible search CV for hyperparameter optimization e.g. GridSearchCV , RandomizedSearchCV , or BayesSearchCV , which needs to be passed as the clf . Thanks to this you can perform hyperparameter optimization at each step of the feature elimination. Lastly, it supports categorical features (object and category dtype) and missing values in the data, as long as the model supports them. We recommend using LGBMClassifier , because by default it handles missing values and categorical features. In case of other models, make sure to handle these issues for your dataset and consider impact it might have on features importance. Examples: import numpy as np import pandas as pd from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import RandomizedSearchCV feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 200 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) # Prepare model and parameter search space clf = RandomForestClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'min_samples_leaf' : [ 3 , 5 , 7 , 10 ], } search = RandomizedSearchCV ( clf , param_grid ) # Run feature elimination shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) # Make plots performance_plot = shap_elimination . plot () # Get final feature set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 3 )","title":"ShapRFECV"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.__init__","text":"This method initializes the class. Parameters: Name Type Description Default clf binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV A model that will be optimized and trained at each round of feature elimination. The recommended model is LGBMClassifier , because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV . required step int or float Number of lowest importance features removed each round. If it is an int, then each round such a number of features are discarded. If float, such a percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise with fewer features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. 1 min_features_to_select int Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. 1 cv int, cross-validation generator or an iterable Determines the cross-validation splitting strategy. Compatible with sklearn cv parameter . If None, then cv of 5 is used. None scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' n_jobs int Number of cores to run in parallel while fitting across folds. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. -1 verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to an integer. None Source code in probatus/feature_elimination/feature_elimination.py def __init__ ( self , clf , step = 1 , min_features_to_select = 1 , cv = None , scoring = \"roc_auc\" , n_jobs =- 1 , verbose = 0 , random_state = None , ): \"\"\" This method initializes the class. Args: clf (binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV): A model that will be optimized and trained at each round of feature elimination. The recommended model is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html), because it by default handles the missing values and categorical variables. This parameter also supports any hyperparameter search schema that is consistent with the sklearn API e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV). step (int or float, optional): Number of lowest importance features removed each round. If it is an int, then each round such a number of features are discarded. If float, such a percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise with fewer features. Note: the last round may remove fewer features in order to reach min_features_to_select. If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after keeping those columns. min_features_to_select (int, optional): Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. If columns_to_keep is specified in the fit method, it may overide this parameter to the maximum between length of columns_to_keep the two. cv (int, cross-validation generator or an iterable, optional): Determines the cross-validation splitting strategy. Compatible with sklearn [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html). If None, then cv of 5 is used. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. n_jobs (int, optional): Number of cores to run in parallel while fitting across folds. None means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to an integer. \"\"\" # noqa self . clf = clf if isinstance ( self . clf , BaseSearchCV ): self . search_clf = True else : self . search_clf = False if ( isinstance ( step , int ) or isinstance ( step , float )) and step > 0 : self . step = step else : raise ( ValueError ( f \"The current value of step = { step } is not allowed. \" f \"It needs to be a positive integer or positive float.\" ) ) if isinstance ( min_features_to_select , int ) and min_features_to_select > 0 : self . min_features_to_select = min_features_to_select else : raise ( ValueError ( f \"The current value of min_features_to_select = { min_features_to_select } is not allowed. \" f \"It needs to be a greater than or equal to 0.\" ) ) self . cv = cv self . scorer = get_single_scorer ( scoring ) self . random_state = random_state self . n_jobs = n_jobs self . report_df = pd . DataFrame ([]) self . verbose = verbose","title":"__init__()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.compute","text":"Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: Type Description (pd.DataFrame) DataFrame with results of feature elimination for each round. Source code in probatus/feature_elimination/feature_elimination.py def compute ( self ): \"\"\" Checks if fit() method has been run. and computes the DataFrame with results of feature elimintation for each round. Returns: (pd.DataFrame): DataFrame with results of feature elimination for each round. \"\"\" self . _check_if_fitted () return self . report_df","title":"compute()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.fit","text":"Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (ShapRFECV) Fitted object. Source code in probatus/feature_elimination/feature_elimination.py def fit ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of column names to keep. If given, these columns will not be eliminated by the feature elimination process. However, these feature will used for the calculation of the SHAP values. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (ShapRFECV): Fitted object. \"\"\" # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # If to columns_to_keep is not provided, then initialise it by an empty string. # If provided check if all the elements in columns_to_keep are of type string. if columns_to_keep is None : len_columns_to_keep = 0 else : if all ( isinstance ( x , str ) for x in columns_to_keep ): len_columns_to_keep = len ( columns_to_keep ) else : raise ( ValueError ( \"The current values of columns_to_keep are not allowed.All the elements should be strings.\" ) ) # If the columns_to_keep parameter is provided, check if they match the column names in the X. if column_names is not None : if all ( x in column_names for x in list ( X . columns )): pass else : raise ( ValueError ( \"The column names in parameter columns_to_keep and column_names are not macthing.\" )) # Check that the total number of columns to select is less than total number of columns in the data. # only when both parameters are provided. if column_names is not None and columns_to_keep is not None : if ( self . min_features_to_select + len_columns_to_keep ) > len ( self . column_names ): raise ValueError ( \"Minimum features to select is greater than number of features.\" \"Lower the value for min_features_to_select or number of columns in columns_to_keep\" ) self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) self . cv = check_cv ( self . cv , self . y , classifier = is_classifier ( self . clf )) remaining_features = current_features_set = self . column_names round_number = 0 # Stop when stopping criteria is met. stopping_criteria = np . max ([ self . min_features_to_select , len_columns_to_keep ]) # Setting up the min_features_to_select parameter. if columns_to_keep is None : pass else : self . min_features_to_select = 0 # This ensures that, if columns_to_keep is provided , # the last features remaining are only the columns_to_keep. if self . verbose > 50 : warnings . warn ( f \"Minimum features to select : { stopping_criteria } \" ) while len ( current_features_set ) > stopping_criteria : round_number += 1 # Get current dataset info current_features_set = remaining_features if columns_to_keep is None : remaining_removeable_features = list ( set ( current_features_set )) else : remaining_removeable_features = list ( set ( current_features_set ) | set ( columns_to_keep )) current_X = self . X [ remaining_removeable_features ] # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) # Optimize parameters if self . search_clf : current_search_clf = clone ( self . clf ) . fit ( current_X , self . y ) current_clf = current_search_clf . estimator . set_params ( ** current_search_clf . best_params_ ) else : current_clf = clone ( self . clf ) # Perform CV to estimate feature importance with SHAP results_per_fold = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _get_feature_shap_values_per_fold )( X = current_X , y = self . y , clf = current_clf , train_index = train_index , val_index = val_index , ** shap_kwargs , ) for train_index , val_index in self . cv . split ( current_X , self . y ) ) shap_values = np . vstack ([ current_result [ 0 ] for current_result in results_per_fold ]) scores_train = [ current_result [ 1 ] for current_result in results_per_fold ] scores_val = [ current_result [ 2 ] for current_result in results_per_fold ] # Calculate the shap features with remaining features and features to keep. shap_importance_df = calculate_shap_importance ( shap_values , remaining_removeable_features ) # Get features to remove features_to_remove = self . _get_current_features_to_remove ( shap_importance_df , columns_to_keep = columns_to_keep ) remaining_features = list ( set ( current_features_set ) - set ( features_to_remove )) # Report results self . _report_current_results ( round_number = round_number , current_features_set = current_features_set , features_to_remove = features_to_remove , train_metric_mean = np . round ( np . mean ( scores_train ), 3 ), train_metric_std = np . round ( np . std ( scores_train ), 3 ), val_metric_mean = np . round ( np . mean ( scores_val ), 3 ), val_metric_std = np . round ( np . std ( scores_val ), 3 ), ) if self . verbose > 50 : print ( f \"Round: { round_number } , Current number of features: { len ( current_features_set ) } , \" f 'Current performance: Train { self . report_df . loc [ round_number ][ \"train_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"train_metric_std\" ] } , CV Validation ' f ' { self . report_df . loc [ round_number ][ \"val_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"val_metric_std\" ] } . \\n ' f \"Features left: { remaining_features } . \" f \"Removed features at the end of the round: { features_to_remove } \" ) self . fitted = True return self","title":"fit()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute","text":"Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. GridSearchCV , RandomizedSearchCV or BayesSearchCV , the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and step lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required columns_to_keep list of str List of columns to keep. If given, these columns will not be eliminated. None column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame) DataFrame containing results of feature elimination from each iteration. Source code in probatus/feature_elimination/feature_elimination.py def fit_compute ( self , X , y , columns_to_keep = None , column_names = None , ** shap_kwargs ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. If sklearn compatible search CV is passed as clf e.g. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), the hyperparameter optimization is applied at each step of the elimination. Then, the SHAP feature importance is calculated using Cross-Validation, and `step` lowest importance features are removed. At the end, the report containing results from each iteration is computed and returned to the user. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. columns_to_keep (list of str, optional): List of columns to keep. If given, these columns will not be eliminated. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame): DataFrame containing results of feature elimination from each iteration. \"\"\" self . fit ( X , y , columns_to_keep = columns_to_keep , column_names = column_names , ** shap_kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set","text":"Gets the features set after the feature elimination process, for a given number of features. Parameters: Name Type Description Default num_features int Number of features in the reduced features set. required Returns: Type Description (list of str) Reduced features set. Source code in probatus/feature_elimination/feature_elimination.py def get_reduced_features_set ( self , num_features ): \"\"\" Gets the features set after the feature elimination process, for a given number of features. Args: num_features (int): Number of features in the reduced features set. Returns: (list of str): Reduced features set. \"\"\" self . _check_if_fitted () if num_features not in self . report_df . num_features . tolist (): raise ( ValueError ( f \"The provided number of features has not been achieved at any stage of the process. \" f \"You can select one of the following: { self . report_df . num_features . tolist () } \" ) ) else : return self . report_df [ self . report_df . num_features == num_features ][ \"features_set\" ] . values [ 0 ]","title":"get_reduced_features_set()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.plot","text":"Generates plot of the model performance for each iteration of feature elimination. Parameters: Name Type Description Default show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description (plt.axis) Axis containing the performance plot. Source code in probatus/feature_elimination/feature_elimination.py def plot ( self , show = True , ** figure_kwargs ): \"\"\" Generates plot of the model performance for each iteration of feature elimination. Args: show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: (plt.axis): Axis containing the performance plot. \"\"\" x_ticks = list ( reversed ( self . report_df [ \"num_features\" ] . tolist ())) plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"train_metric_mean\" ], label = \"Train Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"train_metric_mean\" ] - self . report_df [ \"train_metric_std\" ], self . report_df [ \"train_metric_mean\" ] + self . report_df [ \"train_metric_std\" ], alpha = 0.3 , ) plt . plot ( self . report_df [ \"num_features\" ], self . report_df [ \"val_metric_mean\" ], label = \"Validation Score\" , ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = \"coerce\" ), self . report_df [ \"val_metric_mean\" ] - self . report_df [ \"val_metric_std\" ], self . report_df [ \"val_metric_mean\" ] + self . report_df [ \"val_metric_std\" ], alpha = 0.3 , ) plt . xlabel ( \"Number of features\" ) plt . ylabel ( f \"Performance { self . scorer . metric_name } \" ) plt . title ( \"Backwards Feature Elimination using SHAP & CV\" ) plt . legend ( loc = \"lower left\" ) ax = plt . gca () ax . invert_xaxis () ax . set_xticks ( x_ticks ) if show : plt . show () else : plt . close () return ax","title":"plot()"},{"location":"api/imputation_selector.html","text":"Imputation Selector \u00b6 This module allows us to select imputation strategies. ImputationSelector \u00b6 Comparison of various imputation strategies. that can be used for imputing missing values. The aim of this class is to present the model performance based on imputation strategies and a choosen model. For models like XGBoost & LighGBM which have capabilities to handle missing values by default the model performance with no imputation will be shown as well. The missing values categorical features are imputed with the value missing and an missing indicator is added. Example usage. #Import the class import pandas as pd import numpy as np import matplotlib.pyplot as plt from probatus.missing_values.imputation import ImputationSelector from probatus.utils.missing_helpers import generate_MCAR from sklearn.linear_model import LogisticRegression from sklearn.experimental import enable_iterative_imputer from sklearn.impute import KNNImputer , SimpleImputer , IterativeImputer from sklearn.datasets import make_classification # Create data with missing values. n_features = 10 X , y = make_classification ( n_samples = 1000 , n_features = n_features , random_state = 123 , class_sep = 0.3 ) X = pd . DataFrame ( X , columns = [ \"f_\" + str ( i ) for i in range ( 0 , n_features )]) X_missing = generate_MCAR ( X , missing = 0.2 ) # Create the strategies. strategies = { 'Simple Median Imputer' : SimpleImputer ( strategy = 'median' , add_indicator = True ), 'Simple Mean Imputer' : SimpleImputer ( strategy = 'mean' , add_indicator = True ), 'Iterative Imputer' : IterativeImputer ( add_indicator = True , n_nearest_features = 5 , sample_posterior = True ), 'KNN' : KNNImputer ( n_neighbors = 3 )} #Create a classifier. clf = LogisticRegression () #Create the comparision of the imputation strategies. cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 , model_na_support = False ) cmp . fit_compute ( X_missing , y ) #Plot the results. performance_plot = cmp . plot () __init__ ( self , clf , strategies , scoring = 'roc_auc' , cv = 5 , model_na_support = False , n_jobs =- 1 , verbose = 0 , random_state = None ) special \u00b6 Initialise the class. Args : clf (binary classifier,sklearn.Pipeline): A binary classification model, that will used to evaluate various imputation strategies. strategies ( dictionary of sklearn . impute objects or any other scikit learn compatible imputer .): Dictionary containing the sklearn . impute objects . e . g . strategies = { 'KNN' : KNNImputer ( n_neighbors = 3 ), 'Simple Median Imputer' : SimpleImputer ( strategy = 'median' , add_indicator = True ), 'Iterative Imputer' : IterativeImputer ( add_indicator = True , n_nearest_features = 5 , sample_posterior = True ) } This allows you to have fine grained control over the imputation method . scoring ( string , list of strings , probatus . utils . Scorer or list of probatus . utils . Scorers , optional ): Metrics for which the score is calculated . It can be either a name or list of names metric names and needs to be aligned with predefined [ classification scorers names in sklearn ]( https : // scikit - learn . org / stable / modules / model_evaluation . html ). Another option is using probatus . utils . Scorer to define a custom metric . model_na_support ( boolean ): default False If the classifier supports missing values by default e . g . LightGBM , XGBoost etc . If True an default comparison ` No Imputation ` result will be added indicating the model performance without any explict imputation . If False only the provided strategies will be used . n_jobs ( int , optional ): Number of cores to run in parallel while fitting across folds . None means 1 unless in a ` joblib . parallel_backend ` context . - 1 means using all processors . verbose ( int , optional ): Controls verbosity of the output : - 0 - nether prints nor warnings are shown - 1 - 50 - only most important warnings regarding data properties are shown ( excluding SHAP warnings ) - 51 - 100 - shows most important warnings , prints of the feature removal process - above 100 - presents all prints and all warnings ( including SHAP warnings ). random_state ( int , optional ): Random state set at each round of feature elimination . If it is None , the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested . For reproducible results set it to integer . Source code in probatus/missing_values/imputation.py def __init__ ( self , clf , strategies , scoring = \"roc_auc\" , cv = 5 , model_na_support = False , n_jobs =- 1 , verbose = 0 , random_state = None , ): \"\"\" Initialise the class. Args : clf (binary classifier,sklearn.Pipeline): A binary classification model, that will used to evaluate various imputation strategies. strategies (dictionary of sklearn.impute objects or any other scikit learn compatible imputer.): Dictionary containing the sklearn.impute objects. e.g. strategies = {'KNN' : KNNImputer(n_neighbors=3), 'Simple Median Imputer' : SimpleImputer(strategy='median',add_indicator=True), 'Iterative Imputer' : IterativeImputer(add_indicator=True,n_nearest_features=5, sample_posterior=True)} This allows you to have fine grained control over the imputation method. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. model_na_support(boolean): default False If the classifier supports missing values by default e.g. LightGBM,XGBoost etc. If True an default comparison `No Imputation` result will be added indicating the model performance without any explict imputation. If False only the provided strategies will be used. n_jobs (int, optional): Number of cores to run in parallel while fitting across folds. None means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. verbose (int, optional): Controls verbosity of the output: - 0 - nether prints nor warnings are shown - 1 - 50 - only most important warnings regarding data properties are shown (excluding SHAP warnings) - 51 - 100 - shows most important warnings, prints of the feature removal process - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" # noqa self . clf = clf self . model_na_support = model_na_support self . cv = cv self . scorer = get_single_scorer ( scoring ) self . strategies = strategies self . verbose = verbose self . n_jobs = n_jobs self . random_state = random_state self . fitted = False self . report_df = pd . DataFrame ([]) compute ( self ) \u00b6 Checks if fit() method has been run. and computes the DataFrame with results of imputation for each strategy. Returns: Type Description (pd.DataFrame) DataFrame with results of imputation for each strategy. Source code in probatus/missing_values/imputation.py def compute ( self ): \"\"\" Checks if fit() method has been run. and computes the DataFrame with results of imputation for each strategy. Returns: (pd.DataFrame): DataFrame with results of imputation for each strategy. \"\"\" self . _check_if_fitted () return self . report_df fit ( self , X , y , column_names = None ) \u00b6 Calculates the cross validated results for various imputation strategies. Parameters: Name Type Description Default X pd.DataFrame input variables. required y pd.Series target variable. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X dataframe are used. None Source code in probatus/missing_values/imputation.py def fit ( self , X , y , column_names = None ): \"\"\" Calculates the cross validated results for various imputation strategies. Args: X (pd.DataFrame): input variables. y (pd.Series): target variable. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X dataframe are used. \"\"\" if self . random_state is not None : np . random . seed ( self . random_state ) # Place holder for results. results = [] self . X , self . column_names = preprocess_data ( X , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , index = self . X . index , verbose = self . verbose ) # Identify categorical features. categorical_columns = X . select_dtypes ( include = [ \"category\" , \"object\" ]) . columns # Identify the numeric columns.Numeric columns are all columns expect the categorical columns numeric_columns = X . select_dtypes ( \"number\" ) . columns for strategy in self . strategies : numeric_transformer = Pipeline ( steps = [( \"imputer\" , self . strategies [ strategy ])]) categorical_transformer = Pipeline ( steps = [ ( \"imp_cat\" , SimpleImputer ( strategy = \"constant\" , fill_value = \"missing\" , add_indicator = True , ), ), ( \"ohe_cat\" , OneHotEncoder ( handle_unknown = \"ignore\" )), ] ) preprocessor = ColumnTransformer ( transformers = [ ( \"num\" , numeric_transformer , numeric_columns ), ( \"cat\" , categorical_transformer , categorical_columns ), ], remainder = \"passthrough\" , ) model_pipeline = Pipeline ( steps = [( \"preprocessor\" , preprocessor ), ( \"classifier\" , self . clf )]) temp_results = self . _calculate_results ( X , y , clf = model_pipeline , strategy = strategy ) results . append ( temp_results ) # If model supports missing values by default, then calculate the scores # on raw data without any imputation. if self . model_na_support : categorical_transformer = Pipeline ( steps = [ ( \"ohe_cat\" , OneHotEncoder ( handle_unknown = \"ignore\" )), ] ) preprocessor = ColumnTransformer ( transformers = [( \"cat\" , categorical_transformer , categorical_columns )], remainder = \"passthrough\" , ) model_pipeline = Pipeline ( steps = [( \"preprocessor\" , preprocessor ), ( \"classifier\" , self . clf )]) temp_results = self . _calculate_results ( X , y , clf = model_pipeline , strategy = \"No Imputation\" ) results . append ( temp_results ) self . report_df = pd . DataFrame ( results ) # Set the index of the dataframe to the imputation methods. self . report_df = self . report_df . set_index ( self . report_df . strategy , \"strategy\" ) self . report_df . drop ( columns = [ \"strategy\" ], inplace = True ) self . report_df . sort_values ( by = \"mean_test_score\" , inplace = True ) self . fitted = True return self fit_compute ( self , X , y , column_names = None ) \u00b6 Calculates the cross validated results for various imputation strategies. Parameters: Name Type Description Default X pd.DataFrame input variables. required y pd.Series target variable. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X dataframe are used. None Returns: Type Description (pd.DataFrame) DataFrame with results of imputation for each strategy. Source code in probatus/missing_values/imputation.py def fit_compute ( self , X , y , column_names = None ): \"\"\" Calculates the cross validated results for various imputation strategies. Args: X (pd.DataFrame): input variables. y (pd.Series): target variable. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X dataframe are used. Returns: (pd.DataFrame): DataFrame with results of imputation for each strategy. \"\"\" self . fit ( X , y , column_names = column_names ) return self . compute () plot ( self , show = True , ** figure_kwargs ) \u00b6 Generates plot of the performance of various imputation strategies. Parameters: Name Type Description Default show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description (plt.axis) Axis containing the performance plot. Source code in probatus/missing_values/imputation.py def plot ( self , show = True , ** figure_kwargs ): \"\"\" Generates plot of the performance of various imputation strategies. Args: show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: (plt.axis): Axis containing the performance plot. \"\"\" fig , ax = plt . subplots ( ** figure_kwargs ) report_df = self . compute () imp_methods = list ( report_df . index ) test_performance = list ( report_df [ \"mean_test_score\" ]) test_std_error = list ( report_df [ \"test_score_std\" ]) train_performance = list ( report_df [ \"mean_train_score\" ]) train_std_error = list ( report_df [ \"train_score_std\" ]) y = np . arange ( len ( imp_methods )) # the label locations width = 0.35 # the width of the bars def _autolabel ( rects ): \"\"\" Label the bars of the plot. \"\"\" for rect in rects : width = rect . get_width () ax . annotate ( \" {} \" . format ( width ), xy = (( width + 0.05 * width ), rect . get_y () + rect . get_height () / 2 ), xytext = ( 4 , 0 ), # 4 points horizontal offset textcoords = \"offset points\" , ha = \"center\" , va = \"bottom\" , fontsize = \"small\" , ) train_rect = ax . barh ( y - width / 2 , train_performance , width , xerr = train_std_error , align = \"center\" , label = \"CV-Train\" , ) test_rect = ax . barh ( y + width / 2 , test_performance , width , xerr = test_std_error , align = \"center\" , label = \"CV-Test\" , ) _autolabel ( train_rect ) _autolabel ( test_rect ) ax . set_xlabel ( f ' { self . scorer . metric_name . replace ( \"_\" , \" \" ) . upper () } Score' ) ax . set_title ( \"Imputation Techniques Comparision\" ) ax . set_yticks ( y ) ax . set_yticklabels ( imp_methods , rotation = 45 ) plt . margins ( 0.2 ) plt . legend ( loc = \"best\" , ncol = 2 ) fig . tight_layout () if show : plt . show () else : plt . close () return ax","title":"probatus.missing_values"},{"location":"api/imputation_selector.html#imputation-selector","text":"This module allows us to select imputation strategies.","title":"Imputation Selector"},{"location":"api/imputation_selector.html#probatus.missing_values.imputation.ImputationSelector","text":"Comparison of various imputation strategies. that can be used for imputing missing values. The aim of this class is to present the model performance based on imputation strategies and a choosen model. For models like XGBoost & LighGBM which have capabilities to handle missing values by default the model performance with no imputation will be shown as well. The missing values categorical features are imputed with the value missing and an missing indicator is added. Example usage. #Import the class import pandas as pd import numpy as np import matplotlib.pyplot as plt from probatus.missing_values.imputation import ImputationSelector from probatus.utils.missing_helpers import generate_MCAR from sklearn.linear_model import LogisticRegression from sklearn.experimental import enable_iterative_imputer from sklearn.impute import KNNImputer , SimpleImputer , IterativeImputer from sklearn.datasets import make_classification # Create data with missing values. n_features = 10 X , y = make_classification ( n_samples = 1000 , n_features = n_features , random_state = 123 , class_sep = 0.3 ) X = pd . DataFrame ( X , columns = [ \"f_\" + str ( i ) for i in range ( 0 , n_features )]) X_missing = generate_MCAR ( X , missing = 0.2 ) # Create the strategies. strategies = { 'Simple Median Imputer' : SimpleImputer ( strategy = 'median' , add_indicator = True ), 'Simple Mean Imputer' : SimpleImputer ( strategy = 'mean' , add_indicator = True ), 'Iterative Imputer' : IterativeImputer ( add_indicator = True , n_nearest_features = 5 , sample_posterior = True ), 'KNN' : KNNImputer ( n_neighbors = 3 )} #Create a classifier. clf = LogisticRegression () #Create the comparision of the imputation strategies. cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 , model_na_support = False ) cmp . fit_compute ( X_missing , y ) #Plot the results. performance_plot = cmp . plot ()","title":"ImputationSelector"},{"location":"api/imputation_selector.html#probatus.missing_values.imputation.ImputationSelector.__init__","text":"Initialise the class. Args : clf (binary classifier,sklearn.Pipeline): A binary classification model, that will used to evaluate various imputation strategies. strategies ( dictionary of sklearn . impute objects or any other scikit learn compatible imputer .): Dictionary containing the sklearn . impute objects . e . g . strategies = { 'KNN' : KNNImputer ( n_neighbors = 3 ), 'Simple Median Imputer' : SimpleImputer ( strategy = 'median' , add_indicator = True ), 'Iterative Imputer' : IterativeImputer ( add_indicator = True , n_nearest_features = 5 , sample_posterior = True ) } This allows you to have fine grained control over the imputation method . scoring ( string , list of strings , probatus . utils . Scorer or list of probatus . utils . Scorers , optional ): Metrics for which the score is calculated . It can be either a name or list of names metric names and needs to be aligned with predefined [ classification scorers names in sklearn ]( https : // scikit - learn . org / stable / modules / model_evaluation . html ). Another option is using probatus . utils . Scorer to define a custom metric . model_na_support ( boolean ): default False If the classifier supports missing values by default e . g . LightGBM , XGBoost etc . If True an default comparison ` No Imputation ` result will be added indicating the model performance without any explict imputation . If False only the provided strategies will be used . n_jobs ( int , optional ): Number of cores to run in parallel while fitting across folds . None means 1 unless in a ` joblib . parallel_backend ` context . - 1 means using all processors . verbose ( int , optional ): Controls verbosity of the output : - 0 - nether prints nor warnings are shown - 1 - 50 - only most important warnings regarding data properties are shown ( excluding SHAP warnings ) - 51 - 100 - shows most important warnings , prints of the feature removal process - above 100 - presents all prints and all warnings ( including SHAP warnings ). random_state ( int , optional ): Random state set at each round of feature elimination . If it is None , the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested . For reproducible results set it to integer . Source code in probatus/missing_values/imputation.py def __init__ ( self , clf , strategies , scoring = \"roc_auc\" , cv = 5 , model_na_support = False , n_jobs =- 1 , verbose = 0 , random_state = None , ): \"\"\" Initialise the class. Args : clf (binary classifier,sklearn.Pipeline): A binary classification model, that will used to evaluate various imputation strategies. strategies (dictionary of sklearn.impute objects or any other scikit learn compatible imputer.): Dictionary containing the sklearn.impute objects. e.g. strategies = {'KNN' : KNNImputer(n_neighbors=3), 'Simple Median Imputer' : SimpleImputer(strategy='median',add_indicator=True), 'Iterative Imputer' : IterativeImputer(add_indicator=True,n_nearest_features=5, sample_posterior=True)} This allows you to have fine grained control over the imputation method. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. model_na_support(boolean): default False If the classifier supports missing values by default e.g. LightGBM,XGBoost etc. If True an default comparison `No Imputation` result will be added indicating the model performance without any explict imputation. If False only the provided strategies will be used. n_jobs (int, optional): Number of cores to run in parallel while fitting across folds. None means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. verbose (int, optional): Controls verbosity of the output: - 0 - nether prints nor warnings are shown - 1 - 50 - only most important warnings regarding data properties are shown (excluding SHAP warnings) - 51 - 100 - shows most important warnings, prints of the feature removal process - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" # noqa self . clf = clf self . model_na_support = model_na_support self . cv = cv self . scorer = get_single_scorer ( scoring ) self . strategies = strategies self . verbose = verbose self . n_jobs = n_jobs self . random_state = random_state self . fitted = False self . report_df = pd . DataFrame ([])","title":"__init__()"},{"location":"api/imputation_selector.html#probatus.missing_values.imputation.ImputationSelector.compute","text":"Checks if fit() method has been run. and computes the DataFrame with results of imputation for each strategy. Returns: Type Description (pd.DataFrame) DataFrame with results of imputation for each strategy. Source code in probatus/missing_values/imputation.py def compute ( self ): \"\"\" Checks if fit() method has been run. and computes the DataFrame with results of imputation for each strategy. Returns: (pd.DataFrame): DataFrame with results of imputation for each strategy. \"\"\" self . _check_if_fitted () return self . report_df","title":"compute()"},{"location":"api/imputation_selector.html#probatus.missing_values.imputation.ImputationSelector.fit","text":"Calculates the cross validated results for various imputation strategies. Parameters: Name Type Description Default X pd.DataFrame input variables. required y pd.Series target variable. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X dataframe are used. None Source code in probatus/missing_values/imputation.py def fit ( self , X , y , column_names = None ): \"\"\" Calculates the cross validated results for various imputation strategies. Args: X (pd.DataFrame): input variables. y (pd.Series): target variable. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X dataframe are used. \"\"\" if self . random_state is not None : np . random . seed ( self . random_state ) # Place holder for results. results = [] self . X , self . column_names = preprocess_data ( X , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , index = self . X . index , verbose = self . verbose ) # Identify categorical features. categorical_columns = X . select_dtypes ( include = [ \"category\" , \"object\" ]) . columns # Identify the numeric columns.Numeric columns are all columns expect the categorical columns numeric_columns = X . select_dtypes ( \"number\" ) . columns for strategy in self . strategies : numeric_transformer = Pipeline ( steps = [( \"imputer\" , self . strategies [ strategy ])]) categorical_transformer = Pipeline ( steps = [ ( \"imp_cat\" , SimpleImputer ( strategy = \"constant\" , fill_value = \"missing\" , add_indicator = True , ), ), ( \"ohe_cat\" , OneHotEncoder ( handle_unknown = \"ignore\" )), ] ) preprocessor = ColumnTransformer ( transformers = [ ( \"num\" , numeric_transformer , numeric_columns ), ( \"cat\" , categorical_transformer , categorical_columns ), ], remainder = \"passthrough\" , ) model_pipeline = Pipeline ( steps = [( \"preprocessor\" , preprocessor ), ( \"classifier\" , self . clf )]) temp_results = self . _calculate_results ( X , y , clf = model_pipeline , strategy = strategy ) results . append ( temp_results ) # If model supports missing values by default, then calculate the scores # on raw data without any imputation. if self . model_na_support : categorical_transformer = Pipeline ( steps = [ ( \"ohe_cat\" , OneHotEncoder ( handle_unknown = \"ignore\" )), ] ) preprocessor = ColumnTransformer ( transformers = [( \"cat\" , categorical_transformer , categorical_columns )], remainder = \"passthrough\" , ) model_pipeline = Pipeline ( steps = [( \"preprocessor\" , preprocessor ), ( \"classifier\" , self . clf )]) temp_results = self . _calculate_results ( X , y , clf = model_pipeline , strategy = \"No Imputation\" ) results . append ( temp_results ) self . report_df = pd . DataFrame ( results ) # Set the index of the dataframe to the imputation methods. self . report_df = self . report_df . set_index ( self . report_df . strategy , \"strategy\" ) self . report_df . drop ( columns = [ \"strategy\" ], inplace = True ) self . report_df . sort_values ( by = \"mean_test_score\" , inplace = True ) self . fitted = True return self","title":"fit()"},{"location":"api/imputation_selector.html#probatus.missing_values.imputation.ImputationSelector.fit_compute","text":"Calculates the cross validated results for various imputation strategies. Parameters: Name Type Description Default X pd.DataFrame input variables. required y pd.Series target variable. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X dataframe are used. None Returns: Type Description (pd.DataFrame) DataFrame with results of imputation for each strategy. Source code in probatus/missing_values/imputation.py def fit_compute ( self , X , y , column_names = None ): \"\"\" Calculates the cross validated results for various imputation strategies. Args: X (pd.DataFrame): input variables. y (pd.Series): target variable. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X dataframe are used. Returns: (pd.DataFrame): DataFrame with results of imputation for each strategy. \"\"\" self . fit ( X , y , column_names = column_names ) return self . compute ()","title":"fit_compute()"},{"location":"api/imputation_selector.html#probatus.missing_values.imputation.ImputationSelector.plot","text":"Generates plot of the performance of various imputation strategies. Parameters: Name Type Description Default show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description (plt.axis) Axis containing the performance plot. Source code in probatus/missing_values/imputation.py def plot ( self , show = True , ** figure_kwargs ): \"\"\" Generates plot of the performance of various imputation strategies. Args: show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: (plt.axis): Axis containing the performance plot. \"\"\" fig , ax = plt . subplots ( ** figure_kwargs ) report_df = self . compute () imp_methods = list ( report_df . index ) test_performance = list ( report_df [ \"mean_test_score\" ]) test_std_error = list ( report_df [ \"test_score_std\" ]) train_performance = list ( report_df [ \"mean_train_score\" ]) train_std_error = list ( report_df [ \"train_score_std\" ]) y = np . arange ( len ( imp_methods )) # the label locations width = 0.35 # the width of the bars def _autolabel ( rects ): \"\"\" Label the bars of the plot. \"\"\" for rect in rects : width = rect . get_width () ax . annotate ( \" {} \" . format ( width ), xy = (( width + 0.05 * width ), rect . get_y () + rect . get_height () / 2 ), xytext = ( 4 , 0 ), # 4 points horizontal offset textcoords = \"offset points\" , ha = \"center\" , va = \"bottom\" , fontsize = \"small\" , ) train_rect = ax . barh ( y - width / 2 , train_performance , width , xerr = train_std_error , align = \"center\" , label = \"CV-Train\" , ) test_rect = ax . barh ( y + width / 2 , test_performance , width , xerr = test_std_error , align = \"center\" , label = \"CV-Test\" , ) _autolabel ( train_rect ) _autolabel ( test_rect ) ax . set_xlabel ( f ' { self . scorer . metric_name . replace ( \"_\" , \" \" ) . upper () } Score' ) ax . set_title ( \"Imputation Techniques Comparision\" ) ax . set_yticks ( y ) ax . set_yticklabels ( imp_methods , rotation = 45 ) plt . margins ( 0.2 ) plt . legend ( loc = \"best\" , ncol = 2 ) fig . tight_layout () if show : plt . show () else : plt . close () return ax","title":"plot()"},{"location":"api/metric_volatility.html","text":"Metric Volatility \u00b6 The aim of this module is the analysis of how well a model performs on a given dataset, and how stable the performance is. The following features are implemented: TrainTestVolatility - Estimation of the volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. SplitSeedVolatility - Estimates the volatility of metrics based on splitting the data into train and test sets multiple times randomly, each time with a different seed. BootstrappedVolatility - Estimates the volatility of metrics based on splitting the data into train and test with static seed, and bootstrapping the train and test set. BootstrappedVolatility \u00b6 Estimation of volatility of metrics by bootstrapping both train and test set. By default at every iteration the train test split is the same. The test shows volatility of metric with regards to sampling different rows from static train and test sets. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import BootstrappedVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = BootstrappedVolatility ( clf , iterations = 10 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot () __init__ ( self , clf , iterations = 1000 , scoring = 'roc_auc' , train_sampling_fraction = 1 , test_sampling_fraction = 1 , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' train_sampling_fraction float Fraction of train data sampled, if sample_train_type is not None. Default value is 1. 1 test_sampling_fraction float Fraction of test data sampled, if sample_test_type is not None. Default value is 1. 1 test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply, default is None. Available options: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. None verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/metric_volatility/volatility.py def __init__ ( self , clf , iterations = 1000 , scoring = \"roc_auc\" , train_sampling_fraction = 1 , test_sampling_fraction = 1 , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. train_sampling_fraction (float, optional): Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction (float, optional): Fraction of test data sampled, if sample_test_type is not None. Default value is 1. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply, default is None. Available options: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" super () . __init__ ( clf = clf , sample_train_test_split_seed = False , train_sampling_type = \"bootstrap\" , test_sampling_type = \"bootstrap\" , iterations = iterations , scoring = scoring , train_sampling_fraction = train_sampling_fraction , test_sampling_fraction = test_sampling_fraction , test_prc = test_prc , n_jobs = n_jobs , stats_tests_to_apply = stats_tests_to_apply , verbose = verbose , random_state = random_state , ) compute ( self , metrics = None ) inherited \u00b6 Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . _check_if_fitted () if self . report is None : raise ( ValueError ( \"Report is None, thus it has not been computed by fit method. Please extend the \" \"BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()\" ) ) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ] fit ( self , X , y , column_names = None ) inherited \u00b6 Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.Series or numpy.ndarray Array with targets. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Returns: Type Description (TrainTestVolatility) Fitted object. Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y , column_names = None ): \"\"\" Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.Series or numpy.ndarray): Array with targets. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: (TrainTestVolatility): Fitted object. \"\"\" super () . fit () self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations )) . astype ( int ) if self . random_state : random_seeds = random_seeds * self . random_state if self . verbose > 0 : random_seeds = tqdm ( random_seeds ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = self . X , y = self . y , clf = self . clf , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction , ) for split_seed in random_seeds ) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () return self fit_compute ( self , * args , ** kwargs ) inherited \u00b6 Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute () plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 ) inherited \u00b6 Plots distribution of the metric. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Returns (list(matplotlib.axes)): Axes that include the plot. Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 , ): \"\"\" Plots distribution of the metric. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. Returns (list(matplotlib.axes)): Axes that include the plot. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ], ), ) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = \"Train {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = \"Test {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( \"Distributions {} \" . format ( metric )) axs [ axis_index ] . legend ( loc = \"upper right\" ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = \"Delta {} \" . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( \"Distributions delta {} \" . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = \"upper right\" ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = \" {} score\" . format ( metric ), ylabel = \"Results count\" ) if show : plt . show () else : plt . close () return axs SplitSeedVolatility \u00b6 Estimation of volatility of metrics depending on the seed used to split the data. At every iteration it splits the data into train and test set using a different stratified split and volatility of the metrics is calculated. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import SplitSeedVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = SplitSeedVolatility ( clf , iterations = 10 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot () __init__ ( self , clf , iterations = 1000 , scoring = 'roc_auc' , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply, default is None. Available options: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. None verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/metric_volatility/volatility.py def __init__ ( self , clf , iterations = 1000 , scoring = \"roc_auc\" , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply, default is None. Available options: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" super () . __init__ ( clf = clf , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , iterations = iterations , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , stats_tests_to_apply = stats_tests_to_apply , verbose = verbose , random_state = random_state , ) compute ( self , metrics = None ) inherited \u00b6 Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . _check_if_fitted () if self . report is None : raise ( ValueError ( \"Report is None, thus it has not been computed by fit method. Please extend the \" \"BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()\" ) ) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ] fit ( self , X , y , column_names = None ) inherited \u00b6 Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.Series or numpy.ndarray Array with targets. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Returns: Type Description (TrainTestVolatility) Fitted object. Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y , column_names = None ): \"\"\" Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.Series or numpy.ndarray): Array with targets. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: (TrainTestVolatility): Fitted object. \"\"\" super () . fit () self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations )) . astype ( int ) if self . random_state : random_seeds = random_seeds * self . random_state if self . verbose > 0 : random_seeds = tqdm ( random_seeds ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = self . X , y = self . y , clf = self . clf , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction , ) for split_seed in random_seeds ) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () return self fit_compute ( self , * args , ** kwargs ) inherited \u00b6 Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute () plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 ) inherited \u00b6 Plots distribution of the metric. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Returns (list(matplotlib.axes)): Axes that include the plot. Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 , ): \"\"\" Plots distribution of the metric. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. Returns (list(matplotlib.axes)): Axes that include the plot. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ], ), ) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = \"Train {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = \"Test {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( \"Distributions {} \" . format ( metric )) axs [ axis_index ] . legend ( loc = \"upper right\" ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = \"Delta {} \" . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( \"Distributions delta {} \" . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = \"upper right\" ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = \" {} score\" . format ( metric ), ylabel = \"Results count\" ) if show : plt . show () else : plt . close () return axs TrainTestVolatility \u00b6 Estimation of volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. The class allows for choosing whether at each iteration the train test split should be the same or different, whether and how the train and test sets should be sampled. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import TrainTestVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = TrainTestVolatility ( clf , iterations = 10 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot () __init__ ( self , clf , iterations = 1000 , scoring = 'roc_auc' , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' sample_train_test_split_seed bool Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. True train_sampling_type str String indicating what type of sampling should be applied on train set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on train data, 'subsample' indicates that sampling without repetition will be performed on train data. None test_sampling_type str String indicating what type of sampling should be applied on test set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on test data, 'subsample' indicates that sampling without repetition will be performed on test data. None train_sampling_fraction float Fraction of train data sampled, if sample_train_type is not None. Default value is 1. 1 test_sampling_fraction float Fraction of test data sampled, if sample_test_type is not None. Default value is 1. 1 test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply, default is None. Available options: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. None verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/metric_volatility/volatility.py def __init__ ( self , clf , iterations = 1000 , scoring = \"roc_auc\" , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. sample_train_test_split_seed (bool, optional): Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. train_sampling_type (str, optional): String indicating what type of sampling should be applied on train set: - `None` indicates that no additional sampling is done after splitting data, - `'bootstrap'` indicates that sampling with replacement will be performed on train data, - `'subsample'` indicates that sampling without repetition will be performed on train data. test_sampling_type (str, optional): String indicating what type of sampling should be applied on test set: - `None` indicates that no additional sampling is done after splitting data, - `'bootstrap'` indicates that sampling with replacement will be performed on test data, - `'subsample'` indicates that sampling without repetition will be performed on test data. train_sampling_fraction (float, optional): Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction (float, optional): Fraction of test data sampled, if sample_test_type is not None. Default value is 1. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply, default is None. Available options: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" super () . __init__ ( clf = clf , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , stats_tests_to_apply = stats_tests_to_apply , verbose = verbose , random_state = random_state , ) self . iterations = iterations self . train_sampling_type = train_sampling_type self . test_sampling_type = test_sampling_type self . sample_train_test_split_seed = sample_train_test_split_seed self . train_sampling_fraction = train_sampling_fraction self . test_sampling_fraction = test_sampling_fraction check_sampling_input ( train_sampling_type , train_sampling_fraction , \"train\" ) check_sampling_input ( test_sampling_type , test_sampling_fraction , \"test\" ) compute ( self , metrics = None ) inherited \u00b6 Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . _check_if_fitted () if self . report is None : raise ( ValueError ( \"Report is None, thus it has not been computed by fit method. Please extend the \" \"BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()\" ) ) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ] fit ( self , X , y , column_names = None ) \u00b6 Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.Series or numpy.ndarray Array with targets. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Returns: Type Description (TrainTestVolatility) Fitted object. Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y , column_names = None ): \"\"\" Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.Series or numpy.ndarray): Array with targets. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: (TrainTestVolatility): Fitted object. \"\"\" super () . fit () self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations )) . astype ( int ) if self . random_state : random_seeds = random_seeds * self . random_state if self . verbose > 0 : random_seeds = tqdm ( random_seeds ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = self . X , y = self . y , clf = self . clf , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction , ) for split_seed in random_seeds ) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () return self fit_compute ( self , * args , ** kwargs ) inherited \u00b6 Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute () plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 ) inherited \u00b6 Plots distribution of the metric. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Returns (list(matplotlib.axes)): Axes that include the plot. Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 , ): \"\"\" Plots distribution of the metric. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. Returns (list(matplotlib.axes)): Axes that include the plot. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ], ), ) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = \"Train {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = \"Test {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( \"Distributions {} \" . format ( metric )) axs [ axis_index ] . legend ( loc = \"upper right\" ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = \"Delta {} \" . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( \"Distributions delta {} \" . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = \"upper right\" ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = \" {} score\" . format ( metric ), ylabel = \"Results count\" ) if show : plt . show () else : plt . close () return axs","title":"probatus.metric_volatility"},{"location":"api/metric_volatility.html#metric-volatility","text":"The aim of this module is the analysis of how well a model performs on a given dataset, and how stable the performance is. The following features are implemented: TrainTestVolatility - Estimation of the volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. SplitSeedVolatility - Estimates the volatility of metrics based on splitting the data into train and test sets multiple times randomly, each time with a different seed. BootstrappedVolatility - Estimates the volatility of metrics based on splitting the data into train and test with static seed, and bootstrapping the train and test set.","title":"Metric Volatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility","text":"Estimation of volatility of metrics by bootstrapping both train and test set. By default at every iteration the train test split is the same. The test shows volatility of metric with regards to sampling different rows from static train and test sets. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import BootstrappedVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = BootstrappedVolatility ( clf , iterations = 10 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot ()","title":"BootstrappedVolatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' train_sampling_fraction float Fraction of train data sampled, if sample_train_type is not None. Default value is 1. 1 test_sampling_fraction float Fraction of test data sampled, if sample_test_type is not None. Default value is 1. 1 test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply, default is None. Available options: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. None verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/metric_volatility/volatility.py def __init__ ( self , clf , iterations = 1000 , scoring = \"roc_auc\" , train_sampling_fraction = 1 , test_sampling_fraction = 1 , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. train_sampling_fraction (float, optional): Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction (float, optional): Fraction of test data sampled, if sample_test_type is not None. Default value is 1. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply, default is None. Available options: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" super () . __init__ ( clf = clf , sample_train_test_split_seed = False , train_sampling_type = \"bootstrap\" , test_sampling_type = \"bootstrap\" , iterations = iterations , scoring = scoring , train_sampling_fraction = train_sampling_fraction , test_sampling_fraction = test_sampling_fraction , test_prc = test_prc , n_jobs = n_jobs , stats_tests_to_apply = stats_tests_to_apply , verbose = verbose , random_state = random_state , )","title":"__init__()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.compute","text":"Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . _check_if_fitted () if self . report is None : raise ( ValueError ( \"Report is None, thus it has not been computed by fit method. Please extend the \" \"BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()\" ) ) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ]","title":"compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.fit","text":"Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.Series or numpy.ndarray Array with targets. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Returns: Type Description (TrainTestVolatility) Fitted object. Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y , column_names = None ): \"\"\" Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.Series or numpy.ndarray): Array with targets. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: (TrainTestVolatility): Fitted object. \"\"\" super () . fit () self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations )) . astype ( int ) if self . random_state : random_seeds = random_seeds * self . random_state if self . verbose > 0 : random_seeds = tqdm ( random_seeds ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = self . X , y = self . y , clf = self . clf , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction , ) for split_seed in random_seeds ) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () return self","title":"fit()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.fit_compute","text":"Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.plot","text":"Plots distribution of the metric. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Returns (list(matplotlib.axes)): Axes that include the plot. Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 , ): \"\"\" Plots distribution of the metric. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. Returns (list(matplotlib.axes)): Axes that include the plot. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ], ), ) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = \"Train {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = \"Test {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( \"Distributions {} \" . format ( metric )) axs [ axis_index ] . legend ( loc = \"upper right\" ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = \"Delta {} \" . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( \"Distributions delta {} \" . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = \"upper right\" ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = \" {} score\" . format ( metric ), ylabel = \"Results count\" ) if show : plt . show () else : plt . close () return axs","title":"plot()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility","text":"Estimation of volatility of metrics depending on the seed used to split the data. At every iteration it splits the data into train and test set using a different stratified split and volatility of the metrics is calculated. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import SplitSeedVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = SplitSeedVolatility ( clf , iterations = 10 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot ()","title":"SplitSeedVolatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply, default is None. Available options: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. None verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/metric_volatility/volatility.py def __init__ ( self , clf , iterations = 1000 , scoring = \"roc_auc\" , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply, default is None. Available options: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" super () . __init__ ( clf = clf , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , iterations = iterations , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , stats_tests_to_apply = stats_tests_to_apply , verbose = verbose , random_state = random_state , )","title":"__init__()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.compute","text":"Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . _check_if_fitted () if self . report is None : raise ( ValueError ( \"Report is None, thus it has not been computed by fit method. Please extend the \" \"BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()\" ) ) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ]","title":"compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.fit","text":"Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.Series or numpy.ndarray Array with targets. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Returns: Type Description (TrainTestVolatility) Fitted object. Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y , column_names = None ): \"\"\" Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.Series or numpy.ndarray): Array with targets. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: (TrainTestVolatility): Fitted object. \"\"\" super () . fit () self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations )) . astype ( int ) if self . random_state : random_seeds = random_seeds * self . random_state if self . verbose > 0 : random_seeds = tqdm ( random_seeds ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = self . X , y = self . y , clf = self . clf , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction , ) for split_seed in random_seeds ) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () return self","title":"fit()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.fit_compute","text":"Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.plot","text":"Plots distribution of the metric. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Returns (list(matplotlib.axes)): Axes that include the plot. Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 , ): \"\"\" Plots distribution of the metric. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. Returns (list(matplotlib.axes)): Axes that include the plot. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ], ), ) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = \"Train {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = \"Test {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( \"Distributions {} \" . format ( metric )) axs [ axis_index ] . legend ( loc = \"upper right\" ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = \"Delta {} \" . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( \"Distributions delta {} \" . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = \"upper right\" ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = \" {} score\" . format ( metric ), ylabel = \"Results count\" ) if show : plt . show () else : plt . close () return axs","title":"plot()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility","text":"Estimation of volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. The class allows for choosing whether at each iteration the train test split should be the same or different, whether and how the train and test sets should be sampled. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import TrainTestVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = TrainTestVolatility ( clf , iterations = 10 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot ()","title":"TrainTestVolatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' sample_train_test_split_seed bool Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. True train_sampling_type str String indicating what type of sampling should be applied on train set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on train data, 'subsample' indicates that sampling without repetition will be performed on train data. None test_sampling_type str String indicating what type of sampling should be applied on test set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on test data, 'subsample' indicates that sampling without repetition will be performed on test data. None train_sampling_fraction float Fraction of train data sampled, if sample_train_type is not None. Default value is 1. 1 test_sampling_fraction float Fraction of test data sampled, if sample_test_type is not None. Default value is 1. 1 test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply, default is None. Available options: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. None verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/metric_volatility/volatility.py def __init__ ( self , clf , iterations = 1000 , scoring = \"roc_auc\" , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers, optional): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. sample_train_test_split_seed (bool, optional): Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. train_sampling_type (str, optional): String indicating what type of sampling should be applied on train set: - `None` indicates that no additional sampling is done after splitting data, - `'bootstrap'` indicates that sampling with replacement will be performed on train data, - `'subsample'` indicates that sampling without repetition will be performed on train data. test_sampling_type (str, optional): String indicating what type of sampling should be applied on test set: - `None` indicates that no additional sampling is done after splitting data, - `'bootstrap'` indicates that sampling with replacement will be performed on test data, - `'subsample'` indicates that sampling without repetition will be performed on test data. train_sampling_fraction (float, optional): Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction (float, optional): Fraction of test data sampled, if sample_test_type is not None. Default value is 1. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply, default is None. Available options: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" super () . __init__ ( clf = clf , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , stats_tests_to_apply = stats_tests_to_apply , verbose = verbose , random_state = random_state , ) self . iterations = iterations self . train_sampling_type = train_sampling_type self . test_sampling_type = test_sampling_type self . sample_train_test_split_seed = sample_train_test_split_seed self . train_sampling_fraction = train_sampling_fraction self . test_sampling_fraction = test_sampling_fraction check_sampling_input ( train_sampling_type , train_sampling_fraction , \"train\" ) check_sampling_input ( test_sampling_type , test_sampling_fraction , \"test\" )","title":"__init__()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.compute","text":"Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . _check_if_fitted () if self . report is None : raise ( ValueError ( \"Report is None, thus it has not been computed by fit method. Please extend the \" \"BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()\" ) ) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ]","title":"compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.fit","text":"Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.Series or numpy.ndarray Array with targets. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Returns: Type Description (TrainTestVolatility) Fitted object. Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y , column_names = None ): \"\"\" Fit. Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.Series or numpy.ndarray): Array with targets. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: (TrainTestVolatility): Fitted object. \"\"\" super () . fit () self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations )) . astype ( int ) if self . random_state : random_seeds = random_seeds * self . random_state if self . verbose > 0 : random_seeds = tqdm ( random_seeds ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = self . X , y = self . y , clf = self . clf , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction , ) for split_seed in random_seeds ) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () return self","title":"fit()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.fit_compute","text":"Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: Type Description (pandas.Dataframe) Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Fit compute. Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Takes as arguments the same parameters as fit() method. Returns: (pandas.Dataframe): Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.plot","text":"Plots distribution of the metric. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Returns (list(matplotlib.axes)): Axes that include the plot. Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , show = True , height_per_subplot = 5 , width_per_subplot = 5 , ): \"\"\" Plots distribution of the metric. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. Returns (list(matplotlib.axes)): Axes that include the plot. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ], ), ) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = \"Train {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = \"Test {} \" . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( \"Distributions {} \" . format ( metric )) axs [ axis_index ] . legend ( loc = \"upper right\" ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = \"Delta {} \" . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( \"Distributions delta {} \" . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = \"upper right\" ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = \" {} score\" . format ( metric ), ylabel = \"Results count\" ) if show : plt . show () else : plt . close () return axs","title":"plot()"},{"location":"api/model_interpret.html","text":"Model Interpretation using SHAP \u00b6 The aim of this module is to provide tools for model interpretation using the SHAP library. The class below is a convenience wrapper that implements multiple plots for tree-based & linear models. ShapModelInterpreter \u00b6 This class is a wrapper that allows to easily analyse a model's features. It allows us to plot SHAP feature importance, SHAP summary plot and SHAP dependence plots. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from probatus.interpret import ShapModelInterpreter import numpy as np import pandas as pd feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 5000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf . fit ( X_train , y_train ) # Train ShapModelInterpreter shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test ) # Make plots ax1 = shap_interpreter . plot ( 'importance' ) ax2 = shap_interpreter . plot ( 'summary' ) ax3 = shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' , 'f2' ]) ax4 = shap_interpreter . plot ( 'sample' , samples_index = [ X_test . index . tolist ()[ 0 ]]) __init__ ( self , clf , scoring = 'roc_auc' , verbose = 0 ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf binary classifier Model fitted on X_train. required scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' verbose int Controls verbosity of the output: 0 - niether prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 Source code in probatus/interpret/model_interpret.py def __init__ ( self , clf , scoring = \"roc_auc\" , verbose = 0 ): \"\"\" Initializes the class. Args: clf (binary classifier): Model fitted on X_train. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. verbose (int, optional): Controls verbosity of the output: - 0 - niether prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). \"\"\" self . clf = clf self . scorer = get_single_scorer ( scoring ) self . verbose = verbose compute ( self , return_scores = False ) \u00b6 Computes the DataFrame that presents the importance of each feature. Parameters: Name Type Description Default return_scores bool Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. False Returns: Type Description (pd.DataFrame or tuple(pd.DataFrame, float, float)) Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. Source code in probatus/interpret/model_interpret.py def compute ( self , return_scores = False ): \"\"\" Computes the DataFrame that presents the importance of each feature. Args: return_scores (bool, optional): Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. Returns: (pd.DataFrame or tuple(pd.DataFrame, float, float)): Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. \"\"\" self . _check_if_fitted () # Compute SHAP importance self . importance_df_train = calculate_shap_importance ( self . shap_values_train , self . column_names , output_columns_suffix = \"_train\" ) self . importance_df_test = calculate_shap_importance ( self . shap_values_test , self . column_names , output_columns_suffix = \"_test\" ) # Concatenate the train and test, sort by test set importance and reorder the columns self . importance_df = pd . concat ([ self . importance_df_train , self . importance_df_test ], axis = 1 ) . sort_values ( \"mean_abs_shap_value_test\" , ascending = False )[ [ \"mean_abs_shap_value_test\" , \"mean_abs_shap_value_train\" , \"mean_shap_value_test\" , \"mean_shap_value_train\" , ] ] if return_scores : return self . importance_df , self . train_score , self . test_score else : return self . importance_df fit ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , ** shap_kwargs ) \u00b6 Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Source code in probatus/interpret/model_interpret.py def fit ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , ** shap_kwargs ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. \"\"\" self . X_train , self . column_names = preprocess_data ( X_train , X_name = \"X_train\" , column_names = column_names , verbose = self . verbose ) self . X_test , _ = preprocess_data ( X_test , X_name = \"X_test\" , column_names = column_names , verbose = self . verbose ) self . y_train = preprocess_labels ( y_train , y_name = \"y_train\" , index = self . X_train . index , verbose = self . verbose ) self . y_test = preprocess_labels ( y_test , y_name = \"y_test\" , index = self . X_test . index , verbose = self . verbose ) # Set class names self . class_names = class_names if self . class_names is None : self . class_names = [ \"Negative Class\" , \"Positive Class\" ] # Calculate Metrics self . train_score = self . scorer . score ( self . clf , self . X_train , self . y_train ) self . test_score = self . scorer . score ( self . clf , self . X_test , self . y_test ) self . results_text = ( f \"Train { self . scorer . metric_name } : { np . round ( self . train_score , 3 ) } , \\n \" f \"Test { self . scorer . metric_name } : { np . round ( self . test_score , 3 ) } .\" ) ( self . shap_values_train , self . expected_value_train , self . tdp_train ,) = self . _prep_shap_related_variables ( clf = self . clf , X = self . X_train , y = self . y_train , column_names = self . column_names , class_names = self . class_names , verbose = self . verbose , ** shap_kwargs , ) ( self . shap_values_test , self . expected_value_test , self . tdp_test ,) = self . _prep_shap_related_variables ( clf = self . clf , X = self . X_test , y = self . y_test , column_names = self . column_names , class_names = self . class_names , verbose = self . verbose , ** shap_kwargs , ) self . fitted = True fit_compute ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , return_scores = False , ** shap_kwargs ) \u00b6 Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None return_scores bool Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. False **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame or tuple(pd.DataFrame, float, float)) Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. Source code in probatus/interpret/model_interpret.py def fit_compute ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , return_scores = False , ** shap_kwargs , ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. return_scores (bool, optional): Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame or tuple(pd.DataFrame, float, float)): Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. \"\"\" self . fit ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , column_names = column_names , class_names = class_names , ** shap_kwargs , ) return self . compute () plot ( self , plot_type , target_set = 'test' , target_columns = None , samples_index = None , show = True , ** plot_kwargs ) \u00b6 Plots the appropriate SHAP plot. Parameters: Name Type Description Default plot_type str One of the following: 'importance' : Feature importance plot, SHAP bar summary plot 'summary' : SHAP Summary plot 'dependence' : Dependence plot for each feature 'sample' : Explanation of a given sample in the test data required target_set str The set for which the plot should be generated, either train or test set. We recommend using test set, because it is not biased by model training. The train set plots are mainly used to compare with the test set plots, whether there are significant differences, which indicate shift in data distribution. 'test' target_columns None, str or list of str List of features names, for which the plots should be generated. If None, all features will be plotted. None samples_index None, int, list or pd.Index Index of samples to be explained if the plot_type=sample . None show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **plot_kwargs Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.DependencePlotter.plot method. {} Returns: Type Description (matplotlib.axes or list(matplotlib.axes)) An Axes with the plot, or list of axes when multiple plots are returned. Source code in probatus/interpret/model_interpret.py def plot ( self , plot_type , target_set = \"test\" , target_columns = None , samples_index = None , show = True , ** plot_kwargs ): \"\"\" Plots the appropriate SHAP plot. Args: plot_type (str): One of the following: - `'importance'`: Feature importance plot, SHAP bar summary plot - `'summary'`: SHAP Summary plot - `'dependence'`: Dependence plot for each feature - `'sample'`: Explanation of a given sample in the test data target_set (str, optional): The set for which the plot should be generated, either `train` or `test` set. We recommend using test set, because it is not biased by model training. The train set plots are mainly used to compare with the test set plots, whether there are significant differences, which indicate shift in data distribution. target_columns (None, str or list of str, optional): List of features names, for which the plots should be generated. If None, all features will be plotted. samples_index (None, int, list or pd.Index, optional): Index of samples to be explained if the `plot_type=sample`. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **plot_kwargs: Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.DependencePlotter.plot method. Returns: (matplotlib.axes or list(matplotlib.axes)): An Axes with the plot, or list of axes when multiple plots are returned. \"\"\" # Choose correct columns if target_columns is None : target_columns = self . column_names target_columns = assure_list_of_strings ( target_columns , \"target_columns\" ) target_columns_indices = [ self . column_names . index ( target_column ) for target_column in target_columns ] # Choose the correct dataset if target_set == \"test\" : target_X = self . X_test target_shap_values = self . shap_values_test target_tdp = self . tdp_train target_expected_value = self . expected_value_train elif target_set == \"train\" : target_X = self . X_train target_shap_values = self . shap_values_train target_tdp = self . tdp_test target_expected_value = self . expected_value_test else : raise ( ValueError ( 'The target_set parameter can be either \"train\" or \"test\".' )) if plot_type in [ \"importance\" , \"summary\" ]: target_X = target_X [ target_columns ] target_shap_values = target_shap_values [:, target_columns_indices ] # Set summary plot settings if plot_type == \"importance\" : plot_type = \"bar\" plot_title = f \"SHAP Feature Importance for { target_set } set\" else : plot_type = \"dot\" plot_title = f \"SHAP Summary plot for { target_set } set\" shap . summary_plot ( target_shap_values , target_X , plot_type = plot_type , class_names = self . class_names , show = False , ** plot_kwargs , ) ax = plt . gca () ax . set_title ( plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = \"axes fraction\" , textcoords = \"offset points\" , va = \"top\" , ) if show : plt . show () else : plt . close () elif plot_type == \"dependence\" : ax = [] for feature_name in target_columns : ax . append ( target_tdp . plot ( feature = feature_name , figsize = ( 10 , 7 ), show = show , ** plot_kwargs )) elif plot_type == \"sample\" : # Ensure the correct samples_index type if samples_index is None : raise ( ValueError ( \"For sample plot, you need to specify the samples_index be plotted plot\" )) elif isinstance ( samples_index , int ) or isinstance ( samples_index , str ): samples_index = [ samples_index ] elif not ( isinstance ( samples_index , list ) or isinstance ( samples_index , pd . Index )): raise ( TypeError ( \"sample_index must be one of the following: int, str, list or pd.Index\" )) ax = [] for sample_index in samples_index : sample_loc = target_X . index . get_loc ( sample_index ) shap . plots . _waterfall . waterfall_legacy ( target_expected_value , target_shap_values [ sample_loc , :], target_X . loc [ sample_index ], show = False , ** plot_kwargs , ) plot_title = f \"SHAP Sample Explanation of { target_set } sample for index= { sample_index } \" current_ax = plt . gca () current_ax . set_title ( plot_title ) ax . append ( current_ax ) if show : plt . show () else : plt . close () else : raise ValueError ( \"Wrong plot type, select from 'importance', 'summary', or 'dependence'\" ) if isinstance ( ax , list ) and len ( ax ) == 1 : ax = ax [ 0 ] return ax DependencePlotter \u00b6 Plotter used to plot SHAP dependence plot together with the target rates. Currently it supports tree-based and linear models. Parameters: Name Type Description Default model classifier for which interpretation is done. required Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.interpret import DependencePlotter X , y = make_classification ( n_samples = 15 , n_features = 3 , n_informative = 3 , n_redundant = 0 , random_state = 42 ) clf = RandomForestClassifier () . fit ( X , y ) bdp = DependencePlotter ( clf ) shap_values = bdp . fit_compute ( X , y ) bdp . plot ( feature = 2 , type_binning = 'simple' ) __init__ ( self , clf , verbose = 0 ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings regarding data properties are shown (excluding SHAP warnings) 51 - 100 - shows most important warnings, prints of the feature removal process above 100 - presents all prints and all warnings (including SHAP warnings). 0 Source code in probatus/interpret/shap_dependence.py def __init__ ( self , clf , verbose = 0 ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings regarding data properties are shown (excluding SHAP warnings) - 51 - 100 - shows most important warnings, prints of the feature removal process - above 100 - presents all prints and all warnings (including SHAP warnings). \"\"\" self . clf = clf self . verbose = verbose compute ( self ) \u00b6 Computes the report returned to the user, namely the SHAP values generated on the dataset. Returns: Type Description (pd.DataFrame) SHAP Values for X. Source code in probatus/interpret/shap_dependence.py def compute ( self ): \"\"\" Computes the report returned to the user, namely the SHAP values generated on the dataset. Returns: (pd.DataFrame): SHAP Values for X. \"\"\" self . _check_if_fitted () return self . shap_vals_df fit ( self , X , y , column_names = None , class_names = None , precalc_shap = None , ** shap_kwargs ) \u00b6 Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed. Parameters: Name Type Description Default X pd.DataFrame input variables. required y pd.Series target variable. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None precalc_shap Optional, None or np.array Precalculated shap values, If provided they don't need to be computed. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Source code in probatus/interpret/shap_dependence.py def fit ( self , X , y , column_names = None , class_names = None , precalc_shap = None , ** shap_kwargs ): \"\"\" Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed. Args: X (pd.DataFrame): input variables. y (pd.Series): target variable. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. precalc_shap (Optional, None or np.array): Precalculated shap values, If provided they don't need to be computed. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. \"\"\" self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) # Set class names self . class_names = class_names if self . class_names is None : self . class_names = [ \"Negative Class\" , \"Positive Class\" ] self . shap_vals_df = shap_to_df ( self . clf , self . X , precalc_shap = precalc_shap , verbose = self . verbose , ** shap_kwargs ) self . fitted = True return self fit_compute ( self , X , y , column_names = None , class_names = None , precalc_shap = None , ** shap_kwargs ) \u00b6 Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None precalc_shap Optional, None or np.array Precalculated shap values, If provided they don't need to be computed. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame) SHAP Values for X. Source code in probatus/interpret/shap_dependence.py def fit_compute ( self , X , y , column_names = None , class_names = None , precalc_shap = None , ** shap_kwargs ): \"\"\" Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. precalc_shap (Optional, None or np.array): Precalculated shap values, If provided they don't need to be computed. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame): SHAP Values for X. \"\"\" self . fit ( X , y , column_names = column_names , class_names = class_names , precalc_shap = precalc_shap , ** shap_kwargs ) return self . compute () plot ( self , feature , figsize = ( 15 , 10 ), bins = 10 , type_binning = 'simple' , show = True , min_q = 0 , max_q = 1 ) \u00b6 Plots the shap values for data points for a given feature, as well as the target rate and values distribution. Parameters: Name Type Description Default feature str or int Feature name of the feature to be analyzed. required figsize float, float) Tuple specifying size (width, height) of resulting figure in inches. (15, 10) bins int or list[float] Number of bins or boundaries of bins (supplied in list) for target-rate plot. 10 type_binning {'simple', 'agglomerative', 'quantile'} Type of binning to be used in target-rate plot (see :mod: binning for more information). 'simple' show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True min_q float Optional minimum quantile from which to consider values, used for plotting under outliers. 0 max_q float Optional maximum quantile until which data points are considered, used for plotting under outliers. 1 Returns (list(matplotlib.axes)): List of axes that include the plots. Source code in probatus/interpret/shap_dependence.py def plot ( self , feature , figsize = ( 15 , 10 ), bins = 10 , type_binning = \"simple\" , show = True , min_q = 0 , max_q = 1 , ): \"\"\" Plots the shap values for data points for a given feature, as well as the target rate and values distribution. Args: feature (str or int): Feature name of the feature to be analyzed. figsize ((float, float), optional): Tuple specifying size (width, height) of resulting figure in inches. bins (int or list[float]): Number of bins or boundaries of bins (supplied in list) for target-rate plot. type_binning ({'simple', 'agglomerative', 'quantile'}): Type of binning to be used in target-rate plot (see :mod:`binning` for more information). show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. min_q (float, optional): Optional minimum quantile from which to consider values, used for plotting under outliers. max_q (float, optional): Optional maximum quantile until which data points are considered, used for plotting under outliers. Returns (list(matplotlib.axes)): List of axes that include the plots. \"\"\" self . _check_if_fitted () if min_q >= max_q : raise ValueError ( \"min_q must be smaller than max_q\" ) if feature not in self . X . columns : raise ValueError ( \"Feature not recognized\" ) if type_binning not in [ \"simple\" , \"agglomerative\" , \"quantile\" ]: raise ValueError ( \"Select one of the following binning methods: 'simple', 'agglomerative', 'quantile'\" ) self . min_q , self . max_q = min_q , max_q _ = plt . figure ( 1 , figsize = figsize ) ax1 = plt . subplot2grid (( 3 , 1 ), ( 0 , 0 ), rowspan = 2 ) ax2 = plt . subplot2grid (( 3 , 1 ), ( 2 , 0 )) self . _dependence_plot ( feature = feature , ax = ax1 ) self . _target_rate_plot ( feature = feature , bins = bins , type_binning = type_binning , ax = ax2 ) ax2 . set_xlim ( ax1 . get_xlim ()) if show : plt . show () else : plt . close () return [ ax1 , ax2 ]","title":"probatus.interpret"},{"location":"api/model_interpret.html#model-interpretation-using-shap","text":"The aim of this module is to provide tools for model interpretation using the SHAP library. The class below is a convenience wrapper that implements multiple plots for tree-based & linear models.","title":"Model Interpretation using SHAP"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter","text":"This class is a wrapper that allows to easily analyse a model's features. It allows us to plot SHAP feature importance, SHAP summary plot and SHAP dependence plots. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from probatus.interpret import ShapModelInterpreter import numpy as np import pandas as pd feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 5000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf . fit ( X_train , y_train ) # Train ShapModelInterpreter shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test ) # Make plots ax1 = shap_interpreter . plot ( 'importance' ) ax2 = shap_interpreter . plot ( 'summary' ) ax3 = shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' , 'f2' ]) ax4 = shap_interpreter . plot ( 'sample' , samples_index = [ X_test . index . tolist ()[ 0 ]])","title":"ShapModelInterpreter"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf binary classifier Model fitted on X_train. required scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. 'roc_auc' verbose int Controls verbosity of the output: 0 - niether prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 Source code in probatus/interpret/model_interpret.py def __init__ ( self , clf , scoring = \"roc_auc\" , verbose = 0 ): \"\"\" Initializes the class. Args: clf (binary classifier): Model fitted on X_train. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. verbose (int, optional): Controls verbosity of the output: - 0 - niether prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). \"\"\" self . clf = clf self . scorer = get_single_scorer ( scoring ) self . verbose = verbose","title":"__init__()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.compute","text":"Computes the DataFrame that presents the importance of each feature. Parameters: Name Type Description Default return_scores bool Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. False Returns: Type Description (pd.DataFrame or tuple(pd.DataFrame, float, float)) Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. Source code in probatus/interpret/model_interpret.py def compute ( self , return_scores = False ): \"\"\" Computes the DataFrame that presents the importance of each feature. Args: return_scores (bool, optional): Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. Returns: (pd.DataFrame or tuple(pd.DataFrame, float, float)): Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. \"\"\" self . _check_if_fitted () # Compute SHAP importance self . importance_df_train = calculate_shap_importance ( self . shap_values_train , self . column_names , output_columns_suffix = \"_train\" ) self . importance_df_test = calculate_shap_importance ( self . shap_values_test , self . column_names , output_columns_suffix = \"_test\" ) # Concatenate the train and test, sort by test set importance and reorder the columns self . importance_df = pd . concat ([ self . importance_df_train , self . importance_df_test ], axis = 1 ) . sort_values ( \"mean_abs_shap_value_test\" , ascending = False )[ [ \"mean_abs_shap_value_test\" , \"mean_abs_shap_value_train\" , \"mean_shap_value_test\" , \"mean_shap_value_train\" , ] ] if return_scores : return self . importance_df , self . train_score , self . test_score else : return self . importance_df","title":"compute()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.fit","text":"Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Source code in probatus/interpret/model_interpret.py def fit ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , ** shap_kwargs ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. \"\"\" self . X_train , self . column_names = preprocess_data ( X_train , X_name = \"X_train\" , column_names = column_names , verbose = self . verbose ) self . X_test , _ = preprocess_data ( X_test , X_name = \"X_test\" , column_names = column_names , verbose = self . verbose ) self . y_train = preprocess_labels ( y_train , y_name = \"y_train\" , index = self . X_train . index , verbose = self . verbose ) self . y_test = preprocess_labels ( y_test , y_name = \"y_test\" , index = self . X_test . index , verbose = self . verbose ) # Set class names self . class_names = class_names if self . class_names is None : self . class_names = [ \"Negative Class\" , \"Positive Class\" ] # Calculate Metrics self . train_score = self . scorer . score ( self . clf , self . X_train , self . y_train ) self . test_score = self . scorer . score ( self . clf , self . X_test , self . y_test ) self . results_text = ( f \"Train { self . scorer . metric_name } : { np . round ( self . train_score , 3 ) } , \\n \" f \"Test { self . scorer . metric_name } : { np . round ( self . test_score , 3 ) } .\" ) ( self . shap_values_train , self . expected_value_train , self . tdp_train ,) = self . _prep_shap_related_variables ( clf = self . clf , X = self . X_train , y = self . y_train , column_names = self . column_names , class_names = self . class_names , verbose = self . verbose , ** shap_kwargs , ) ( self . shap_values_test , self . expected_value_test , self . tdp_test ,) = self . _prep_shap_related_variables ( clf = self . clf , X = self . X_test , y = self . y_test , column_names = self . column_names , class_names = self . class_names , verbose = self . verbose , ** shap_kwargs , ) self . fitted = True","title":"fit()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.fit_compute","text":"Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None return_scores bool Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. False **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame or tuple(pd.DataFrame, float, float)) Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. Source code in probatus/interpret/model_interpret.py def fit_compute ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , return_scores = False , ** shap_kwargs , ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. return_scores (bool, optional): Flag indicating whether the method should return the train and test score of the model, together with the model interpretation report. If true, the output of this method is a tuple of DataFrame, float, float. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame or tuple(pd.DataFrame, float, float)): Dataframe with SHAP feature importance, or tuple containing the dataframe, train and test scores of the model. \"\"\" self . fit ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , column_names = column_names , class_names = class_names , ** shap_kwargs , ) return self . compute ()","title":"fit_compute()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.plot","text":"Plots the appropriate SHAP plot. Parameters: Name Type Description Default plot_type str One of the following: 'importance' : Feature importance plot, SHAP bar summary plot 'summary' : SHAP Summary plot 'dependence' : Dependence plot for each feature 'sample' : Explanation of a given sample in the test data required target_set str The set for which the plot should be generated, either train or test set. We recommend using test set, because it is not biased by model training. The train set plots are mainly used to compare with the test set plots, whether there are significant differences, which indicate shift in data distribution. 'test' target_columns None, str or list of str List of features names, for which the plots should be generated. If None, all features will be plotted. None samples_index None, int, list or pd.Index Index of samples to be explained if the plot_type=sample . None show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **plot_kwargs Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.DependencePlotter.plot method. {} Returns: Type Description (matplotlib.axes or list(matplotlib.axes)) An Axes with the plot, or list of axes when multiple plots are returned. Source code in probatus/interpret/model_interpret.py def plot ( self , plot_type , target_set = \"test\" , target_columns = None , samples_index = None , show = True , ** plot_kwargs ): \"\"\" Plots the appropriate SHAP plot. Args: plot_type (str): One of the following: - `'importance'`: Feature importance plot, SHAP bar summary plot - `'summary'`: SHAP Summary plot - `'dependence'`: Dependence plot for each feature - `'sample'`: Explanation of a given sample in the test data target_set (str, optional): The set for which the plot should be generated, either `train` or `test` set. We recommend using test set, because it is not biased by model training. The train set plots are mainly used to compare with the test set plots, whether there are significant differences, which indicate shift in data distribution. target_columns (None, str or list of str, optional): List of features names, for which the plots should be generated. If None, all features will be plotted. samples_index (None, int, list or pd.Index, optional): Index of samples to be explained if the `plot_type=sample`. show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **plot_kwargs: Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.DependencePlotter.plot method. Returns: (matplotlib.axes or list(matplotlib.axes)): An Axes with the plot, or list of axes when multiple plots are returned. \"\"\" # Choose correct columns if target_columns is None : target_columns = self . column_names target_columns = assure_list_of_strings ( target_columns , \"target_columns\" ) target_columns_indices = [ self . column_names . index ( target_column ) for target_column in target_columns ] # Choose the correct dataset if target_set == \"test\" : target_X = self . X_test target_shap_values = self . shap_values_test target_tdp = self . tdp_train target_expected_value = self . expected_value_train elif target_set == \"train\" : target_X = self . X_train target_shap_values = self . shap_values_train target_tdp = self . tdp_test target_expected_value = self . expected_value_test else : raise ( ValueError ( 'The target_set parameter can be either \"train\" or \"test\".' )) if plot_type in [ \"importance\" , \"summary\" ]: target_X = target_X [ target_columns ] target_shap_values = target_shap_values [:, target_columns_indices ] # Set summary plot settings if plot_type == \"importance\" : plot_type = \"bar\" plot_title = f \"SHAP Feature Importance for { target_set } set\" else : plot_type = \"dot\" plot_title = f \"SHAP Summary plot for { target_set } set\" shap . summary_plot ( target_shap_values , target_X , plot_type = plot_type , class_names = self . class_names , show = False , ** plot_kwargs , ) ax = plt . gca () ax . set_title ( plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = \"axes fraction\" , textcoords = \"offset points\" , va = \"top\" , ) if show : plt . show () else : plt . close () elif plot_type == \"dependence\" : ax = [] for feature_name in target_columns : ax . append ( target_tdp . plot ( feature = feature_name , figsize = ( 10 , 7 ), show = show , ** plot_kwargs )) elif plot_type == \"sample\" : # Ensure the correct samples_index type if samples_index is None : raise ( ValueError ( \"For sample plot, you need to specify the samples_index be plotted plot\" )) elif isinstance ( samples_index , int ) or isinstance ( samples_index , str ): samples_index = [ samples_index ] elif not ( isinstance ( samples_index , list ) or isinstance ( samples_index , pd . Index )): raise ( TypeError ( \"sample_index must be one of the following: int, str, list or pd.Index\" )) ax = [] for sample_index in samples_index : sample_loc = target_X . index . get_loc ( sample_index ) shap . plots . _waterfall . waterfall_legacy ( target_expected_value , target_shap_values [ sample_loc , :], target_X . loc [ sample_index ], show = False , ** plot_kwargs , ) plot_title = f \"SHAP Sample Explanation of { target_set } sample for index= { sample_index } \" current_ax = plt . gca () current_ax . set_title ( plot_title ) ax . append ( current_ax ) if show : plt . show () else : plt . close () else : raise ValueError ( \"Wrong plot type, select from 'importance', 'summary', or 'dependence'\" ) if isinstance ( ax , list ) and len ( ax ) == 1 : ax = ax [ 0 ] return ax","title":"plot()"},{"location":"api/model_interpret.html#probatus.interpret.shap_dependence.DependencePlotter","text":"Plotter used to plot SHAP dependence plot together with the target rates. Currently it supports tree-based and linear models. Parameters: Name Type Description Default model classifier for which interpretation is done. required Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.interpret import DependencePlotter X , y = make_classification ( n_samples = 15 , n_features = 3 , n_informative = 3 , n_redundant = 0 , random_state = 42 ) clf = RandomForestClassifier () . fit ( X , y ) bdp = DependencePlotter ( clf ) shap_values = bdp . fit_compute ( X , y ) bdp . plot ( feature = 2 , type_binning = 'simple' )","title":"DependencePlotter"},{"location":"api/model_interpret.html#probatus.interpret.shap_dependence.DependencePlotter.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings regarding data properties are shown (excluding SHAP warnings) 51 - 100 - shows most important warnings, prints of the feature removal process above 100 - presents all prints and all warnings (including SHAP warnings). 0 Source code in probatus/interpret/shap_dependence.py def __init__ ( self , clf , verbose = 0 ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings regarding data properties are shown (excluding SHAP warnings) - 51 - 100 - shows most important warnings, prints of the feature removal process - above 100 - presents all prints and all warnings (including SHAP warnings). \"\"\" self . clf = clf self . verbose = verbose","title":"__init__()"},{"location":"api/model_interpret.html#probatus.interpret.shap_dependence.DependencePlotter.compute","text":"Computes the report returned to the user, namely the SHAP values generated on the dataset. Returns: Type Description (pd.DataFrame) SHAP Values for X. Source code in probatus/interpret/shap_dependence.py def compute ( self ): \"\"\" Computes the report returned to the user, namely the SHAP values generated on the dataset. Returns: (pd.DataFrame): SHAP Values for X. \"\"\" self . _check_if_fitted () return self . shap_vals_df","title":"compute()"},{"location":"api/model_interpret.html#probatus.interpret.shap_dependence.DependencePlotter.fit","text":"Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed. Parameters: Name Type Description Default X pd.DataFrame input variables. required y pd.Series target variable. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None precalc_shap Optional, None or np.array Precalculated shap values, If provided they don't need to be computed. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Source code in probatus/interpret/shap_dependence.py def fit ( self , X , y , column_names = None , class_names = None , precalc_shap = None , ** shap_kwargs ): \"\"\" Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed. Args: X (pd.DataFrame): input variables. y (pd.Series): target variable. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. precalc_shap (Optional, None or np.array): Precalculated shap values, If provided they don't need to be computed. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. \"\"\" self . X , self . column_names = preprocess_data ( X , X_name = \"X\" , column_names = column_names , verbose = self . verbose ) self . y = preprocess_labels ( y , y_name = \"y\" , index = self . X . index , verbose = self . verbose ) # Set class names self . class_names = class_names if self . class_names is None : self . class_names = [ \"Negative Class\" , \"Positive Class\" ] self . shap_vals_df = shap_to_df ( self . clf , self . X , precalc_shap = precalc_shap , verbose = self . verbose , ** shap_kwargs ) self . fitted = True return self","title":"fit()"},{"location":"api/model_interpret.html#probatus.interpret.shap_dependence.DependencePlotter.fit_compute","text":"Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required column_names None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None precalc_shap Optional, None or np.array Precalculated shap values, If provided they don't need to be computed. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (pd.DataFrame) SHAP Values for X. Source code in probatus/interpret/shap_dependence.py def fit_compute ( self , X , y , column_names = None , class_names = None , precalc_shap = None , ** shap_kwargs ): \"\"\" Fits the plotter to the model and data by computing the shap values. If the shap_values are passed, they do not need to be computed Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. column_names (None, or list of str, optional): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (None, or list of str, optional): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. precalc_shap (Optional, None or np.array): Precalculated shap values, If provided they don't need to be computed. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (pd.DataFrame): SHAP Values for X. \"\"\" self . fit ( X , y , column_names = column_names , class_names = class_names , precalc_shap = precalc_shap , ** shap_kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/model_interpret.html#probatus.interpret.shap_dependence.DependencePlotter.plot","text":"Plots the shap values for data points for a given feature, as well as the target rate and values distribution. Parameters: Name Type Description Default feature str or int Feature name of the feature to be analyzed. required figsize float, float) Tuple specifying size (width, height) of resulting figure in inches. (15, 10) bins int or list[float] Number of bins or boundaries of bins (supplied in list) for target-rate plot. 10 type_binning {'simple', 'agglomerative', 'quantile'} Type of binning to be used in target-rate plot (see :mod: binning for more information). 'simple' show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True min_q float Optional minimum quantile from which to consider values, used for plotting under outliers. 0 max_q float Optional maximum quantile until which data points are considered, used for plotting under outliers. 1 Returns (list(matplotlib.axes)): List of axes that include the plots. Source code in probatus/interpret/shap_dependence.py def plot ( self , feature , figsize = ( 15 , 10 ), bins = 10 , type_binning = \"simple\" , show = True , min_q = 0 , max_q = 1 , ): \"\"\" Plots the shap values for data points for a given feature, as well as the target rate and values distribution. Args: feature (str or int): Feature name of the feature to be analyzed. figsize ((float, float), optional): Tuple specifying size (width, height) of resulting figure in inches. bins (int or list[float]): Number of bins or boundaries of bins (supplied in list) for target-rate plot. type_binning ({'simple', 'agglomerative', 'quantile'}): Type of binning to be used in target-rate plot (see :mod:`binning` for more information). show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. min_q (float, optional): Optional minimum quantile from which to consider values, used for plotting under outliers. max_q (float, optional): Optional maximum quantile until which data points are considered, used for plotting under outliers. Returns (list(matplotlib.axes)): List of axes that include the plots. \"\"\" self . _check_if_fitted () if min_q >= max_q : raise ValueError ( \"min_q must be smaller than max_q\" ) if feature not in self . X . columns : raise ValueError ( \"Feature not recognized\" ) if type_binning not in [ \"simple\" , \"agglomerative\" , \"quantile\" ]: raise ValueError ( \"Select one of the following binning methods: 'simple', 'agglomerative', 'quantile'\" ) self . min_q , self . max_q = min_q , max_q _ = plt . figure ( 1 , figsize = figsize ) ax1 = plt . subplot2grid (( 3 , 1 ), ( 0 , 0 ), rowspan = 2 ) ax2 = plt . subplot2grid (( 3 , 1 ), ( 2 , 0 )) self . _dependence_plot ( feature = feature , ax = ax1 ) self . _target_rate_plot ( feature = feature , bins = bins , type_binning = type_binning , ax = ax2 ) ax2 . set_xlim ( ax1 . get_xlim ()) if show : plt . show () else : plt . close () return [ ax1 , ax2 ]","title":"plot()"},{"location":"api/sample_similarity.html","text":"Sample Similarity \u00b6 The goal of sample similarity module is understanding how different two samples are from a multivariate perspective. One of the ways to indicate this is Resemblance Model. Having two datasets - say X1 and X2 - one can analyse how easy it is to recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to the dataset X1, and label 1 to X2 and trains a binary classification model to predict which sample a given row comes from. By looking at the test AUC, one can conclude that the samples have a different distribution if the AUC is significantly higher than 0.5. Furthermore, by analysing feature importance one can understand which of the features have predictive power. The following features are implemented: SHAPImportanceResemblance (Recommended) - The class applies SHAP library, in order to interpret the tree based resemblance model. PermutationImportanceResemblance - The class applies permutation feature importance in order to understand which features the current model relies on the most. The higher the importance of the feature, the more a given feature possibly differs in X2 compared to X1. The importance indicates how much the test AUC drops if a given feature is permuted. PermutationImportanceResemblance \u00b6 This model checks the similarity of two samples. A possible use case is analysis of whether the train sample differs from the test sample, due to e.g. non-stationarity. It assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. If the model achieves a test AUC significantly different than 0.5, it indicates that it is possible to distinguish between the samples, and therefore, the samples differ. Features with a high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import PermutationImportanceResemblance X1 , _ = make_classification ( n_samples = 100 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 100 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) perm = PermutationImportanceResemblance ( clf ) feature_importance = perm . fit_compute ( X1 , X2 ) perm . plot () __init__ ( self , clf , iterations = 100 , scoring = 'roc_auc' , test_prc = 0.25 , n_jobs = 1 , verbose = 0 , random_state = None ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. 100 scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. 'roc_auc' test_prc float Percentage of data used to test the model. By default 0.25 is set. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , clf , iterations = 100 , scoring = \"roc_auc\" , test_prc = 0.25 , n_jobs = 1 , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. test_prc (float, optional): Percentage of data used to test the model. By default 0.25 is set. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" # noqa super () . __init__ ( clf = clf , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , verbose = verbose , random_state = random_state , ) self . iterations = iterations self . iterations_columns = [ \"feature\" , \"importance\" ] self . iterations_results = pd . DataFrame ( columns = self . iterations_columns ) self . plot_x_label = \"Permutation Feature Importance\" self . plot_y_label = \"Feature Name\" self . plot_title = \"Permutation Feature Importance of Resemblance Model\" compute ( self , return_scores = False ) inherited \u00b6 Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False Returns: Type Description (tuple(pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_scores = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. Returns: (tuple(pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_scores : return self . report , self . train_score , self . test_score else : return self . report fit ( self , X1 , X2 , column_names = None , class_names = None ) \u00b6 This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None Returns: Type Description (PermutationImportanceResemblance) Fitted object. Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None , class_names = None ): \"\"\" This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. Returns: (PermutationImportanceResemblance): Fitted object. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names , class_names = class_names ) permutation_result = permutation_importance ( self . clf , self . X_test , self . y_test , scoring = self . scorer . scorer , n_repeats = self . iterations , n_jobs = self . n_jobs , ) # Prepare report self . report_columns = [ \"mean_importance\" , \"std_importance\" ] self . report = pd . DataFrame ( index = self . column_names , columns = self . report_columns , dtype = float ) for feature_index , feature_name in enumerate ( self . column_names ): # Fill in the report self . report . loc [ feature_name , \"mean_importance\" ] = permutation_result [ \"importances_mean\" ][ feature_index ] self . report . loc [ feature_name , \"std_importance\" ] = permutation_result [ \"importances_std\" ][ feature_index ] # Fill in the iterations current_iterations = pd . DataFrame ( np . stack ( [ np . repeat ( feature_name , self . iterations ), permutation_result [ \"importances\" ][ feature_index , :] . reshape (( self . iterations ,)), ], axis = 1 , ), columns = self . iterations_columns , ) self . iterations_results = pd . concat ([ self . iterations_results , current_iterations ]) self . iterations_results [ \"importance\" ] = self . iterations_results [ \"importance\" ] . astype ( float ) # Sort by mean test score of first metric self . report . sort_values ( by = \"mean_importance\" , ascending = False , inplace = True ) return self fit_compute ( self , X1 , X2 , column_names = None , class_names = None , return_scores = False , ** fit_kwargs ) inherited \u00b6 Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False **fit_kwargs In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. {} Returns: Type Description (tuple of (pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , class_names = None , return_scores = False , ** fit_kwargs , ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. **fit_kwargs: In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. Returns: (tuple of (pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , class_names = class_names , ** fit_kwargs ) return self . compute ( return_scores = return_scores ) get_data_splits ( self ) inherited \u00b6 Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test plot ( self , ax = None , top_n = None , show = True , ** plot_kwargs ) \u00b6 Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default ax matplotlib.axes Axes to which the output should be plotted. If not provided new axes are created. None top_n int Number of the most important features to be plotted. By default features are included in the plot. None show bool If True, the plots are shown to the user, otherwise they are not shown. Not showing a plot can be useful when you want to edit the returned axis before showing it. True **plot_kwargs Keyword arguments passed to the matplotlib.plotly.subplots method. {} Returns: Type Description (matplotlib.axes) Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , ax = None , top_n = None , show = True , ** plot_kwargs ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: ax (matplotlib.axes, optional): Axes to which the output should be plotted. If not provided new axes are created. top_n (int, optional): Number of the most important features to be plotted. By default features are included in the plot. show (bool, optional): If True, the plots are shown to the user, otherwise they are not shown. Not showing a plot can be useful when you want to edit the returned axis before showing it. **plot_kwargs: Keyword arguments passed to the matplotlib.plotly.subplots method. Returns: (matplotlib.axes): Axes that include the plot. \"\"\" feature_report = self . compute () self . iterations_results [ \"importance\" ] = self . iterations_results [ \"importance\" ] . astype ( float ) sorted_features = feature_report [ \"mean_importance\" ] . sort_values ( ascending = True ) . index . values if top_n is not None and top_n > 0 : sorted_features = sorted_features [ - top_n :] if ax is None : fig , ax = plt . subplots ( ** plot_kwargs ) for position , feature in enumerate ( sorted_features ): ax . boxplot ( self . iterations_results [ self . iterations_results [ \"feature\" ] == feature ][ \"importance\" ], positions = [ position ], vert = False , ) ax . set_yticks ( range ( position + 1 )) ax . set_yticklabels ( sorted_features ) ax . set_xlabel ( self . plot_x_label ) ax . set_ylabel ( self . plot_y_label ) ax . set_title ( self . plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = \"axes fraction\" , textcoords = \"offset points\" , va = \"top\" , ) if show : plt . show () else : plt . close () return ax SHAPImportanceResemblance \u00b6 This model checks for similarity of two samples. A possible use case is analysis of whether the train sample differs from the test sample, due to e.g. non-stationarity. It assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. If the model achieves test AUC significantly different than 0.5, it indicates that it is possible to distinguish between the samples, and therefore, the samples differ. Features with a high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. This class currently works only with the Tree based models. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import SHAPImportanceResemblance X1 , _ = make_classification ( n_samples = 100 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 100 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) rm = SHAPImportanceResemblance ( clf ) feature_importance = rm . fit_compute ( X1 , X2 ) rm . plot () __init__ ( self , clf , scoring = 'roc_auc' , test_prc = 0.25 , n_jobs = 1 , verbose = 0 , random_state = None ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. 'roc_auc' test_prc float Percentage of data used to test the model. By default 0.25 is set. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , clf , scoring = \"roc_auc\" , test_prc = 0.25 , n_jobs = 1 , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. test_prc (float, optional): Percentage of data used to test the model. By default 0.25 is set. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" # noqa super () . __init__ ( clf = clf , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , verbose = verbose , random_state = random_state , ) self . plot_title = \"SHAP summary plot\" compute ( self , return_scores = False ) inherited \u00b6 Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False Returns: Type Description (tuple(pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_scores = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. Returns: (tuple(pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_scores : return self . report , self . train_score , self . test_score else : return self . report fit ( self , X1 , X2 , column_names = None , class_names = None , ** shap_kwargs ) \u00b6 This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (SHAPImportanceResemblance) Fitted object. Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None , class_names = None , ** shap_kwargs ): \"\"\" This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (SHAPImportanceResemblance): Fitted object. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names , class_names = class_names ) self . shap_values_test = shap_calc ( self . clf , self . X_test , verbose = self . verbose , ** shap_kwargs ) self . report = calculate_shap_importance ( self . shap_values_test , self . column_names ) return self fit_compute ( self , X1 , X2 , column_names = None , class_names = None , return_scores = False , ** fit_kwargs ) inherited \u00b6 Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False **fit_kwargs In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. {} Returns: Type Description (tuple of (pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , class_names = None , return_scores = False , ** fit_kwargs , ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. **fit_kwargs: In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. Returns: (tuple of (pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , class_names = class_names , ** fit_kwargs ) return self . compute ( return_scores = return_scores ) get_data_splits ( self ) inherited \u00b6 Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test get_shap_values ( self ) \u00b6 Gets the SHAP values generated on the test set. Returns: Type Description (np.array) SHAP values generated on the test set. Source code in probatus/sample_similarity/resemblance_model.py def get_shap_values ( self ): \"\"\" Gets the SHAP values generated on the test set. Returns: (np.array): SHAP values generated on the test set. \"\"\" self . _check_if_fitted () return self . shap_values_test plot ( self , plot_type = 'bar' , show = True , ** summary_plot_kwargs ) \u00b6 Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default plot_type str Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", 'bar' show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **summary_plot_kwargs kwargs passed to the shap.summary_plot. {} Returns: Type Description (matplotlib.axes) Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , plot_type = \"bar\" , show = True , ** summary_plot_kwargs ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: plot_type (str, optional): Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **summary_plot_kwargs: kwargs passed to the shap.summary_plot. Returns: (matplotlib.axes): Axes that include the plot. \"\"\" # This line serves as a double check if the object has been fitted self . _check_if_fitted () shap . summary_plot ( self . shap_values_test , self . X_test , plot_type = plot_type , class_names = self . class_names , show = False , ** summary_plot_kwargs , ) ax = plt . gca () ax . set_title ( self . plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = \"axes fraction\" , textcoords = \"offset points\" , va = \"top\" , ) if show : plt . show () else : plt . close () return ax","title":"probatus.sample_similarity"},{"location":"api/sample_similarity.html#sample-similarity","text":"The goal of sample similarity module is understanding how different two samples are from a multivariate perspective. One of the ways to indicate this is Resemblance Model. Having two datasets - say X1 and X2 - one can analyse how easy it is to recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to the dataset X1, and label 1 to X2 and trains a binary classification model to predict which sample a given row comes from. By looking at the test AUC, one can conclude that the samples have a different distribution if the AUC is significantly higher than 0.5. Furthermore, by analysing feature importance one can understand which of the features have predictive power. The following features are implemented: SHAPImportanceResemblance (Recommended) - The class applies SHAP library, in order to interpret the tree based resemblance model. PermutationImportanceResemblance - The class applies permutation feature importance in order to understand which features the current model relies on the most. The higher the importance of the feature, the more a given feature possibly differs in X2 compared to X1. The importance indicates how much the test AUC drops if a given feature is permuted.","title":"Sample Similarity"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance","text":"This model checks the similarity of two samples. A possible use case is analysis of whether the train sample differs from the test sample, due to e.g. non-stationarity. It assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. If the model achieves a test AUC significantly different than 0.5, it indicates that it is possible to distinguish between the samples, and therefore, the samples differ. Features with a high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import PermutationImportanceResemblance X1 , _ = make_classification ( n_samples = 100 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 100 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) perm = PermutationImportanceResemblance ( clf ) feature_importance = perm . fit_compute ( X1 , X2 ) perm . plot ()","title":"PermutationImportanceResemblance"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required iterations int Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. 100 scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. 'roc_auc' test_prc float Percentage of data used to test the model. By default 0.25 is set. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , clf , iterations = 100 , scoring = \"roc_auc\" , test_prc = 0.25 , n_jobs = 1 , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. test_prc (float, optional): Percentage of data used to test the model. By default 0.25 is set. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" # noqa super () . __init__ ( clf = clf , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , verbose = verbose , random_state = random_state , ) self . iterations = iterations self . iterations_columns = [ \"feature\" , \"importance\" ] self . iterations_results = pd . DataFrame ( columns = self . iterations_columns ) self . plot_x_label = \"Permutation Feature Importance\" self . plot_y_label = \"Feature Name\" self . plot_title = \"Permutation Feature Importance of Resemblance Model\"","title":"__init__()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.compute","text":"Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False Returns: Type Description (tuple(pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_scores = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. Returns: (tuple(pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_scores : return self . report , self . train_score , self . test_score else : return self . report","title":"compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.fit","text":"This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None Returns: Type Description (PermutationImportanceResemblance) Fitted object. Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None , class_names = None ): \"\"\" This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. Returns: (PermutationImportanceResemblance): Fitted object. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names , class_names = class_names ) permutation_result = permutation_importance ( self . clf , self . X_test , self . y_test , scoring = self . scorer . scorer , n_repeats = self . iterations , n_jobs = self . n_jobs , ) # Prepare report self . report_columns = [ \"mean_importance\" , \"std_importance\" ] self . report = pd . DataFrame ( index = self . column_names , columns = self . report_columns , dtype = float ) for feature_index , feature_name in enumerate ( self . column_names ): # Fill in the report self . report . loc [ feature_name , \"mean_importance\" ] = permutation_result [ \"importances_mean\" ][ feature_index ] self . report . loc [ feature_name , \"std_importance\" ] = permutation_result [ \"importances_std\" ][ feature_index ] # Fill in the iterations current_iterations = pd . DataFrame ( np . stack ( [ np . repeat ( feature_name , self . iterations ), permutation_result [ \"importances\" ][ feature_index , :] . reshape (( self . iterations ,)), ], axis = 1 , ), columns = self . iterations_columns , ) self . iterations_results = pd . concat ([ self . iterations_results , current_iterations ]) self . iterations_results [ \"importance\" ] = self . iterations_results [ \"importance\" ] . astype ( float ) # Sort by mean test score of first metric self . report . sort_values ( by = \"mean_importance\" , ascending = False , inplace = True ) return self","title":"fit()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.fit_compute","text":"Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False **fit_kwargs In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. {} Returns: Type Description (tuple of (pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , class_names = None , return_scores = False , ** fit_kwargs , ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. **fit_kwargs: In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. Returns: (tuple of (pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , class_names = class_names , ** fit_kwargs ) return self . compute ( return_scores = return_scores )","title":"fit_compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.get_data_splits","text":"Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test","title":"get_data_splits()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.plot","text":"Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default ax matplotlib.axes Axes to which the output should be plotted. If not provided new axes are created. None top_n int Number of the most important features to be plotted. By default features are included in the plot. None show bool If True, the plots are shown to the user, otherwise they are not shown. Not showing a plot can be useful when you want to edit the returned axis before showing it. True **plot_kwargs Keyword arguments passed to the matplotlib.plotly.subplots method. {} Returns: Type Description (matplotlib.axes) Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , ax = None , top_n = None , show = True , ** plot_kwargs ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: ax (matplotlib.axes, optional): Axes to which the output should be plotted. If not provided new axes are created. top_n (int, optional): Number of the most important features to be plotted. By default features are included in the plot. show (bool, optional): If True, the plots are shown to the user, otherwise they are not shown. Not showing a plot can be useful when you want to edit the returned axis before showing it. **plot_kwargs: Keyword arguments passed to the matplotlib.plotly.subplots method. Returns: (matplotlib.axes): Axes that include the plot. \"\"\" feature_report = self . compute () self . iterations_results [ \"importance\" ] = self . iterations_results [ \"importance\" ] . astype ( float ) sorted_features = feature_report [ \"mean_importance\" ] . sort_values ( ascending = True ) . index . values if top_n is not None and top_n > 0 : sorted_features = sorted_features [ - top_n :] if ax is None : fig , ax = plt . subplots ( ** plot_kwargs ) for position , feature in enumerate ( sorted_features ): ax . boxplot ( self . iterations_results [ self . iterations_results [ \"feature\" ] == feature ][ \"importance\" ], positions = [ position ], vert = False , ) ax . set_yticks ( range ( position + 1 )) ax . set_yticklabels ( sorted_features ) ax . set_xlabel ( self . plot_x_label ) ax . set_ylabel ( self . plot_y_label ) ax . set_title ( self . plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = \"axes fraction\" , textcoords = \"offset points\" , va = \"top\" , ) if show : plt . show () else : plt . close () return ax","title":"plot()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance","text":"This model checks for similarity of two samples. A possible use case is analysis of whether the train sample differs from the test sample, due to e.g. non-stationarity. It assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. If the model achieves test AUC significantly different than 0.5, it indicates that it is possible to distinguish between the samples, and therefore, the samples differ. Features with a high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. This class currently works only with the Tree based models. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import SHAPImportanceResemblance X1 , _ = make_classification ( n_samples = 100 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 100 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) rm = SHAPImportanceResemblance ( clf ) feature_importance = rm . fit_compute ( X1 , X2 ) rm . plot ()","title":"SHAPImportanceResemblance"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf model object Binary classification model or pipeline. required scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn . Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. 'roc_auc' test_prc float Percentage of data used to test the model. By default 0.25 is set. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 verbose int Controls verbosity of the output: 0 - neither prints nor warnings are shown 1 - 50 - only most important warnings 51 - 100 - shows other warnings and prints above 100 - presents all prints and all warnings (including SHAP warnings). 0 random_state int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , clf , scoring = \"roc_auc\" , test_prc = 0.25 , n_jobs = 1 , verbose = 0 , random_state = None , ): \"\"\" Initializes the class. Args: clf (model object): Binary classification model or pipeline. scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html). Another option is using probatus.utils.Scorer to define a custom metric. Recommended option for this class is 'roc_auc'. test_prc (float, optional): Percentage of data used to test the model. By default 0.25 is set. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. verbose (int, optional): Controls verbosity of the output: - 0 - neither prints nor warnings are shown - 1 - 50 - only most important warnings - 51 - 100 - shows other warnings and prints - above 100 - presents all prints and all warnings (including SHAP warnings). random_state (int, optional): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. \"\"\" # noqa super () . __init__ ( clf = clf , scoring = scoring , test_prc = test_prc , n_jobs = n_jobs , verbose = verbose , random_state = random_state , ) self . plot_title = \"SHAP summary plot\"","title":"__init__()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.compute","text":"Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False Returns: Type Description (tuple(pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_scores = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. Returns: (tuple(pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_scores : return self . report , self . train_score , self . test_score else : return self . report","title":"compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.fit","text":"This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None **shap_kwargs keyword arguments passed to shap.Explainer . It also enables approximate and check_additivity parameters, passed while calculating SHAP values. The approximate=True causes less accurate, but faster SHAP values calculation, while check_additivity=False disables the additivity check inside SHAP. {} Returns: Type Description (SHAPImportanceResemblance) Fitted object. Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None , class_names = None , ** shap_kwargs ): \"\"\" This function assigns labels to each sample, 0 to the first sample, 1 to the second. Then, it randomly selects a portion of data to train on. The resulting model tries to distinguish which sample a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. **shap_kwargs: keyword arguments passed to [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer). It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values. The `approximate=True` causes less accurate, but faster SHAP values calculation, while `check_additivity=False` disables the additivity check inside SHAP. Returns: (SHAPImportanceResemblance): Fitted object. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names , class_names = class_names ) self . shap_values_test = shap_calc ( self . clf , self . X_test , verbose = self . verbose , ** shap_kwargs ) self . report = calculate_shap_importance ( self . shap_values_test , self . column_names ) return self","title":"fit()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.fit_compute","text":"Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None class_names None, or list of str List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. None return_scores bool Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. False **fit_kwargs In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. {} Returns: Type Description (tuple of (pd.DataFrame, float, float) or pd.DataFrame) Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , class_names = None , return_scores = False , ** fit_kwargs , ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. class_names (None, or list of str, optional): List of class names assigned, in this case provided samples e.g. ['sample1', 'sample2']. If none, the default ['First Sample', 'Second Sample'] are used. return_scores (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train score, test score), or feature importances. By default the second option is selected. **fit_kwargs: In case any other arguments are accepted by fit() method, they can be passed as keyword arguments. Returns: (tuple of (pd.DataFrame, float, float) or pd.DataFrame): Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , class_names = class_names , ** fit_kwargs ) return self . compute ( return_scores = return_scores )","title":"fit_compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.get_data_splits","text":"Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test","title":"get_data_splits()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.get_shap_values","text":"Gets the SHAP values generated on the test set. Returns: Type Description (np.array) SHAP values generated on the test set. Source code in probatus/sample_similarity/resemblance_model.py def get_shap_values ( self ): \"\"\" Gets the SHAP values generated on the test set. Returns: (np.array): SHAP values generated on the test set. \"\"\" self . _check_if_fitted () return self . shap_values_test","title":"get_shap_values()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.plot","text":"Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default plot_type str Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", 'bar' show bool If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. True **summary_plot_kwargs kwargs passed to the shap.summary_plot. {} Returns: Type Description (matplotlib.axes) Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , plot_type = \"bar\" , show = True , ** summary_plot_kwargs ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: plot_type (str, optional): Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", show (bool, optional): If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful, when you want to edit the returned axis, before showing it. **summary_plot_kwargs: kwargs passed to the shap.summary_plot. Returns: (matplotlib.axes): Axes that include the plot. \"\"\" # This line serves as a double check if the object has been fitted self . _check_if_fitted () shap . summary_plot ( self . shap_values_test , self . X_test , plot_type = plot_type , class_names = self . class_names , show = False , ** summary_plot_kwargs , ) ax = plt . gca () ax . set_title ( self . plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = \"axes fraction\" , textcoords = \"offset points\" , va = \"top\" , ) if show : plt . show () else : plt . close () return ax","title":"plot()"},{"location":"api/stat_tests.html","text":"Statistical Tests \u00b6 This module allows us to apply different statistical tests. AutoDist \u00b6 Class to automatically apply all implemented statistical distribution tests and binning strategies. to (a selection of) features in two dataframes. Examples: import numpy as np import pandas as pd from probatus.stat_tests import AutoDist df1 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) df2 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) myAutoDist = AutoDist ( statistical_tests = [ \"KS\" , \"PSI\" ], binning_strategies = 'simplebucketer' , bin_count = 10 ) myAutoDist . compute ( df1 , df2 , column_names = df1 . columns ) __init__ ( self , statistical_tests = 'all' , binning_strategies = 'default' , bin_count = 10 ) special \u00b6 Initializes the class. Parameters: Name Type Description Default statistical_tests str Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'AD' : Anderson-Darling TS. 'all' binning_strategies str Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: 'SimpleBucketer' : equally spaced bins, 'AgglomerativeBucketer' : binning by applying the Scikit-learn implementation of Agglomerative Clustering, 'QuantileBucketer' : bins with equal number of elements, None : no binning is applied. Note that not all statistical tests will be performed since some of them require binning strategies. 'default' : applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. 'all' : each binning strategy is used for each statistical test 'default' bin_count integer, None or list of integers bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_tests = \"all\" , binning_strategies = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_tests (str, optional): Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'AD'`: Anderson-Darling TS. binning_strategies (str, optional): Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: - `'SimpleBucketer'`: equally spaced bins, - `'AgglomerativeBucketer'`: binning by applying the Scikit-learn implementation of Agglomerative Clustering, - `'QuantileBucketer'`: bins with equal number of elements, - `None`: no binning is applied. Note that not all statistical tests will be performed since some of them require binning strategies. - `'default'`: applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. - `'all'`: each binning strategy is used for each statistical test bin_count (integer, None or list of integers, optional): bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. \"\"\" self . fitted = False # Initialize statistical tests to be performed if statistical_tests == \"all\" : self . statistical_tests = list ( DistributionStatistics . statistical_test_dict . keys ()) elif isinstance ( statistical_tests , str ): self . statistical_tests = [ statistical_tests ] else : self . statistical_tests = statistical_tests # Initialize binning strategies to be used if binning_strategies == \"all\" : self . binning_strategies = list ( DistributionStatistics . binning_strategy_dict . keys ()) elif isinstance ( binning_strategies , str ): self . binning_strategies = [ binning_strategies ] elif binning_strategies is None : self . binning_strategies = [ None ] else : self . binning_strategies = binning_strategies if not isinstance ( bin_count , list ): self . bin_count = [ bin_count ] else : self . bin_count = bin_count compute ( self , df1 , df2 , column_names = None , return_failed_tests = True , suppress_warnings = False ) \u00b6 Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies. Parameters: Name Type Description Default df1 pd.DataFrame dataframe 1 for distribution comparison with dataframe 2. required df2 pd.DataFrame dataframe 2 for distribution comparison with dataframe 1. required column_names list of str list of columns in df1 and df2 that should be compared. If None, all column names will be compared None return_failed_tests bool remove tests in result that did not succeed. True suppress_warnings bool whether to suppress warnings during the fit process. False Returns: Type Description (pd.DataFrame) dataframe with results of the performed statistical tests and binning strategies. Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , df1 , df2 , column_names = None , return_failed_tests = True , suppress_warnings = False , ): \"\"\" Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies. Args: df1 (pd.DataFrame): dataframe 1 for distribution comparison with dataframe 2. df2 (pd.DataFrame): dataframe 2 for distribution comparison with dataframe 1. column_names (list of str, optional): list of columns in df1 and df2 that should be compared. If None, all column names will be compared return_failed_tests (bool, optional): remove tests in result that did not succeed. suppress_warnings (bool, optional): whether to suppress warnings during the fit process. Returns: (pd.DataFrame): dataframe with results of the performed statistical tests and binning strategies. \"\"\" if column_names is None : column_names = df1 . columns . to_list () if len ( set ( column_names ) - set ( df2 . columns )): raise Exception ( \"column_names was set to None but columns in provided dataframes are different\" ) # Check if all columns in column_names are in df1 and df2 elif len ( set ( column_names ) - set ( df1 . columns )) or len ( set ( column_names ) - set ( df2 . columns )): raise Exception ( \"Not all columns in `column_names` are in the provided dataframes\" ) # Calculate statistics and p-values for all combinations result_all = pd . DataFrame () for col , stat_test , bin_strat , bins in tqdm ( list ( itertools . product ( column_names , self . statistical_tests , self . binning_strategies , self . bin_count , ) ) ): if self . binning_strategies == [ \"default\" ]: bin_strat = DistributionStatistics . statistical_test_dict [ stat_test ][ \"default_binning\" ] dist = DistributionStatistics ( statistical_test = stat_test , binning_strategy = bin_strat , bin_count = bins ) try : if suppress_warnings : warnings . filterwarnings ( \"ignore\" ) _ = dist . compute ( df1 [ col ], df2 [ col ]) if suppress_warnings : warnings . filterwarnings ( \"default\" ) statistic = dist . statistic p_value = dist . p_value except Exception : statistic , p_value = \"an error occurred\" , None pass # Append result to results list result_ = { \"column\" : col , \"statistical_test\" : stat_test , \"binning_strategy\" : bin_strat , \"bin_count\" : bins , \"statistic\" : statistic , \"p_value\" : p_value , } result_all = result_all . append ( result_ , ignore_index = True ) if not return_failed_tests : result_all = result_all [ result_all [ \"statistic\" ] != \"an error occurred\" ] self . fitted = True self . _result = result_all [ [ \"column\" , \"statistical_test\" , \"binning_strategy\" , \"bin_count\" , \"statistic\" , \"p_value\" , ] ] self . _result [ \"bin_count\" ] = self . _result [ \"bin_count\" ] . astype ( int ) self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"bin_count\" ] = 0 self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"binning_strategy\" ] = \"no_bucketing\" # Remove duplicates that appear if multiple bin numbers are passed, and binning strategy None self . _result = self . _result . drop_duplicates ( subset = [ \"column\" , \"statistical_test\" , \"binning_strategy\" , \"bin_count\" ], keep = \"first\" , ) # create pivot table as final output self . result = pd . pivot_table ( self . _result , values = [ \"statistic\" , \"p_value\" ], index = \"column\" , columns = [ \"statistical_test\" , \"binning_strategy\" , \"bin_count\" ], aggfunc = \"sum\" , ) # flatten multi-index self . result . columns = [ \"_\" . join ([ str ( x ) for x in line ]) for line in self . result . columns . values ] self . result . reset_index ( inplace = True ) return self . result DistributionStatistics \u00b6 Wrapper that applies a statistical method to compare two distributions. Depending on a test, one can also apply binning of the data. Examples: import numpy as np import pandas as pd from probatus.stat_tests import DistributionStatistics d1 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] d2 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] myTest = DistributionStatistics ( 'KS' , bin_count = 10 ) test_statistic , p_value = myTest . compute ( d1 , d2 , verbose = True ) __init__ ( self , statistical_test , binning_strategy = 'default' , bin_count = 10 ) special \u00b6 Initializes the class. Parameters: Name Type Description Default statistical_test string Statistical method to apply, statistical methods implemented: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. required binning_strategy string Binning strategy to apply, binning strategies implemented: 'simplebucketer' : equally spaced bins, 'agglomerativebucketer' : binning by applying the Scikit-learn implementation of Agglomerative Clustering, 'quantilebucketer' : bins with equal number of elements, 'default' : applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used, None : no binning is applied. The test is computed based on original distribution. 'default' bin_count int In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_test , binning_strategy = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_test (string): Statistical method to apply, statistical methods implemented: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. binning_strategy (string, optional): Binning strategy to apply, binning strategies implemented: - `'simplebucketer'`: equally spaced bins, - `'agglomerativebucketer'`: binning by applying the Scikit-learn implementation of Agglomerative Clustering, - `'quantilebucketer'`: bins with equal number of elements, - `'default'`: applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used, - `None`: no binning is applied. The test is computed based on original distribution. bin_count (int, optional): In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. \"\"\" self . statistical_test = statistical_test . upper () self . binning_strategy = binning_strategy self . bin_count = bin_count self . fitted = False # Initialize the statistical test if self . statistical_test not in self . statistical_test_dict : raise NotImplementedError ( \"The statistical test should be one of {} \" . format ( self . statistical_test_dict . keys ()) ) else : self . statistical_test_name = self . statistical_test_dict [ self . statistical_test ][ \"name\" ] self . _statistical_test_function = self . statistical_test_dict [ self . statistical_test ][ \"func\" ] # Initialize the binning strategy if self . binning_strategy : self . binning_strategy = self . binning_strategy . lower () if self . binning_strategy == \"default\" : self . binning_strategy = self . statistical_test_dict [ self . statistical_test ][ \"default_binning\" ] if self . binning_strategy not in self . binning_strategy_dict : raise NotImplementedError ( \"The binning strategy should be one of {} \" . format ( list ( self . binning_strategy_dict . keys ())) ) else : binner = self . binning_strategy_dict [ self . binning_strategy ] if binner is not None : self . binner = binner ( bin_count = self . bin_count ) compute ( self , d1 , d2 , verbose = False ) \u00b6 Apply the statistical test and compute statistic value and p-value. Parameters: Name Type Description Default d1 (np.array or pd.DataFrame): distribution 1. required d2 (np.array or pd.DataFrame): distribution 2. required verbose (bool, optional): Flag indicating whether prints should be shown. False Returns: Type Description (Tuple of floats) statistic value and p_value. For PSI test the return is only statistic Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , d1 , d2 , verbose = False ): \"\"\" Apply the statistical test and compute statistic value and p-value. Args: d1: (np.array or pd.DataFrame): distribution 1. d2: (np.array or pd.DataFrame): distribution 2. verbose: (bool, optional): Flag indicating whether prints should be shown. Returns: (Tuple of floats): statistic value and p_value. For PSI test the return is only statistic \"\"\" check_numeric_dtypes ( d1 ) check_numeric_dtypes ( d2 ) # Bin the data if self . binning_strategy : self . binner . fit ( d1 ) d1_preprocessed = self . binner . compute ( d1 ) d2_preprocessed = self . binner . compute ( d2 ) else : d1_preprocessed , d2_preprocessed = d1 , d2 # Perform the statistical test res = self . _statistical_test_function ( d1_preprocessed , d2_preprocessed , verbose = verbose ) self . fitted = True # Check form of results and return if type ( res ) == tuple : self . statistic , self . p_value = res return self . statistic , self . p_value else : self . statistic = res return self . statistic","title":"probatus.stat_tests"},{"location":"api/stat_tests.html#statistical-tests","text":"This module allows us to apply different statistical tests.","title":"Statistical Tests"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.AutoDist","text":"Class to automatically apply all implemented statistical distribution tests and binning strategies. to (a selection of) features in two dataframes. Examples: import numpy as np import pandas as pd from probatus.stat_tests import AutoDist df1 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) df2 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) myAutoDist = AutoDist ( statistical_tests = [ \"KS\" , \"PSI\" ], binning_strategies = 'simplebucketer' , bin_count = 10 ) myAutoDist . compute ( df1 , df2 , column_names = df1 . columns )","title":"AutoDist"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.AutoDist.__init__","text":"Initializes the class. Parameters: Name Type Description Default statistical_tests str Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'AD' : Anderson-Darling TS. 'all' binning_strategies str Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: 'SimpleBucketer' : equally spaced bins, 'AgglomerativeBucketer' : binning by applying the Scikit-learn implementation of Agglomerative Clustering, 'QuantileBucketer' : bins with equal number of elements, None : no binning is applied. Note that not all statistical tests will be performed since some of them require binning strategies. 'default' : applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. 'all' : each binning strategy is used for each statistical test 'default' bin_count integer, None or list of integers bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_tests = \"all\" , binning_strategies = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_tests (str, optional): Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'AD'`: Anderson-Darling TS. binning_strategies (str, optional): Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: - `'SimpleBucketer'`: equally spaced bins, - `'AgglomerativeBucketer'`: binning by applying the Scikit-learn implementation of Agglomerative Clustering, - `'QuantileBucketer'`: bins with equal number of elements, - `None`: no binning is applied. Note that not all statistical tests will be performed since some of them require binning strategies. - `'default'`: applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. - `'all'`: each binning strategy is used for each statistical test bin_count (integer, None or list of integers, optional): bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. \"\"\" self . fitted = False # Initialize statistical tests to be performed if statistical_tests == \"all\" : self . statistical_tests = list ( DistributionStatistics . statistical_test_dict . keys ()) elif isinstance ( statistical_tests , str ): self . statistical_tests = [ statistical_tests ] else : self . statistical_tests = statistical_tests # Initialize binning strategies to be used if binning_strategies == \"all\" : self . binning_strategies = list ( DistributionStatistics . binning_strategy_dict . keys ()) elif isinstance ( binning_strategies , str ): self . binning_strategies = [ binning_strategies ] elif binning_strategies is None : self . binning_strategies = [ None ] else : self . binning_strategies = binning_strategies if not isinstance ( bin_count , list ): self . bin_count = [ bin_count ] else : self . bin_count = bin_count","title":"__init__()"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.AutoDist.compute","text":"Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies. Parameters: Name Type Description Default df1 pd.DataFrame dataframe 1 for distribution comparison with dataframe 2. required df2 pd.DataFrame dataframe 2 for distribution comparison with dataframe 1. required column_names list of str list of columns in df1 and df2 that should be compared. If None, all column names will be compared None return_failed_tests bool remove tests in result that did not succeed. True suppress_warnings bool whether to suppress warnings during the fit process. False Returns: Type Description (pd.DataFrame) dataframe with results of the performed statistical tests and binning strategies. Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , df1 , df2 , column_names = None , return_failed_tests = True , suppress_warnings = False , ): \"\"\" Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies. Args: df1 (pd.DataFrame): dataframe 1 for distribution comparison with dataframe 2. df2 (pd.DataFrame): dataframe 2 for distribution comparison with dataframe 1. column_names (list of str, optional): list of columns in df1 and df2 that should be compared. If None, all column names will be compared return_failed_tests (bool, optional): remove tests in result that did not succeed. suppress_warnings (bool, optional): whether to suppress warnings during the fit process. Returns: (pd.DataFrame): dataframe with results of the performed statistical tests and binning strategies. \"\"\" if column_names is None : column_names = df1 . columns . to_list () if len ( set ( column_names ) - set ( df2 . columns )): raise Exception ( \"column_names was set to None but columns in provided dataframes are different\" ) # Check if all columns in column_names are in df1 and df2 elif len ( set ( column_names ) - set ( df1 . columns )) or len ( set ( column_names ) - set ( df2 . columns )): raise Exception ( \"Not all columns in `column_names` are in the provided dataframes\" ) # Calculate statistics and p-values for all combinations result_all = pd . DataFrame () for col , stat_test , bin_strat , bins in tqdm ( list ( itertools . product ( column_names , self . statistical_tests , self . binning_strategies , self . bin_count , ) ) ): if self . binning_strategies == [ \"default\" ]: bin_strat = DistributionStatistics . statistical_test_dict [ stat_test ][ \"default_binning\" ] dist = DistributionStatistics ( statistical_test = stat_test , binning_strategy = bin_strat , bin_count = bins ) try : if suppress_warnings : warnings . filterwarnings ( \"ignore\" ) _ = dist . compute ( df1 [ col ], df2 [ col ]) if suppress_warnings : warnings . filterwarnings ( \"default\" ) statistic = dist . statistic p_value = dist . p_value except Exception : statistic , p_value = \"an error occurred\" , None pass # Append result to results list result_ = { \"column\" : col , \"statistical_test\" : stat_test , \"binning_strategy\" : bin_strat , \"bin_count\" : bins , \"statistic\" : statistic , \"p_value\" : p_value , } result_all = result_all . append ( result_ , ignore_index = True ) if not return_failed_tests : result_all = result_all [ result_all [ \"statistic\" ] != \"an error occurred\" ] self . fitted = True self . _result = result_all [ [ \"column\" , \"statistical_test\" , \"binning_strategy\" , \"bin_count\" , \"statistic\" , \"p_value\" , ] ] self . _result [ \"bin_count\" ] = self . _result [ \"bin_count\" ] . astype ( int ) self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"bin_count\" ] = 0 self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"binning_strategy\" ] = \"no_bucketing\" # Remove duplicates that appear if multiple bin numbers are passed, and binning strategy None self . _result = self . _result . drop_duplicates ( subset = [ \"column\" , \"statistical_test\" , \"binning_strategy\" , \"bin_count\" ], keep = \"first\" , ) # create pivot table as final output self . result = pd . pivot_table ( self . _result , values = [ \"statistic\" , \"p_value\" ], index = \"column\" , columns = [ \"statistical_test\" , \"binning_strategy\" , \"bin_count\" ], aggfunc = \"sum\" , ) # flatten multi-index self . result . columns = [ \"_\" . join ([ str ( x ) for x in line ]) for line in self . result . columns . values ] self . result . reset_index ( inplace = True ) return self . result","title":"compute()"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.DistributionStatistics","text":"Wrapper that applies a statistical method to compare two distributions. Depending on a test, one can also apply binning of the data. Examples: import numpy as np import pandas as pd from probatus.stat_tests import DistributionStatistics d1 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] d2 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] myTest = DistributionStatistics ( 'KS' , bin_count = 10 ) test_statistic , p_value = myTest . compute ( d1 , d2 , verbose = True )","title":"DistributionStatistics"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.DistributionStatistics.__init__","text":"Initializes the class. Parameters: Name Type Description Default statistical_test string Statistical method to apply, statistical methods implemented: 'ES' : Epps-Singleton, 'KS' : Kolmogorov-Smirnov statistic, 'PSI' : Population Stability Index, 'SW' : Shapiro-Wilk based difference statistic, 'AD' : Anderson-Darling TS. required binning_strategy string Binning strategy to apply, binning strategies implemented: 'simplebucketer' : equally spaced bins, 'agglomerativebucketer' : binning by applying the Scikit-learn implementation of Agglomerative Clustering, 'quantilebucketer' : bins with equal number of elements, 'default' : applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used, None : no binning is applied. The test is computed based on original distribution. 'default' bin_count int In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_test , binning_strategy = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_test (string): Statistical method to apply, statistical methods implemented: - `'ES'`: Epps-Singleton, - `'KS'`: Kolmogorov-Smirnov statistic, - `'PSI'`: Population Stability Index, - `'SW'`: Shapiro-Wilk based difference statistic, - `'AD'`: Anderson-Darling TS. binning_strategy (string, optional): Binning strategy to apply, binning strategies implemented: - `'simplebucketer'`: equally spaced bins, - `'agglomerativebucketer'`: binning by applying the Scikit-learn implementation of Agglomerative Clustering, - `'quantilebucketer'`: bins with equal number of elements, - `'default'`: applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used, - `None`: no binning is applied. The test is computed based on original distribution. bin_count (int, optional): In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. \"\"\" self . statistical_test = statistical_test . upper () self . binning_strategy = binning_strategy self . bin_count = bin_count self . fitted = False # Initialize the statistical test if self . statistical_test not in self . statistical_test_dict : raise NotImplementedError ( \"The statistical test should be one of {} \" . format ( self . statistical_test_dict . keys ()) ) else : self . statistical_test_name = self . statistical_test_dict [ self . statistical_test ][ \"name\" ] self . _statistical_test_function = self . statistical_test_dict [ self . statistical_test ][ \"func\" ] # Initialize the binning strategy if self . binning_strategy : self . binning_strategy = self . binning_strategy . lower () if self . binning_strategy == \"default\" : self . binning_strategy = self . statistical_test_dict [ self . statistical_test ][ \"default_binning\" ] if self . binning_strategy not in self . binning_strategy_dict : raise NotImplementedError ( \"The binning strategy should be one of {} \" . format ( list ( self . binning_strategy_dict . keys ())) ) else : binner = self . binning_strategy_dict [ self . binning_strategy ] if binner is not None : self . binner = binner ( bin_count = self . bin_count )","title":"__init__()"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.DistributionStatistics.compute","text":"Apply the statistical test and compute statistic value and p-value. Parameters: Name Type Description Default d1 (np.array or pd.DataFrame): distribution 1. required d2 (np.array or pd.DataFrame): distribution 2. required verbose (bool, optional): Flag indicating whether prints should be shown. False Returns: Type Description (Tuple of floats) statistic value and p_value. For PSI test the return is only statistic Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , d1 , d2 , verbose = False ): \"\"\" Apply the statistical test and compute statistic value and p-value. Args: d1: (np.array or pd.DataFrame): distribution 1. d2: (np.array or pd.DataFrame): distribution 2. verbose: (bool, optional): Flag indicating whether prints should be shown. Returns: (Tuple of floats): statistic value and p_value. For PSI test the return is only statistic \"\"\" check_numeric_dtypes ( d1 ) check_numeric_dtypes ( d2 ) # Bin the data if self . binning_strategy : self . binner . fit ( d1 ) d1_preprocessed = self . binner . compute ( d1 ) d2_preprocessed = self . binner . compute ( d2 ) else : d1_preprocessed , d2_preprocessed = d1 , d2 # Perform the statistical test res = self . _statistical_test_function ( d1_preprocessed , d2_preprocessed , verbose = verbose ) self . fitted = True # Check form of results and return if type ( res ) == tuple : self . statistic , self . p_value = res return self . statistic , self . p_value else : self . statistic = res return self . statistic","title":"compute()"},{"location":"api/utils.html","text":"Utility Functions \u00b6 This module contains various smaller functionalities that can be used across the probatus package. Scorer \u00b6 Scores a given machine learning model based on the provided metric name and optionally a custom scoring function. Examples: from probatus.utils import Scorer from sklearn.metrics import make_scorer from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier import pandas as pd # Make ROC AUC scorer scorer1 = Scorer ( 'roc_auc' ) # Make custom scorer with following function: def custom_metric ( y_true , y_pred ): return ( y_true == y_pred ) . sum () scorer2 = Scorer ( 'custom_metric' , custom_scorer = make_scorer ( custom_metric )) # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf = clf . fit ( X_train , y_train ) # Score model score_test_scorer1 = scorer1 . score ( clf , X_test , y_test ) score_test_scorer2 = scorer2 . score ( clf , X_test , y_test ) print ( f 'Test ROC AUC is { score_test_scorer1 } , Test { scorer2 . metric_name } is { score_test_scorer2 } ' ) __init__ ( self , metric_name , custom_scorer = None ) special \u00b6 Initializes the class. Parameters: Name Type Description Default metric_name str Name of the metric used to evaluate the model. If the custom_scorer is not passed, the metric name needs to be aligned with classification scorers names in sklearn ( link ). required custom_scorer sklearn.metrics Scorer callable Callable that can score samples. None Source code in probatus/utils/scoring.py def __init__ ( self , metric_name , custom_scorer = None ): \"\"\" Initializes the class. Args: metric_name (str): Name of the metric used to evaluate the model. If the custom_scorer is not passed, the metric name needs to be aligned with classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). custom_scorer (sklearn.metrics Scorer callable, optional): Callable that can score samples. \"\"\" self . metric_name = metric_name if custom_scorer is not None : self . scorer = custom_scorer else : self . scorer = get_scorer ( self . metric_name ) score ( self , model , X , y ) \u00b6 Scores the samples model based on the provided metric name. Parameters: Name Type Description Default model model object Model to be scored. required X array-like of shape (n_samples,n_features Samples on which the model is scored. required y array-like of shape (n_samples, Labels on which the model is scored. required Returns: Type Description (float) Score returned by the model Source code in probatus/utils/scoring.py def score ( self , model , X , y ): \"\"\" Scores the samples model based on the provided metric name. Args: model (model object): Model to be scored. X (array-like of shape (n_samples,n_features)): Samples on which the model is scored. y (array-like of shape (n_samples,)): Labels on which the model is scored. Returns: (float): Score returned by the model \"\"\" return self . scorer ( model , X , y ) get_scorers ( scoring ) \u00b6 Returns Scorers list based on the provided scoring. Parameters: Name Type Description Default scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. required Returns: Type Description (list of probatus.utils.Scorer) List of scorers that can be used for scoring models Source code in probatus/utils/scoring.py def get_scorers ( scoring ): \"\"\" Returns Scorers list based on the provided scoring. Args: scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. Returns: (list of probatus.utils.Scorer): List of scorers that can be used for scoring models \"\"\" scorers = [] if isinstance ( scoring , list ): for scorer in scoring : scorers . append ( get_single_scorer ( scorer )) else : scorers . append ( get_single_scorer ( scoring )) return scorers get_single_scorer ( scoring ) \u00b6 Returns single Scorer, based on provided input in scoring argument. Parameters: Name Type Description Default scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. required Returns: Type Description (probatus.utils.Scorer) Scorer that can be used for scoring models Source code in probatus/utils/scoring.py def get_single_scorer ( scoring ): \"\"\" Returns single Scorer, based on provided input in scoring argument. Args: scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. Returns: (probatus.utils.Scorer): Scorer that can be used for scoring models \"\"\" if isinstance ( scoring , str ): return Scorer ( scoring ) elif isinstance ( scoring , Scorer ): return scoring else : raise ( ValueError ( \"The scoring should contain either strings or probatus.utils.Scorer class\" )) Utility functions for sampling. This module holds utility functions for sampling data. They can be used, for example, to provide a random single typical datapoin in a technical report. sample_row ( X , filter_rows_with_na = False , random_state = 42 , max_field_len = 50 ) \u00b6 Sample a row from pandas dataframe. Extracts the column name, datatype, minimum and maximum values for each column in the supplied dataframe. The orientation is row-based (as opposed to df.sample(1) ), which allows for better printing when a dataset contains many features. This function is usefull when providing a sample row in technical model documentation. Examples: from probatus.utils import sample_row from sklearn.datasets import load_iris iris = load_iris ( as_frame = True ) . get ( 'data' ) sample = sample_row ( iris , filter_rows_with_na = False , random_state = 12 ) print ( sample . to_markdown ()) Example output column dtype sample range_low range_high sepal length (cm) float64 5 4.3 7.9 sepal width (cm) float64 3.5 2 4.4 petal length (cm) float64 1.3 1 6.9 petal width (cm) float64 0.3 0.1 2.5 Parameters: Name Type Description Default X DataFrame Pandas DataFrame to be sampled required filter_rows_with_na bool if true, rows with na values are not considered for sampling False random_state int Optional random state to ensure reproducability 42 max_field_len int Maximum number of characters for fields, beyond which any text is truncated 50 Returns: Type Description DataFrame (pd.DataFrame): A Pandas DataFrame containing the sampled row Source code in probatus/utils/sampling.py def sample_row ( X : pd . DataFrame , filter_rows_with_na : bool = False , random_state : int = 42 , max_field_len : int = 50 , ) -> pd . DataFrame : \"\"\"Sample a row from pandas dataframe. Extracts the column name, datatype, minimum and maximum values for each column in the supplied dataframe. The orientation is row-based (as opposed to `df.sample(1)`), which allows for better printing when a dataset contains many features. This function is usefull when providing a sample row in technical model documentation. Example: ```python from probatus.utils import sample_row from sklearn.datasets import load_iris iris = load_iris(as_frame=True).get('data') sample = sample_row(iris, filter_rows_with_na=False, random_state=12) print(sample.to_markdown()) ``` ??? info \"Example output\" | column | dtype | sample | range_low | range_high | |:------------------|:--------|---------:|------------:|-------------:| | sepal length (cm) | float64 | 5 | 4.3 | 7.9 | | sepal width (cm) | float64 | 3.5 | 2 | 4.4 | | petal length (cm) | float64 | 1.3 | 1 | 6.9 | | petal width (cm) | float64 | 0.3 | 0.1 | 2.5 | Args: X (DataFrame): Pandas DataFrame to be sampled filter_rows_with_na (bool): if true, rows with na values are not considered for sampling random_state (int): Optional random state to ensure reproducability max_field_len (int): Maximum number of characters for fields, beyond which any text is truncated Returns: (pd.DataFrame): A Pandas DataFrame containing the sampled row \"\"\" # Input validation assert type ( X ) == pd . DataFrame , \"X should be pandas DataFrame\" assert X . empty is False , \"X should not be an empty DataFrame\" assert type ( filter_rows_with_na ) == bool , \"filter_rows_with_na should be a boolean\" assert type ( random_state ) == int , \"random_state should be an integer\" assert type ( max_field_len ) == int , \"max_field_len should be an integer\" # Create new empty df sample_df = pd . DataFrame () # Convert dtypes of pandas to ensure detection of data types X_dtypes = X . convert_dtypes () # Sample row from X sample_row = X . sample ( 1 , random_state = random_state ) if filter_rows_with_na : try : sample_row = X . dropna () . sample ( 1 , random_state = random_state ) except ValueError : logging . info ( \"sample_row(): No rows without NaN found, sampling from all rows..\" ) # Sample every column of X for i , col in enumerate ( sample_row . columns ): # Extract sample from X if not all samples are nan sample = sample_row [ col ] . values [ 0 ] # If datatype allows it, extract low and high range if is_numeric_dtype ( X_dtypes [ col ]): low = X [ col ] . min ( skipna = True ) high = X [ col ] . max ( skipna = True ) else : low = \"\" high = \"\" # Shorten sampled datapoint if too long if isinstance ( sample , str ) and len ( sample ) > max_field_len : sample = sample [: ( max_field_len // 2 ) - 1 ] + \"...\" + sample [( - max_field_len // 2 ) + 2 :] # Add new row to sample_df row_df = pd . DataFrame ( { \"column\" : [ col ], \"dtype\" : [ X [ col ] . dtype ], \"sample\" : [ sample ], \"range_low\" : [ low ], \"range_high\" : [ high ], } ) sample_df = pd . concat ([ sample_df , row_df ], ignore_index = True ) sample_df = sample_df . set_index ([ \"column\" ]) return sample_df","title":"probatus.utils"},{"location":"api/utils.html#utility-functions","text":"This module contains various smaller functionalities that can be used across the probatus package.","title":"Utility Functions"},{"location":"api/utils.html#probatus.utils.scoring.Scorer","text":"Scores a given machine learning model based on the provided metric name and optionally a custom scoring function. Examples: from probatus.utils import Scorer from sklearn.metrics import make_scorer from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier import pandas as pd # Make ROC AUC scorer scorer1 = Scorer ( 'roc_auc' ) # Make custom scorer with following function: def custom_metric ( y_true , y_pred ): return ( y_true == y_pred ) . sum () scorer2 = Scorer ( 'custom_metric' , custom_scorer = make_scorer ( custom_metric )) # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf = clf . fit ( X_train , y_train ) # Score model score_test_scorer1 = scorer1 . score ( clf , X_test , y_test ) score_test_scorer2 = scorer2 . score ( clf , X_test , y_test ) print ( f 'Test ROC AUC is { score_test_scorer1 } , Test { scorer2 . metric_name } is { score_test_scorer2 } ' )","title":"Scorer"},{"location":"api/utils.html#probatus.utils.scoring.Scorer.__init__","text":"Initializes the class. Parameters: Name Type Description Default metric_name str Name of the metric used to evaluate the model. If the custom_scorer is not passed, the metric name needs to be aligned with classification scorers names in sklearn ( link ). required custom_scorer sklearn.metrics Scorer callable Callable that can score samples. None Source code in probatus/utils/scoring.py def __init__ ( self , metric_name , custom_scorer = None ): \"\"\" Initializes the class. Args: metric_name (str): Name of the metric used to evaluate the model. If the custom_scorer is not passed, the metric name needs to be aligned with classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). custom_scorer (sklearn.metrics Scorer callable, optional): Callable that can score samples. \"\"\" self . metric_name = metric_name if custom_scorer is not None : self . scorer = custom_scorer else : self . scorer = get_scorer ( self . metric_name )","title":"__init__()"},{"location":"api/utils.html#probatus.utils.scoring.Scorer.score","text":"Scores the samples model based on the provided metric name. Parameters: Name Type Description Default model model object Model to be scored. required X array-like of shape (n_samples,n_features Samples on which the model is scored. required y array-like of shape (n_samples, Labels on which the model is scored. required Returns: Type Description (float) Score returned by the model Source code in probatus/utils/scoring.py def score ( self , model , X , y ): \"\"\" Scores the samples model based on the provided metric name. Args: model (model object): Model to be scored. X (array-like of shape (n_samples,n_features)): Samples on which the model is scored. y (array-like of shape (n_samples,)): Labels on which the model is scored. Returns: (float): Score returned by the model \"\"\" return self . scorer ( model , X , y )","title":"score()"},{"location":"api/utils.html#probatus.utils.scoring.get_scorers","text":"Returns Scorers list based on the provided scoring. Parameters: Name Type Description Default scoring string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. required Returns: Type Description (list of probatus.utils.Scorer) List of scorers that can be used for scoring models Source code in probatus/utils/scoring.py def get_scorers ( scoring ): \"\"\" Returns Scorers list based on the provided scoring. Args: scoring (string, list of strings, probatus.utils.Scorer or list of probatus.utils.Scorers): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. Returns: (list of probatus.utils.Scorer): List of scorers that can be used for scoring models \"\"\" scorers = [] if isinstance ( scoring , list ): for scorer in scoring : scorers . append ( get_single_scorer ( scorer )) else : scorers . append ( get_single_scorer ( scoring )) return scorers","title":"get_scorers()"},{"location":"api/utils.html#probatus.utils.scoring.get_single_scorer","text":"Returns single Scorer, based on provided input in scoring argument. Parameters: Name Type Description Default scoring string or probatus.utils.Scorer Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ( link ). Another option is using probatus.utils.Scorer to define a custom metric. required Returns: Type Description (probatus.utils.Scorer) Scorer that can be used for scoring models Source code in probatus/utils/scoring.py def get_single_scorer ( scoring ): \"\"\" Returns single Scorer, based on provided input in scoring argument. Args: scoring (string or probatus.utils.Scorer, optional): Metric for which the model performance is calculated. It can be either a metric name aligned with predefined classification scorers names in sklearn ([link](https://scikit-learn.org/stable/modules/model_evaluation.html)). Another option is using probatus.utils.Scorer to define a custom metric. Returns: (probatus.utils.Scorer): Scorer that can be used for scoring models \"\"\" if isinstance ( scoring , str ): return Scorer ( scoring ) elif isinstance ( scoring , Scorer ): return scoring else : raise ( ValueError ( \"The scoring should contain either strings or probatus.utils.Scorer class\" )) Utility functions for sampling. This module holds utility functions for sampling data. They can be used, for example, to provide a random single typical datapoin in a technical report.","title":"get_single_scorer()"},{"location":"api/utils.html#probatus.utils.sampling.sample_row","text":"Sample a row from pandas dataframe. Extracts the column name, datatype, minimum and maximum values for each column in the supplied dataframe. The orientation is row-based (as opposed to df.sample(1) ), which allows for better printing when a dataset contains many features. This function is usefull when providing a sample row in technical model documentation. Examples: from probatus.utils import sample_row from sklearn.datasets import load_iris iris = load_iris ( as_frame = True ) . get ( 'data' ) sample = sample_row ( iris , filter_rows_with_na = False , random_state = 12 ) print ( sample . to_markdown ()) Example output column dtype sample range_low range_high sepal length (cm) float64 5 4.3 7.9 sepal width (cm) float64 3.5 2 4.4 petal length (cm) float64 1.3 1 6.9 petal width (cm) float64 0.3 0.1 2.5 Parameters: Name Type Description Default X DataFrame Pandas DataFrame to be sampled required filter_rows_with_na bool if true, rows with na values are not considered for sampling False random_state int Optional random state to ensure reproducability 42 max_field_len int Maximum number of characters for fields, beyond which any text is truncated 50 Returns: Type Description DataFrame (pd.DataFrame): A Pandas DataFrame containing the sampled row Source code in probatus/utils/sampling.py def sample_row ( X : pd . DataFrame , filter_rows_with_na : bool = False , random_state : int = 42 , max_field_len : int = 50 , ) -> pd . DataFrame : \"\"\"Sample a row from pandas dataframe. Extracts the column name, datatype, minimum and maximum values for each column in the supplied dataframe. The orientation is row-based (as opposed to `df.sample(1)`), which allows for better printing when a dataset contains many features. This function is usefull when providing a sample row in technical model documentation. Example: ```python from probatus.utils import sample_row from sklearn.datasets import load_iris iris = load_iris(as_frame=True).get('data') sample = sample_row(iris, filter_rows_with_na=False, random_state=12) print(sample.to_markdown()) ``` ??? info \"Example output\" | column | dtype | sample | range_low | range_high | |:------------------|:--------|---------:|------------:|-------------:| | sepal length (cm) | float64 | 5 | 4.3 | 7.9 | | sepal width (cm) | float64 | 3.5 | 2 | 4.4 | | petal length (cm) | float64 | 1.3 | 1 | 6.9 | | petal width (cm) | float64 | 0.3 | 0.1 | 2.5 | Args: X (DataFrame): Pandas DataFrame to be sampled filter_rows_with_na (bool): if true, rows with na values are not considered for sampling random_state (int): Optional random state to ensure reproducability max_field_len (int): Maximum number of characters for fields, beyond which any text is truncated Returns: (pd.DataFrame): A Pandas DataFrame containing the sampled row \"\"\" # Input validation assert type ( X ) == pd . DataFrame , \"X should be pandas DataFrame\" assert X . empty is False , \"X should not be an empty DataFrame\" assert type ( filter_rows_with_na ) == bool , \"filter_rows_with_na should be a boolean\" assert type ( random_state ) == int , \"random_state should be an integer\" assert type ( max_field_len ) == int , \"max_field_len should be an integer\" # Create new empty df sample_df = pd . DataFrame () # Convert dtypes of pandas to ensure detection of data types X_dtypes = X . convert_dtypes () # Sample row from X sample_row = X . sample ( 1 , random_state = random_state ) if filter_rows_with_na : try : sample_row = X . dropna () . sample ( 1 , random_state = random_state ) except ValueError : logging . info ( \"sample_row(): No rows without NaN found, sampling from all rows..\" ) # Sample every column of X for i , col in enumerate ( sample_row . columns ): # Extract sample from X if not all samples are nan sample = sample_row [ col ] . values [ 0 ] # If datatype allows it, extract low and high range if is_numeric_dtype ( X_dtypes [ col ]): low = X [ col ] . min ( skipna = True ) high = X [ col ] . max ( skipna = True ) else : low = \"\" high = \"\" # Shorten sampled datapoint if too long if isinstance ( sample , str ) and len ( sample ) > max_field_len : sample = sample [: ( max_field_len // 2 ) - 1 ] + \"...\" + sample [( - max_field_len // 2 ) + 2 :] # Add new row to sample_df row_df = pd . DataFrame ( { \"column\" : [ col ], \"dtype\" : [ X [ col ] . dtype ], \"sample\" : [ sample ], \"range_low\" : [ low ], \"range_high\" : [ high ], } ) sample_df = pd . concat ([ sample_df , row_df ], ignore_index = True ) sample_df = sample_df . set_index ([ \"column\" ]) return sample_df","title":"sample_row()"},{"location":"discussion/contributing.html","text":"Contributing guide \u00b6 probatus aims to provide a set of tools that can speed up common workflows around validating binary classifiers and the data used to train them. We're very much open to contributions but there are some things to keep in mind: Discuss the feature and implementation you want to add on Github before you write a PR for it. On disagreements, maintainer(s) will have the final word. Features need a somewhat general usecase. If the usecase is very niche it will be hard for us to consider maintaining it. If you\u2019re going to add a feature, consider if you could help out in the maintenance of it. When issues or pull requests are not going to be resolved or merged, they should be closed as soon as possible. This is kinder than deciding this after a long period. Our issue tracker should reflect work to be done. That said, there are many ways to contribute to probatus, including: Contribution to code Improving the documentation Reviewing merge requests Investigating bugs Reporting issues Starting out with open source? See the guide How to Contribute to Open Source and have a look at our issues labelled good first issue . Setup \u00b6 Development install: pip install -e 'probatus[all]' Unit testing: pytest We use pre-commit hooks to ensure code styling. Install with: pre-commit install Standards \u00b6 Python 3.6+ Follow PEP8 as closely as possible (except line length) google docstring format Git: Include a short description of what and why was done, how can be seen in the code. Use present tense, imperative mood Git: limit the length of the first line to 72 chars. You can use multiple messages to specify a second (longer) line: git commit -m \"Patch load function\" -m \"This is a much longer explanation of what was done\" Code structure \u00b6 Model validation modules assume that trained models passed for validation are developed in a scikit-learn framework (i.e. have predict_proba and other standard functions), or follow a scikit-learn API e.g. XGBoost. Every python file used for model validation needs to be in /probatus/ Class structure for a given module should have a base class and specific functionality classes that inherit from base. If a given module implements only a single way of computing the output, the base class is not required. Functions should not be as short as possible in terms of lines of code. If a lot of code is needed, try to put together snippets of code into other functions. This make the code more readable, and easier to test. Classes follow the probatus API structure: Each class implements fit() , compute() and fit_compute() methods. fit() is used to fit an object with provided data (unless no fit is required), and compute() calculates the output e.g. DataFrame with a report for the user. Lastly, fit_compute() applies one after the other. If applicable, the plot() method presents the user with the appropriate graphs. For compute() and plot() , check if the object is fitted first. Documentation \u00b6 Documentation is a very crucial part of the project because it ensures usability of the package. We develop the docs in the following way: We use mkdocs with mkdocs-material theme. The docs/ folder contains all the relevant documentation. We use mkdocs serve to view the documentation locally. Use it to test the documentation everytime you make any changes. Maintainers can deploy the docs using mkdocs gh-deploy . The documentation is deployed to https://ing-bank.github.io/probatus/ .","title":"Contributing"},{"location":"discussion/contributing.html#contributing-guide","text":"probatus aims to provide a set of tools that can speed up common workflows around validating binary classifiers and the data used to train them. We're very much open to contributions but there are some things to keep in mind: Discuss the feature and implementation you want to add on Github before you write a PR for it. On disagreements, maintainer(s) will have the final word. Features need a somewhat general usecase. If the usecase is very niche it will be hard for us to consider maintaining it. If you\u2019re going to add a feature, consider if you could help out in the maintenance of it. When issues or pull requests are not going to be resolved or merged, they should be closed as soon as possible. This is kinder than deciding this after a long period. Our issue tracker should reflect work to be done. That said, there are many ways to contribute to probatus, including: Contribution to code Improving the documentation Reviewing merge requests Investigating bugs Reporting issues Starting out with open source? See the guide How to Contribute to Open Source and have a look at our issues labelled good first issue .","title":"Contributing guide"},{"location":"discussion/contributing.html#setup","text":"Development install: pip install -e 'probatus[all]' Unit testing: pytest We use pre-commit hooks to ensure code styling. Install with: pre-commit install","title":"Setup"},{"location":"discussion/contributing.html#standards","text":"Python 3.6+ Follow PEP8 as closely as possible (except line length) google docstring format Git: Include a short description of what and why was done, how can be seen in the code. Use present tense, imperative mood Git: limit the length of the first line to 72 chars. You can use multiple messages to specify a second (longer) line: git commit -m \"Patch load function\" -m \"This is a much longer explanation of what was done\"","title":"Standards"},{"location":"discussion/contributing.html#code-structure","text":"Model validation modules assume that trained models passed for validation are developed in a scikit-learn framework (i.e. have predict_proba and other standard functions), or follow a scikit-learn API e.g. XGBoost. Every python file used for model validation needs to be in /probatus/ Class structure for a given module should have a base class and specific functionality classes that inherit from base. If a given module implements only a single way of computing the output, the base class is not required. Functions should not be as short as possible in terms of lines of code. If a lot of code is needed, try to put together snippets of code into other functions. This make the code more readable, and easier to test. Classes follow the probatus API structure: Each class implements fit() , compute() and fit_compute() methods. fit() is used to fit an object with provided data (unless no fit is required), and compute() calculates the output e.g. DataFrame with a report for the user. Lastly, fit_compute() applies one after the other. If applicable, the plot() method presents the user with the appropriate graphs. For compute() and plot() , check if the object is fitted first.","title":"Code structure"},{"location":"discussion/contributing.html#documentation","text":"Documentation is a very crucial part of the project because it ensures usability of the package. We develop the docs in the following way: We use mkdocs with mkdocs-material theme. The docs/ folder contains all the relevant documentation. We use mkdocs serve to view the documentation locally. Use it to test the documentation everytime you make any changes. Maintainers can deploy the docs using mkdocs gh-deploy . The documentation is deployed to https://ing-bank.github.io/probatus/ .","title":"Documentation"},{"location":"discussion/nb_rfecv_vs_shaprfecv.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); ShapRFECV vs sklearn RFECV \u00b6 In this section we will compare the performance of the model trained on the features selected using the probatus ShapRFECV and the sklearn RFECV . In order to compare them let's first prepare a dataset, and a model that will be applied: from probatus.feature_elimination import ShapRFECV import numpy as np import pandas as pd import lightgbm from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split , cross_val_score from sklearn.feature_selection import RFECV import matplotlib.pyplot as plt # Prepare train and test data: X , y = make_classification ( n_samples = 10000 , class_sep = 0.1 , n_informative = 40 , n_features = 50 , random_state = 0 , n_clusters_per_class = 10 ) X = pd . DataFrame ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.5 , random_state = 42 ) # Set up the model: clf = lightgbm . LGBMClassifier ( n_estimators = 10 , num_leaves = 7 ) Now, we can run ShapRFECV and RFECV with the same parameters, to extract the optimal feature sets: # Run RFECV and ShapRFECV with the same parameters rfe = RFECV ( clf , step = 1 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 ) . fit ( X_train , y_train ) shap_elimination = ShapRFECV ( clf = clf , step = 1 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 ) shap_report = shap_elimination . fit_compute ( X_train , y_train ) # Compare the CV Validation AUC for different number of features in each method. ax = pd . DataFrame ({ 'RFECV Validation AUC' : list ( reversed ( rfe . grid_scores_ )), 'ShapRFECV Validation AUC' : shap_report [ 'val_metric_mean' ] . values . tolist ()}, index = shap_report [ 'num_features' ] . values . tolist ()) . plot ( ylim = ( 0.5 , 0.7 ), rot = 10 , title = 'Comparison of RFECV and ShapRFECV' , figsize = ( 10 , 5 )) ax . set_ylabel ( \"Model Performance\" ) ax . set_xlabel ( \"Number of features\" ) ax . invert_xaxis () plt . show () 2021-04-14T13:35:23.285664 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} The plot above presents the averaged CV Validation AUC of model performance for each round of the RFE process in both ShapRFECV and RFECV. The optimal number of features is 21 for the former, and 13 for the latter. Now we will compare the performance of the model trained on: All 50 available features (baseline), 13 features selected by RFECV (final), 21 features selected by ShapRFECV (final), 13 feature selected by ShapRFECV (baseline). n_features_shap = 21 n_features_rfecv = rfe . n_features_ # Calculate the AUC for the models with different feature sets test_auc_full = clf . fit ( X_train , y_train ) . score ( X_test , y_test ) val_auc_full = np . mean ( cross_val_score ( clf , X_train , y_train , cv = 10 )) rfe_features_set = X_train . columns [ rfe . support_ ] test_auc_rfe = clf . fit ( X_train [ rfe_features_set ], y_train ) . score ( X_test [ rfe_features_set ], y_test ) val_auc_rfe = rfe . grid_scores_ [ n_features_rfecv ] shap_feature_set = X_train . columns [ shap_elimination . get_reduced_features_set ( n_features_shap )] test_auc_shap = clf . fit ( X_train [ shap_feature_set ], y_train ) . score ( X_test [ shap_feature_set ], y_test ) val_auc_shap = shap_report [ shap_report . num_features == n_features_shap ][ 'val_metric_mean' ] . values [ 0 ] shap_feature_set_size_rfe = X_train . columns [ shap_elimination . get_reduced_features_set ( n_features_rfecv )] test_auc_shap_size_rfe = clf . fit ( X_train [ shap_feature_set_size_rfe ], y_train ) . score ( X_test [ shap_feature_set_size_rfe ], y_test ) val_auc_shap_size_rfe = shap_report [ shap_report . num_features == n_features_rfecv ][ 'val_metric_mean' ] . values [ 0 ] # Plot Test and Validation Performance variants = ( 'All 50 features' , f 'RFECV { n_features_rfecv } features' , f 'ShapRFECV { n_features_shap } features' , f 'ShapRFECV { n_features_rfecv } features' ) results_test = [ test_auc_full , test_auc_rfe , test_auc_shap , test_auc_shap_size_rfe ] results_val = [ val_auc_full , val_auc_rfe , val_auc_shap , val_auc_shap_size_rfe ] ax = pd . DataFrame ({ 'CV Validation AUC' : results_val , 'Test AUC' : results_test }, index = variants ) . plot . bar ( ylim = ( 0.5 , 0.6 ), rot = 10 , title = 'Comparison of RFECV and ShapRFECV' , figsize = ( 10 , 5 )) plt . axhline ( y = 0.5 ) ax . set_ylabel ( \"Model Performance\" ) plt . show () 2021-04-14T13:35:24.307210 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} As shown in the plot, ShapRFECV provides superior results for both: CV Validation and Test AUC, compared to RFECV and the baseline model with all the available features. Not only the introduced method allows to eliminate features without the loss in performance, but also it may increase the performance of the model. When it comes to time required to perform the feature selection in the experiment above, RFECV takes 6.11 s \u00b1 33.7 ms, while ShapRFECV takes 10.1 s \u00b1 72.8 mss, which shows that the latter is more computation expensive, due to SHAP values calculation.","title":"ShapRFECV vs sklearn RFECV"},{"location":"discussion/nb_rfecv_vs_shaprfecv.html#shaprfecv-vs-sklearn-rfecv","text":"In this section we will compare the performance of the model trained on the features selected using the probatus ShapRFECV and the sklearn RFECV . In order to compare them let's first prepare a dataset, and a model that will be applied: from probatus.feature_elimination import ShapRFECV import numpy as np import pandas as pd import lightgbm from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split , cross_val_score from sklearn.feature_selection import RFECV import matplotlib.pyplot as plt # Prepare train and test data: X , y = make_classification ( n_samples = 10000 , class_sep = 0.1 , n_informative = 40 , n_features = 50 , random_state = 0 , n_clusters_per_class = 10 ) X = pd . DataFrame ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.5 , random_state = 42 ) # Set up the model: clf = lightgbm . LGBMClassifier ( n_estimators = 10 , num_leaves = 7 ) Now, we can run ShapRFECV and RFECV with the same parameters, to extract the optimal feature sets: # Run RFECV and ShapRFECV with the same parameters rfe = RFECV ( clf , step = 1 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 ) . fit ( X_train , y_train ) shap_elimination = ShapRFECV ( clf = clf , step = 1 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 ) shap_report = shap_elimination . fit_compute ( X_train , y_train ) # Compare the CV Validation AUC for different number of features in each method. ax = pd . DataFrame ({ 'RFECV Validation AUC' : list ( reversed ( rfe . grid_scores_ )), 'ShapRFECV Validation AUC' : shap_report [ 'val_metric_mean' ] . values . tolist ()}, index = shap_report [ 'num_features' ] . values . tolist ()) . plot ( ylim = ( 0.5 , 0.7 ), rot = 10 , title = 'Comparison of RFECV and ShapRFECV' , figsize = ( 10 , 5 )) ax . set_ylabel ( \"Model Performance\" ) ax . set_xlabel ( \"Number of features\" ) ax . invert_xaxis () plt . show () 2021-04-14T13:35:23.285664 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} The plot above presents the averaged CV Validation AUC of model performance for each round of the RFE process in both ShapRFECV and RFECV. The optimal number of features is 21 for the former, and 13 for the latter. Now we will compare the performance of the model trained on: All 50 available features (baseline), 13 features selected by RFECV (final), 21 features selected by ShapRFECV (final), 13 feature selected by ShapRFECV (baseline). n_features_shap = 21 n_features_rfecv = rfe . n_features_ # Calculate the AUC for the models with different feature sets test_auc_full = clf . fit ( X_train , y_train ) . score ( X_test , y_test ) val_auc_full = np . mean ( cross_val_score ( clf , X_train , y_train , cv = 10 )) rfe_features_set = X_train . columns [ rfe . support_ ] test_auc_rfe = clf . fit ( X_train [ rfe_features_set ], y_train ) . score ( X_test [ rfe_features_set ], y_test ) val_auc_rfe = rfe . grid_scores_ [ n_features_rfecv ] shap_feature_set = X_train . columns [ shap_elimination . get_reduced_features_set ( n_features_shap )] test_auc_shap = clf . fit ( X_train [ shap_feature_set ], y_train ) . score ( X_test [ shap_feature_set ], y_test ) val_auc_shap = shap_report [ shap_report . num_features == n_features_shap ][ 'val_metric_mean' ] . values [ 0 ] shap_feature_set_size_rfe = X_train . columns [ shap_elimination . get_reduced_features_set ( n_features_rfecv )] test_auc_shap_size_rfe = clf . fit ( X_train [ shap_feature_set_size_rfe ], y_train ) . score ( X_test [ shap_feature_set_size_rfe ], y_test ) val_auc_shap_size_rfe = shap_report [ shap_report . num_features == n_features_rfecv ][ 'val_metric_mean' ] . values [ 0 ] # Plot Test and Validation Performance variants = ( 'All 50 features' , f 'RFECV { n_features_rfecv } features' , f 'ShapRFECV { n_features_shap } features' , f 'ShapRFECV { n_features_rfecv } features' ) results_test = [ test_auc_full , test_auc_rfe , test_auc_shap , test_auc_shap_size_rfe ] results_val = [ val_auc_full , val_auc_rfe , val_auc_shap , val_auc_shap_size_rfe ] ax = pd . DataFrame ({ 'CV Validation AUC' : results_val , 'Test AUC' : results_test }, index = variants ) . plot . bar ( ylim = ( 0.5 , 0.6 ), rot = 10 , title = 'Comparison of RFECV and ShapRFECV' , figsize = ( 10 , 5 )) plt . axhline ( y = 0.5 ) ax . set_ylabel ( \"Model Performance\" ) plt . show () 2021-04-14T13:35:24.307210 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} As shown in the plot, ShapRFECV provides superior results for both: CV Validation and Test AUC, compared to RFECV and the baseline model with all the available features. Not only the introduced method allows to eliminate features without the loss in performance, but also it may increase the performance of the model. When it comes to time required to perform the feature selection in the experiment above, RFECV takes 6.11 s \u00b1 33.7 ms, while ShapRFECV takes 10.1 s \u00b1 72.8 mss, which shows that the latter is more computation expensive, due to SHAP values calculation.","title":"ShapRFECV vs sklearn RFECV"},{"location":"discussion/vision.html","text":"The Vision \u00b6 This page describes the main principles that drive the development of probatus as well as the general directions, in which the development of the package will be heading. The Purpose \u00b6 probatus has started as a side project of Data Scientists at ING Bank. Later, we have decided to open-source it, in order to share the tools and enable collaboration with the Data Science community. We mainly focus on analysing the following aspects of building classification models: - Model input: the quality of the dataset and how to prepare it for modelling, - Model performance: the quality of the model and stability of the results. - Model interpretation: understanding the model decision making, Our main goals are: - Continue maintaining the tools that we have built, and make sure that they are well documented and tested - Continuously extend functionality available in the package - Build a community of users, which use the package in day-to-day work and learn from each other, while contributing to probatus The Principles \u00b6 The main principles that drive development of probatus are the following Usefulness - any tool that we build should be useful for a broad range of users, Simplicity - simple to understand and analyse steps over state-of-the-art, Usability - the developed functionality must be have good documentation, consistent API and work flawlessly with scikit-learn compatible models, Reliability - the code that is available for the users should be well tested and reliable, and bugs should be fixed as soon as they are detected. The Roadmap \u00b6 The following issue keeps track of the features coming to probatus. We are open to new ideas, so if you can think of a feature that fits the vision, make an issue and help us further develop this package.","title":"Vision"},{"location":"discussion/vision.html#the-vision","text":"This page describes the main principles that drive the development of probatus as well as the general directions, in which the development of the package will be heading.","title":"The Vision"},{"location":"discussion/vision.html#the-purpose","text":"probatus has started as a side project of Data Scientists at ING Bank. Later, we have decided to open-source it, in order to share the tools and enable collaboration with the Data Science community. We mainly focus on analysing the following aspects of building classification models: - Model input: the quality of the dataset and how to prepare it for modelling, - Model performance: the quality of the model and stability of the results. - Model interpretation: understanding the model decision making, Our main goals are: - Continue maintaining the tools that we have built, and make sure that they are well documented and tested - Continuously extend functionality available in the package - Build a community of users, which use the package in day-to-day work and learn from each other, while contributing to probatus","title":"The Purpose"},{"location":"discussion/vision.html#the-principles","text":"The main principles that drive development of probatus are the following Usefulness - any tool that we build should be useful for a broad range of users, Simplicity - simple to understand and analyse steps over state-of-the-art, Usability - the developed functionality must be have good documentation, consistent API and work flawlessly with scikit-learn compatible models, Reliability - the code that is available for the users should be well tested and reliable, and bugs should be fixed as soon as they are detected.","title":"The Principles"},{"location":"discussion/vision.html#the-roadmap","text":"The following issue keeps track of the features coming to probatus. We are open to new ideas, so if you can think of a feature that fits the vision, make an issue and help us further develop this package.","title":"The Roadmap"},{"location":"howto/reproducibility.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How to ensure reproducibility of the results \u00b6 This page describes how to make sure that the analysis that you perform using probatus is fully reproducible. There are two factors that influence reproducibility of the results: Inputs of probatus modules, The random_state of probatus modules. The below sections cover how to ensure reproducibility of the results by controling these aspects. Inputs of probatus modules \u00b6 There are various parameters that modules of probatus take as input. Below we will cover the most often occurring ones. Static dataset \u00b6 When using probatus , one of the most crucial aspects is the provided dataset. Therefore, the first thing to do is to ensure that the passed dataset does not change along the way. Below is a code snipped of random data preparation. In sklearn, you can ensure this by setting the random_state parameter. You will probably use a different dataset in your projects, but always make sure that the input data is static. from sklearn.datasets import make_classification X , y = make_classification ( n_samples = 100 , n_features = 10 , random_state = 42 ) Static data splits \u00b6 Whenever you split the data in any way, you need to make sure that the splits are always the same. If you use the train_test_split functionality from sklearn, this can be enforced by setting the random_state parameter. Another crucial aspect is how you use the cv parameter, which defines the folds settings that you will use in the experiments. If the cv is set to an integer, you don't need to worry about it - the random_state of probatus will take care of it. However, if you want to pass a custom cv generator object, you have to set the random_state there as well. Below are some examples of static splits: from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import train_test_split # Static train/test split X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 ) # Static CV settings cv1 = 5 cv2 = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 ) Static classifier \u00b6 Most of probatus modules work with the provided classifiers. Whenever one needs to provide a not-fitted classifier, it is enough to set the random_state . However, if the classifier needs to be fitted beforehand, you have to make sure that the model training is reproducible as well. from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( random_state = 42 ) Static search CV for hyperparameter tuning \u00b6 Some of the modules e.g. ShapRFECV , allow you to perform optimization of the model. Whenever, you use such functionality, make sure that these classes have set the random_state . This way, in every round of optimization, you will explore the same set of parameter permutations. In case the search space is also generated based on randomness, make sure that the random_state is set to it as well. from sklearn.model_selection import RandomizedSearchCV param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'max_leaf_nodes' : [ 3 , 5 , 7 , 10 ], } search = RandomizedSearchCV ( clf , param_grid , n_iter = 1 , random_state = 42 ) Any other sources of randomness \u00b6 Before running probatus modules think about the inputs, and consider if there is any other type of randomness involved. If there is, one option to possibly solve the issue is setting the random seed at the beginning of the code. # Optional step import numpy as np np . random . seed ( 42 ) Reproducibility in probatus \u00b6 Most of the modules in probatus allow you to set the random_state . This setting essentially makes sure that any code that the functions operate on has a static flow. As long as it is seet and you ensure all other inputs do not cause additional fluctuations between runs, you can make sure that your results are reproducible. from probatus.feature_elimination import ShapRFECV shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = cv2 , scoring = 'roc_auc' , n_jobs = 3 , random_state = 42 ) report = shap_elimination . fit_compute ( X , y ) report [[ 'num_features' , 'eliminated_features' , 'val_metric_mean' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_features eliminated_features val_metric_mean 1 10 [8, 9] 0.983 2 8 [5] 0.969 3 7 [7] 0.984 4 6 [1] 0.979 5 5 [4] 0.978 6 4 [6] 0.989 7 3 [3] 0.991 8 2 [0] 0.956 9 1 [] 0.969","title":"Reproducibility of the results"},{"location":"howto/reproducibility.html#how-to-ensure-reproducibility-of-the-results","text":"This page describes how to make sure that the analysis that you perform using probatus is fully reproducible. There are two factors that influence reproducibility of the results: Inputs of probatus modules, The random_state of probatus modules. The below sections cover how to ensure reproducibility of the results by controling these aspects.","title":"How to ensure reproducibility of the results"},{"location":"howto/reproducibility.html#inputs-of-probatus-modules","text":"There are various parameters that modules of probatus take as input. Below we will cover the most often occurring ones.","title":"Inputs of probatus modules"},{"location":"howto/reproducibility.html#static-dataset","text":"When using probatus , one of the most crucial aspects is the provided dataset. Therefore, the first thing to do is to ensure that the passed dataset does not change along the way. Below is a code snipped of random data preparation. In sklearn, you can ensure this by setting the random_state parameter. You will probably use a different dataset in your projects, but always make sure that the input data is static. from sklearn.datasets import make_classification X , y = make_classification ( n_samples = 100 , n_features = 10 , random_state = 42 )","title":"Static dataset"},{"location":"howto/reproducibility.html#static-data-splits","text":"Whenever you split the data in any way, you need to make sure that the splits are always the same. If you use the train_test_split functionality from sklearn, this can be enforced by setting the random_state parameter. Another crucial aspect is how you use the cv parameter, which defines the folds settings that you will use in the experiments. If the cv is set to an integer, you don't need to worry about it - the random_state of probatus will take care of it. However, if you want to pass a custom cv generator object, you have to set the random_state there as well. Below are some examples of static splits: from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import train_test_split # Static train/test split X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 42 ) # Static CV settings cv1 = 5 cv2 = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 )","title":"Static data splits"},{"location":"howto/reproducibility.html#static-classifier","text":"Most of probatus modules work with the provided classifiers. Whenever one needs to provide a not-fitted classifier, it is enough to set the random_state . However, if the classifier needs to be fitted beforehand, you have to make sure that the model training is reproducible as well. from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( random_state = 42 )","title":"Static classifier"},{"location":"howto/reproducibility.html#static-search-cv-for-hyperparameter-tuning","text":"Some of the modules e.g. ShapRFECV , allow you to perform optimization of the model. Whenever, you use such functionality, make sure that these classes have set the random_state . This way, in every round of optimization, you will explore the same set of parameter permutations. In case the search space is also generated based on randomness, make sure that the random_state is set to it as well. from sklearn.model_selection import RandomizedSearchCV param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'max_leaf_nodes' : [ 3 , 5 , 7 , 10 ], } search = RandomizedSearchCV ( clf , param_grid , n_iter = 1 , random_state = 42 )","title":"Static search CV for hyperparameter tuning"},{"location":"howto/reproducibility.html#any-other-sources-of-randomness","text":"Before running probatus modules think about the inputs, and consider if there is any other type of randomness involved. If there is, one option to possibly solve the issue is setting the random seed at the beginning of the code. # Optional step import numpy as np np . random . seed ( 42 )","title":"Any other sources of randomness"},{"location":"howto/reproducibility.html#reproducibility-in-probatus","text":"Most of the modules in probatus allow you to set the random_state . This setting essentially makes sure that any code that the functions operate on has a static flow. As long as it is seet and you ensure all other inputs do not cause additional fluctuations between runs, you can make sure that your results are reproducible. from probatus.feature_elimination import ShapRFECV shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = cv2 , scoring = 'roc_auc' , n_jobs = 3 , random_state = 42 ) report = shap_elimination . fit_compute ( X , y ) report [[ 'num_features' , 'eliminated_features' , 'val_metric_mean' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_features eliminated_features val_metric_mean 1 10 [8, 9] 0.983 2 8 [5] 0.969 3 7 [7] 0.984 4 6 [1] 0.979 5 5 [4] 0.978 6 4 [6] 0.989 7 3 [3] 0.991 8 2 [0] 0.956 9 1 [] 0.969","title":"Reproducibility in probatus"},{"location":"tutorials/nb_binning.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Binning \u00b6 %% capture ! pip install probatus % matplotlib inline % config Completer . use_jedi = False % load_ext autoreload % autoreload 2 import pandas as pd import numpy as np import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.max_row' , 500 ) pd . set_option ( 'display.max_colwidth' , 200 ) This notebook explains how the various implemented binning strategies of probatus work. First, we import all binning strategies: from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer , TreeBucketer Let's create some data on which we want to apply the binning strategies. We choose a logistic function because it clearly supports the explanation on how binning strategies work. Moreover, the typical reliability curve for a trained random forest model has this shape and binning strategies could be used for probability calibration (see also the website of Scikit-learn on probability calibration ). def log_function ( x ): return 1 / ( 1 + np . exp ( - x )) x = [ log_function ( x ) for x in np . arange ( - 10 , 10 , 0.01 )] plt . plot ( x ); Simple binning \u00b6 The SimpleBucketer object creates binning of the values of x into equally sized bins. The attributes counts , the number of elements per bin, and boundaries , the actual boundaries that resulted from the binning strategy, are assigned to the object instance. In this example we choose to get 4 bins: mySimpleBucketer = SimpleBucketer ( bin_count = 4 ) mySimpleBucketer . fit ( x ) print ( 'counts' , mySimpleBucketer . counts_ ) print ( 'boundaries' , mySimpleBucketer . boundaries_ ) counts [891 109 110 890] boundaries [4.53978687e-05 2.50022585e-01 4.99999772e-01 7.49976959e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = mySimpleBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements in the tails of the data is larger than in the middle: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' ); Quantile binning \u00b6 The QuantileBucketer object creates bins that all contain an equal amount of samples myQuantileBucketer = QuantileBucketer ( bin_count = 4 ) myQuantileBucketer . fit ( x ) print ( 'counts' , myQuantileBucketer . counts_ ) print ( 'boundaries' , myQuantileBucketer . boundaries_ ) counts [500 500 500 500] boundaries [4.53978687e-05 6.67631251e-03 4.98750010e-01 9.93257042e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = myQuantileBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements is the same in all bins: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' ); Binning by agglomerative clustering \u00b6 The AgglomerativeBucketer class applies the Scikit-Learn AgglomerativeClustering algorithm to the data and uses the clusters to determine the bins. We use different data to show the value of this algoritm; we create the following distribution: x_agglomerative = np . append ( np . random . normal ( 0 , 1 , size = 1000 ), np . random . normal ( 6 , 0.2 , size = 50 )) plt . hist ( x_agglomerative , bins = 20 ); plt . title ( 'Sample data' ); When we apply the AgglomerativeBucketer algorithm with 2 bins, we see that the algorithm nicely creates a split in between the two centers myAgglomerativeBucketer = AgglomerativeBucketer ( bin_count = 2 ) myAgglomerativeBucketer . fit ( x_agglomerative ) print ( 'counts' , myAgglomerativeBucketer . counts_ ) print ( 'boundaries' , myAgglomerativeBucketer . boundaries_ ) counts [1000 50] boundaries [-2.71525699097944, 4.582406874429196, 6.454492599188006] df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = myAgglomerativeBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) Note that the SimpleBucketer strategy would just have created a split in the middle of the maximum and the minimum (at about 1.75). The QuantileBucketer strategy had created two bins with equal amount of elements in it, resulting in a split at around 0. counts_agglomerative_simple , boundaries_agglomerative_simple = SimpleBucketer . simple_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_simple , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) counts_agglomerative_quantile , boundaries_agglomerative_quantile = QuantileBucketer . quantile_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_quantile , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) Binning with Decision Trees \u00b6 Binning with decision trees leverages the information of a binary feature or the binary target in order to create buckets that have a significantly different proportion of the binary feature/target. It works by fitting a tree on 1 feature only. It leverages the properties of the split finder algorithm in the decision tree. The splits are done to maximize the gini/entropy. The leaves approximate the optimal bins. The example below shows a distribution defined by a step function def make_step_function ( x ): if x < 4 : return 0.001 elif x < 6 : return 0.3 elif x < 8 : return 0.5 elif x < 9 : return 0.95 else : return 0.9999 x = np . arange ( 0 , 10 , 0.001 ) probs = [ make_step_function ( x_ ) for x_ in x ] y = np . array ([ 1 if np . random . rand () < prob else 0 for prob in probs ]) fig , ax = plt . subplots () ax2 = ax . twinx () ax . hist ( x [ y == 0 ], alpha = 0.15 ) ax . hist ( x [ y == 1 ], alpha = 0.15 ) ax2 . plot ( x , probs , color = 'black' ) plt . show () The light blue histogram indicates the distribution of class 0 ( y=0 ), while the light orange histogram indicates the distribution of class 1 ( y=1 ). The black line indicates the probability function that isused to assign class 0 or 1. In this toy example, it's a step function. # Try a tree bucketer tb = TreeBucketer ( inf_edges = True , max_depth = 4 , criterion = 'entropy' , min_samples_leaf = 400 , #Minimum number of entries in the bins min_impurity_decrease = 0.001 ) . fit ( x , y ) counts_tree , boundaries_tree = tb . counts_ , tb . boundaries_ df_tree = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_tree [ 'label' ] = pd . cut ( x , bins = boundaries_tree , include_lowest = True ) # Try a quantile bucketer myQuantileBucketer = QuantileBucketer ( bin_count = 16 ) myQuantileBucketer . fit ( x ) q_boundaries = myQuantileBucketer . boundaries_ q_counts = myQuantileBucketer . counts_ df_q = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_q [ 'label' ] = pd . cut ( x , bins = q_boundaries , include_lowest = True ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) for label in df_tree . label . unique (): df_tree [ df_tree . label == label ] . plot ( ax = ax [ 0 ], x = 'x' , y = 'probs' , legend = False ) ax [ 0 ] . scatter ( df_tree [ df_tree . label == label ][ 'x' ] . mean (), df_tree [ df_tree . label == label ][ 'y' ] . mean ()) ax [ 0 ] . set_title ( \"Tree bucketer\" ) for label in df_q . label . unique (): df_q [ df_q . label == label ] . plot ( ax = ax [ 1 ], x = 'x' , y = 'probs' , legend = False ) ax [ 1 ] . scatter ( df_q [ df_q . label == label ][ 'x' ] . mean (), df_q [ df_q . label == label ][ 'y' ] . mean ()) ax [ 1 ] . set_title ( \"Quantile bucketer\" ) print ( f \"counts by TreeBucketer: { counts_tree } \" ) print ( f \"counts by QuantileBucketer: { q_counts } \" ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 17985.87it/s] counts by TreeBucketer: [4000 1998 2001 936 1065] counts by QuantileBucketer: [625 625 625 625 625 625 625 625 625 625 625 625 625 625 625 625] Comparing the TreeBucketer and the QuantileBucketer (the dots compare the average distribution of class 1 in the bin): Each buckets obtained by the TreeBucketer follow the probability distribution (i.e. the entries in the bucket have the same probability of being class 1). On the contrary, the QuantileBucketer splits the values below 4 in 6 buckets, which all have the same probability of being class 1. Note also that the tree is grown with the maximum depth of 4, which potentially lets it grow up to 16 buckets ($2^4$). The learned tree is visualized below, whreere the splitting according to the step function is visualized clearly. from sklearn.tree import plot_tree fig , ax = plt . subplots ( figsize = ( 12 , 5 )) tre_out = plot_tree ( tb . tree , ax = ax )","title":"Nb binning"},{"location":"tutorials/nb_binning.html#binning","text":"%% capture ! pip install probatus % matplotlib inline % config Completer . use_jedi = False % load_ext autoreload % autoreload 2 import pandas as pd import numpy as np import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.max_row' , 500 ) pd . set_option ( 'display.max_colwidth' , 200 ) This notebook explains how the various implemented binning strategies of probatus work. First, we import all binning strategies: from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer , TreeBucketer Let's create some data on which we want to apply the binning strategies. We choose a logistic function because it clearly supports the explanation on how binning strategies work. Moreover, the typical reliability curve for a trained random forest model has this shape and binning strategies could be used for probability calibration (see also the website of Scikit-learn on probability calibration ). def log_function ( x ): return 1 / ( 1 + np . exp ( - x )) x = [ log_function ( x ) for x in np . arange ( - 10 , 10 , 0.01 )] plt . plot ( x );","title":"Binning"},{"location":"tutorials/nb_binning.html#simple-binning","text":"The SimpleBucketer object creates binning of the values of x into equally sized bins. The attributes counts , the number of elements per bin, and boundaries , the actual boundaries that resulted from the binning strategy, are assigned to the object instance. In this example we choose to get 4 bins: mySimpleBucketer = SimpleBucketer ( bin_count = 4 ) mySimpleBucketer . fit ( x ) print ( 'counts' , mySimpleBucketer . counts_ ) print ( 'boundaries' , mySimpleBucketer . boundaries_ ) counts [891 109 110 890] boundaries [4.53978687e-05 2.50022585e-01 4.99999772e-01 7.49976959e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = mySimpleBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements in the tails of the data is larger than in the middle: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' );","title":"Simple binning"},{"location":"tutorials/nb_binning.html#quantile-binning","text":"The QuantileBucketer object creates bins that all contain an equal amount of samples myQuantileBucketer = QuantileBucketer ( bin_count = 4 ) myQuantileBucketer . fit ( x ) print ( 'counts' , myQuantileBucketer . counts_ ) print ( 'boundaries' , myQuantileBucketer . boundaries_ ) counts [500 500 500 500] boundaries [4.53978687e-05 6.67631251e-03 4.98750010e-01 9.93257042e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = myQuantileBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements is the same in all bins: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' );","title":"Quantile binning"},{"location":"tutorials/nb_binning.html#binning-by-agglomerative-clustering","text":"The AgglomerativeBucketer class applies the Scikit-Learn AgglomerativeClustering algorithm to the data and uses the clusters to determine the bins. We use different data to show the value of this algoritm; we create the following distribution: x_agglomerative = np . append ( np . random . normal ( 0 , 1 , size = 1000 ), np . random . normal ( 6 , 0.2 , size = 50 )) plt . hist ( x_agglomerative , bins = 20 ); plt . title ( 'Sample data' ); When we apply the AgglomerativeBucketer algorithm with 2 bins, we see that the algorithm nicely creates a split in between the two centers myAgglomerativeBucketer = AgglomerativeBucketer ( bin_count = 2 ) myAgglomerativeBucketer . fit ( x_agglomerative ) print ( 'counts' , myAgglomerativeBucketer . counts_ ) print ( 'boundaries' , myAgglomerativeBucketer . boundaries_ ) counts [1000 50] boundaries [-2.71525699097944, 4.582406874429196, 6.454492599188006] df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = myAgglomerativeBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) Note that the SimpleBucketer strategy would just have created a split in the middle of the maximum and the minimum (at about 1.75). The QuantileBucketer strategy had created two bins with equal amount of elements in it, resulting in a split at around 0. counts_agglomerative_simple , boundaries_agglomerative_simple = SimpleBucketer . simple_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_simple , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) counts_agglomerative_quantile , boundaries_agglomerative_quantile = QuantileBucketer . quantile_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_quantile , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax )","title":"Binning by agglomerative clustering"},{"location":"tutorials/nb_binning.html#binning-with-decision-trees","text":"Binning with decision trees leverages the information of a binary feature or the binary target in order to create buckets that have a significantly different proportion of the binary feature/target. It works by fitting a tree on 1 feature only. It leverages the properties of the split finder algorithm in the decision tree. The splits are done to maximize the gini/entropy. The leaves approximate the optimal bins. The example below shows a distribution defined by a step function def make_step_function ( x ): if x < 4 : return 0.001 elif x < 6 : return 0.3 elif x < 8 : return 0.5 elif x < 9 : return 0.95 else : return 0.9999 x = np . arange ( 0 , 10 , 0.001 ) probs = [ make_step_function ( x_ ) for x_ in x ] y = np . array ([ 1 if np . random . rand () < prob else 0 for prob in probs ]) fig , ax = plt . subplots () ax2 = ax . twinx () ax . hist ( x [ y == 0 ], alpha = 0.15 ) ax . hist ( x [ y == 1 ], alpha = 0.15 ) ax2 . plot ( x , probs , color = 'black' ) plt . show () The light blue histogram indicates the distribution of class 0 ( y=0 ), while the light orange histogram indicates the distribution of class 1 ( y=1 ). The black line indicates the probability function that isused to assign class 0 or 1. In this toy example, it's a step function. # Try a tree bucketer tb = TreeBucketer ( inf_edges = True , max_depth = 4 , criterion = 'entropy' , min_samples_leaf = 400 , #Minimum number of entries in the bins min_impurity_decrease = 0.001 ) . fit ( x , y ) counts_tree , boundaries_tree = tb . counts_ , tb . boundaries_ df_tree = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_tree [ 'label' ] = pd . cut ( x , bins = boundaries_tree , include_lowest = True ) # Try a quantile bucketer myQuantileBucketer = QuantileBucketer ( bin_count = 16 ) myQuantileBucketer . fit ( x ) q_boundaries = myQuantileBucketer . boundaries_ q_counts = myQuantileBucketer . counts_ df_q = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_q [ 'label' ] = pd . cut ( x , bins = q_boundaries , include_lowest = True ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) for label in df_tree . label . unique (): df_tree [ df_tree . label == label ] . plot ( ax = ax [ 0 ], x = 'x' , y = 'probs' , legend = False ) ax [ 0 ] . scatter ( df_tree [ df_tree . label == label ][ 'x' ] . mean (), df_tree [ df_tree . label == label ][ 'y' ] . mean ()) ax [ 0 ] . set_title ( \"Tree bucketer\" ) for label in df_q . label . unique (): df_q [ df_q . label == label ] . plot ( ax = ax [ 1 ], x = 'x' , y = 'probs' , legend = False ) ax [ 1 ] . scatter ( df_q [ df_q . label == label ][ 'x' ] . mean (), df_q [ df_q . label == label ][ 'y' ] . mean ()) ax [ 1 ] . set_title ( \"Quantile bucketer\" ) print ( f \"counts by TreeBucketer: { counts_tree } \" ) print ( f \"counts by QuantileBucketer: { q_counts } \" ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 17985.87it/s] counts by TreeBucketer: [4000 1998 2001 936 1065] counts by QuantileBucketer: [625 625 625 625 625 625 625 625 625 625 625 625 625 625 625 625] Comparing the TreeBucketer and the QuantileBucketer (the dots compare the average distribution of class 1 in the bin): Each buckets obtained by the TreeBucketer follow the probability distribution (i.e. the entries in the bucket have the same probability of being class 1). On the contrary, the QuantileBucketer splits the values below 4 in 6 buckets, which all have the same probability of being class 1. Note also that the tree is grown with the maximum depth of 4, which potentially lets it grow up to 16 buckets ($2^4$). The learned tree is visualized below, whreere the splitting according to the step function is visualized clearly. from sklearn.tree import plot_tree fig , ax = plt . subplots ( figsize = ( 12 , 5 )) tre_out = plot_tree ( tb . tree , ax = ax )","title":"Binning with Decision Trees"},{"location":"tutorials/nb_custom_scoring.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Custom Scoring Metrics \u00b6 In many features of probatus, the user can provide the scoring parameter. The parameter can be one of the following: String indicating the scoring metric, one of the classification scorers names in sklearn . Object of a class Scorer from probatus.utils.Scorer. This object encapsulates the scoring metric name and the scorer used to calculate the model performance. The following tutorial will present how the scoring parameter can be used on the example of a Resemblance Model. Setup \u00b6 Let's prepare some data: %% capture ! pip install probatus from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import SHAPImportanceResemblance import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import pandas as pd from probatus.utils import Scorer from sklearn.metrics import make_scorer # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X1 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 )[ 0 ], columns = feature_names ) X2 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , shift = 0.5 , random_state = 0 )[ 0 ], columns = feature_names ) # Prepare model clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 ) Standard metrics \u00b6 Now, we can set the scoring parameter as a string: rm = SHAPImportanceResemblance ( clf , scoring = 'accuracy' ) feature_importance , train_score , test_score = rm . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) print ( f 'Train Accuracy: { np . round ( train_score , 3 ) } , \\n ' \\ f 'Test Accuracy: { np . round ( test_score , 3 ) } .' ) Train Accuracy: 0.714, Test Accuracy: 0.706. Custom metric \u00b6 Let's make a custom function (in this case accuracy as well), that we want to use for scoring and use it within ShapImportanceResemblance def custom_metric ( y_true , y_pred ): return np . sum ( y_true == y_pred ) / len ( y_true ) scorer = Scorer ( 'custom_metric' , custom_scorer = make_scorer ( custom_metric )) rm2 = SHAPImportanceResemblance ( clf , scoring = scorer ) feature_importance2 , train_score2 , test_score2 = rm2 . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) print ( f 'Train custom_metric: { np . round ( train_score2 , 3 ) } , \\n ' \\ f 'Test custom_metric: { np . round ( test_score2 , 3 ) } .' ) Train Score: 0.714, Test Score: 0.706. figure = rm2 . plot ()","title":"Custom Scoring Metrics"},{"location":"tutorials/nb_custom_scoring.html#custom-scoring-metrics","text":"In many features of probatus, the user can provide the scoring parameter. The parameter can be one of the following: String indicating the scoring metric, one of the classification scorers names in sklearn . Object of a class Scorer from probatus.utils.Scorer. This object encapsulates the scoring metric name and the scorer used to calculate the model performance. The following tutorial will present how the scoring parameter can be used on the example of a Resemblance Model.","title":"Custom Scoring Metrics"},{"location":"tutorials/nb_custom_scoring.html#setup","text":"Let's prepare some data: %% capture ! pip install probatus from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import SHAPImportanceResemblance import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import pandas as pd from probatus.utils import Scorer from sklearn.metrics import make_scorer # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X1 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 )[ 0 ], columns = feature_names ) X2 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , shift = 0.5 , random_state = 0 )[ 0 ], columns = feature_names ) # Prepare model clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 )","title":"Setup"},{"location":"tutorials/nb_custom_scoring.html#standard-metrics","text":"Now, we can set the scoring parameter as a string: rm = SHAPImportanceResemblance ( clf , scoring = 'accuracy' ) feature_importance , train_score , test_score = rm . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) print ( f 'Train Accuracy: { np . round ( train_score , 3 ) } , \\n ' \\ f 'Test Accuracy: { np . round ( test_score , 3 ) } .' ) Train Accuracy: 0.714, Test Accuracy: 0.706.","title":"Standard metrics"},{"location":"tutorials/nb_custom_scoring.html#custom-metric","text":"Let's make a custom function (in this case accuracy as well), that we want to use for scoring and use it within ShapImportanceResemblance def custom_metric ( y_true , y_pred ): return np . sum ( y_true == y_pred ) / len ( y_true ) scorer = Scorer ( 'custom_metric' , custom_scorer = make_scorer ( custom_metric )) rm2 = SHAPImportanceResemblance ( clf , scoring = scorer ) feature_importance2 , train_score2 , test_score2 = rm2 . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) print ( f 'Train custom_metric: { np . round ( train_score2 , 3 ) } , \\n ' \\ f 'Test custom_metric: { np . round ( test_score2 , 3 ) } .' ) Train Score: 0.714, Test Score: 0.706. figure = rm2 . plot ()","title":"Custom metric"},{"location":"tutorials/nb_distribution_statistics.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Univariate Distribution Similarity \u00b6 There are many situations when you want to perform univariate distribution comparison of a given feature, e.g. stability of the feature over different months. In order to do that, you can use statistical tests. In this tutorial we present how to easily do this using the DistributionStatistics class, and with the statistical tests directly. The available statistical tests in probatus.stat_tests are: - Epps-Singleton ('ES') - Kolmogorov-Smirnov statistic ('KS') - Population Stability Index ('PSI') - Shapiro-Wilk based difference statistic ('SW') - Anderson-Darling TS ('AD') You can perform all these tests using a convenient wrapper class called DistributionStatistics . In this tutorial we will focus on how to perform two useful tests: Population Stability Index (widely applied in banking industry) and Kolmogorov-Smirnov. Setup \u00b6 %% capture ! pip install probatus % load_ext autoreload % autoreload 2 import numpy as np import pandas as pd import matplotlib.pyplot as plt from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer from probatus.stat_tests import DistributionStatistics , psi , ks Let's define some test distributions and visualize them. For these examples, we will use a normal distribution and a shifted version of the same distribution. counts = 1000 np . random . seed ( 0 ) d1 = pd . Series ( np . random . normal ( size = counts ), name = 'feature_1' ) d2 = pd . Series ( np . random . normal ( loc = 0.5 , size = counts ), name = 'feature_1' ) from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ d1 , d2 ] sample_names = [ 'expected' , 'actual' ] plot_distributions_of_feature ( feature_distributions , sample_names = sample_names , plot_perc_outliers_removed = 0.01 ) Binning - QuantileBucketer \u00b6 To visualize the data, we will bin the data using a quantile bucketer, available in the probatus.binning module. Binning is used by all the stats_tests in order to group observations. bins = 10 myBucketer = QuantileBucketer ( bins ) d1_bincounts = myBucketer . fit_compute ( d1 ) d2_bincounts = myBucketer . compute ( d2 ) print ( \"Bincounts for d1 and d2:\" ) print ( d1_bincounts ) print ( d2_bincounts ) Bincounts for d1 and d2: [100 100 100 100 100 100 100 100 100 100] [ 25 62 50 68 76 90 84 169 149 217] Let's plot the distribution for which we will calculate the statistics. plt . figure ( figsize = ( 20 , 5 )) plt . bar ( range ( 0 , len ( d1_bincounts )), d1_bincounts , label = 'd1: expected' ) plt . bar ( range ( 0 , len ( d2_bincounts )), d2_bincounts , label = 'd2: actual' , alpha = 0.5 ) plt . title ( 'PSI (bucketed)' , fontsize = 16 , fontweight = 'bold' ) plt . legend ( fontsize = 15 ) plt . show () By visualizing the bins, we can already notice that the distributions are different. Let's use the statistical test to prove that. PSI - Population Stability Index \u00b6 The population stability index ( Karakoulas, 2004 ) has long been used to evaluate distribution similarity in the banking industry, while developing credit decision models. In probatus we have implemented the PSI according to Yurdakul 2018 , which derives a p-value, based on the hard to interpret PSI statistic. Using the p-value is a more reliable choice, because the banking industry-standard PSI critical values of 0.1 and 0.25 are unreliable heuristics as there is a strong dependency on sample sizes and number of bins. Aside from these heuristics, the PSI value is not easily interpretable in the context of common statistical frameworks (like a p-value or confidence levels). psi_value , p_value = psi ( d1_bincounts , d2_bincounts , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed. Based on the above test, the distribution between the two samples significantly differ. Not only is the PSI statistic above the commonly used critical value, but also the p-value shows a very high confidence. PSI with DistributionStatistics \u00b6 Using the DistributionStatistics class one can apply the above test without the need to manually perform the binning. We initialize a DistributionStatistics instance with the desired test, binning_strategy (or choose \"default\" to choose the test's most appropriate binning strategy) and the number of bins. Then we start the test with the unbinned values as input. distribution_test = DistributionStatistics ( \"psi\" , binning_strategy = \"default\" , bin_count = 10 ) psi_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed. KS: Kolmogorov-Smirnov with DistributionStatistics \u00b6 The Kolmogorov-Smirnov test compares two distributions by calculating the maximum difference of the two samples' distribution functions, as illustrated by the black arrow in the following figure. The KS test is available in probatus.stat_tests.ks . The main advantage of this method is its sensitivity to differences in both location and shape of the empirical cumulative distribution functions of the two samples. The main disadvantages are that: it works for continuous distributions (unless modified, e.g. see ( Jeng 2006 )); in large samples, small and unimportant differences can be statistically significant ( Taplin & Hunt 2019 ); and finally in small samples, large and important differences can be statistically insignificant ( Taplin & Hunt 2019 ). As before, using the test requires you to perform the binning beforehand k_value , p_value = ks ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different. Again, we can also choose to combine the binning and the statistical test using the DistributionStatistics class. distribution_test = DistributionStatistics ( \"ks\" , binning_strategy = None ) ks_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different. AutoDist \u00b6 from probatus.stat_tests import AutoDist Multiple statistics can automatically be calculated using AutoDist . To show this, let's create two new dataframes with two features each. size , n_features = 100 , 2 df1 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) df2 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) We can now specify the statistical tests we want to perform and the binning strategies to perform. We can also set both of these variables to 'all' or binning strategies to 'default' to use the default binning strategy for every chosen statistical test. statistical_tests = [ \"KS\" , \"PSI\" ] binning_strategies = \"default\" Let's compute the statistics and their p_values: myAutoDist = AutoDist ( statistical_tests = statistical_tests , binning_strategies = binning_strategies , bin_count = 10 ) myAutoDist . compute ( df1 , df2 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 140.75it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column p_value_KS_no_bucketing_0 p_value_PSI_quantilebucketer_10 statistic_KS_no_bucketing_0 statistic_PSI_quantilebucketer_10 0 feat_0 0.815415 0.556756 0.09 0.192113 1 feat_1 0.281942 0.989078 0.14 0.374575","title":"Univariate Sample Similarity"},{"location":"tutorials/nb_distribution_statistics.html#univariate-distribution-similarity","text":"There are many situations when you want to perform univariate distribution comparison of a given feature, e.g. stability of the feature over different months. In order to do that, you can use statistical tests. In this tutorial we present how to easily do this using the DistributionStatistics class, and with the statistical tests directly. The available statistical tests in probatus.stat_tests are: - Epps-Singleton ('ES') - Kolmogorov-Smirnov statistic ('KS') - Population Stability Index ('PSI') - Shapiro-Wilk based difference statistic ('SW') - Anderson-Darling TS ('AD') You can perform all these tests using a convenient wrapper class called DistributionStatistics . In this tutorial we will focus on how to perform two useful tests: Population Stability Index (widely applied in banking industry) and Kolmogorov-Smirnov.","title":"Univariate Distribution Similarity"},{"location":"tutorials/nb_distribution_statistics.html#setup","text":"%% capture ! pip install probatus % load_ext autoreload % autoreload 2 import numpy as np import pandas as pd import matplotlib.pyplot as plt from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer from probatus.stat_tests import DistributionStatistics , psi , ks Let's define some test distributions and visualize them. For these examples, we will use a normal distribution and a shifted version of the same distribution. counts = 1000 np . random . seed ( 0 ) d1 = pd . Series ( np . random . normal ( size = counts ), name = 'feature_1' ) d2 = pd . Series ( np . random . normal ( loc = 0.5 , size = counts ), name = 'feature_1' ) from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ d1 , d2 ] sample_names = [ 'expected' , 'actual' ] plot_distributions_of_feature ( feature_distributions , sample_names = sample_names , plot_perc_outliers_removed = 0.01 )","title":"Setup"},{"location":"tutorials/nb_distribution_statistics.html#binning-quantilebucketer","text":"To visualize the data, we will bin the data using a quantile bucketer, available in the probatus.binning module. Binning is used by all the stats_tests in order to group observations. bins = 10 myBucketer = QuantileBucketer ( bins ) d1_bincounts = myBucketer . fit_compute ( d1 ) d2_bincounts = myBucketer . compute ( d2 ) print ( \"Bincounts for d1 and d2:\" ) print ( d1_bincounts ) print ( d2_bincounts ) Bincounts for d1 and d2: [100 100 100 100 100 100 100 100 100 100] [ 25 62 50 68 76 90 84 169 149 217] Let's plot the distribution for which we will calculate the statistics. plt . figure ( figsize = ( 20 , 5 )) plt . bar ( range ( 0 , len ( d1_bincounts )), d1_bincounts , label = 'd1: expected' ) plt . bar ( range ( 0 , len ( d2_bincounts )), d2_bincounts , label = 'd2: actual' , alpha = 0.5 ) plt . title ( 'PSI (bucketed)' , fontsize = 16 , fontweight = 'bold' ) plt . legend ( fontsize = 15 ) plt . show () By visualizing the bins, we can already notice that the distributions are different. Let's use the statistical test to prove that.","title":"Binning - QuantileBucketer"},{"location":"tutorials/nb_distribution_statistics.html#psi-population-stability-index","text":"The population stability index ( Karakoulas, 2004 ) has long been used to evaluate distribution similarity in the banking industry, while developing credit decision models. In probatus we have implemented the PSI according to Yurdakul 2018 , which derives a p-value, based on the hard to interpret PSI statistic. Using the p-value is a more reliable choice, because the banking industry-standard PSI critical values of 0.1 and 0.25 are unreliable heuristics as there is a strong dependency on sample sizes and number of bins. Aside from these heuristics, the PSI value is not easily interpretable in the context of common statistical frameworks (like a p-value or confidence levels). psi_value , p_value = psi ( d1_bincounts , d2_bincounts , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed. Based on the above test, the distribution between the two samples significantly differ. Not only is the PSI statistic above the commonly used critical value, but also the p-value shows a very high confidence.","title":"PSI - Population Stability Index"},{"location":"tutorials/nb_distribution_statistics.html#psi-with-distributionstatistics","text":"Using the DistributionStatistics class one can apply the above test without the need to manually perform the binning. We initialize a DistributionStatistics instance with the desired test, binning_strategy (or choose \"default\" to choose the test's most appropriate binning strategy) and the number of bins. Then we start the test with the unbinned values as input. distribution_test = DistributionStatistics ( \"psi\" , binning_strategy = \"default\" , bin_count = 10 ) psi_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed.","title":"PSI with DistributionStatistics"},{"location":"tutorials/nb_distribution_statistics.html#ks-kolmogorov-smirnov-with-distributionstatistics","text":"The Kolmogorov-Smirnov test compares two distributions by calculating the maximum difference of the two samples' distribution functions, as illustrated by the black arrow in the following figure. The KS test is available in probatus.stat_tests.ks . The main advantage of this method is its sensitivity to differences in both location and shape of the empirical cumulative distribution functions of the two samples. The main disadvantages are that: it works for continuous distributions (unless modified, e.g. see ( Jeng 2006 )); in large samples, small and unimportant differences can be statistically significant ( Taplin & Hunt 2019 ); and finally in small samples, large and important differences can be statistically insignificant ( Taplin & Hunt 2019 ). As before, using the test requires you to perform the binning beforehand k_value , p_value = ks ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different. Again, we can also choose to combine the binning and the statistical test using the DistributionStatistics class. distribution_test = DistributionStatistics ( \"ks\" , binning_strategy = None ) ks_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different.","title":"KS: Kolmogorov-Smirnov with DistributionStatistics"},{"location":"tutorials/nb_distribution_statistics.html#autodist","text":"from probatus.stat_tests import AutoDist Multiple statistics can automatically be calculated using AutoDist . To show this, let's create two new dataframes with two features each. size , n_features = 100 , 2 df1 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) df2 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) We can now specify the statistical tests we want to perform and the binning strategies to perform. We can also set both of these variables to 'all' or binning strategies to 'default' to use the default binning strategy for every chosen statistical test. statistical_tests = [ \"KS\" , \"PSI\" ] binning_strategies = \"default\" Let's compute the statistics and their p_values: myAutoDist = AutoDist ( statistical_tests = statistical_tests , binning_strategies = binning_strategies , bin_count = 10 ) myAutoDist . compute ( df1 , df2 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 140.75it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column p_value_KS_no_bucketing_0 p_value_PSI_quantilebucketer_10 statistic_KS_no_bucketing_0 statistic_PSI_quantilebucketer_10 0 feat_0 0.815415 0.556756 0.09 0.192113 1 feat_1 0.281942 0.989078 0.14 0.374575","title":"AutoDist"},{"location":"tutorials/nb_imputation_comparison.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Imputation Comparison \u00b6 This notebook explains how the ImputationSelector class works in probatus . With ImputationSelector you can compare multiple imputation strategies and choose a strategy which works the best for a given model and a dataset. Currently ImputationSelector supports any scikit-learn compatible imputation strategy. For categorical variables the missing values are replaced by a missing token and OneHotEncoder is applied. The user-supplied imputation strategies are applied to numerical columns only. Support for user-supplied imputation strategies for categorical columns can be added in the future releases. Let us look at an example and start by importing all the required classes and methods. ###Install the packages #%%capture #!pip install probatus #!pip install lightgbm % matplotlib inline % load_ext autoreload % autoreload 2 import pandas as pd import numpy as np import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.max_row' , 500 ) pd . set_option ( 'display.max_colwidth' , 200 ) from probatus.missing_values.imputation import ImputationSelector from probatus.utils.missing_helpers import generate_MCAR import lightgbm as lgb from sklearn.linear_model import LogisticRegression from sklearn.experimental import enable_iterative_imputer from sklearn.impute import KNNImputer , SimpleImputer , IterativeImputer from sklearn.datasets import make_classification Let's create a classification dataset to apply the various imputation strategies. We'll use the probatus.utils.missing_helpers.generate_MCAR function to randomly add missing values to the dataset. n_features = 20 X , y = make_classification ( n_samples = 2000 , n_features = n_features , random_state = 123 , class_sep = 0.3 ) X = pd . DataFrame ( X , columns = [ \"f_\" + str ( i ) for i in range ( 0 , n_features )]) print ( f \"Shape of X,y : { X . shape } , { y . shape } \" ) Shape of X,y : (2000, 20),(2000,) X_missing = generate_MCAR ( X , missing = 0.2 ) missing_stats = pd . DataFrame ( X_missing . isnull () . mean ()) . T missing_stats . index = [ 'Missing %' ] missing_stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f_0 f_1 f_2 f_3 f_4 f_5 f_6 f_7 f_8 f_9 f_10 f_11 f_12 f_13 f_14 f_15 f_16 f_17 f_18 f_19 Missing % 0.226 0.1985 0.186 0.213 0.201 0.2045 0.196 0.1995 0.2095 0.195 0.2015 0.201 0.1835 0.176 0.199 0.193 0.1965 0.212 0.202 0.21 The data has approximately 20% missing values in each feature. Imputation Strategies \u00b6 Create a dictionary with all the strategies to compare. Also, create a classifier to use for evaluating various strategies. If the model supports handling of missing features by default then the model performance on an unimputed dataset is calculated. You can indicate that the model supports handling missing values by setting the parameter model_na_support=True . The model performance against the unimputed dataset can be found in No Imputation results. strategies = { 'KNN Imputer' : KNNImputer ( n_neighbors = 3 ), 'Median Imputer' : SimpleImputer ( strategy = 'median' , add_indicator = True ), 'Iterative Imputer' : IterativeImputer ( add_indicator = True , n_nearest_features = 5 , sample_posterior = True ) } clf = lgb . LGBMClassifier ( n_estimators = 2 ) cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 , random_state = 45 , model_na_support = True ) cmp . fit_compute ( X_missing , y ) result_plot = cmp . plot () 2021-03-15T09:32:06.660519 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} However if the model does not support missing values by default (e.g. LogisticRegression ), results for only the imputation strategies are calculated. clf = LogisticRegression () cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 ) cmp . fit_compute ( X_missing , y ) result_plot = cmp . plot () 2021-03-15T09:32:25.260269 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} You can also pass a sklearn pipeline instead of a classifier. from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline steps = [( 'scaler' , StandardScaler ()), ( 'LR' , LogisticRegression ())] clf = Pipeline ( steps ) cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 , model_na_support = False ) cmp . fit_compute ( X_missing , y ) result_plot = cmp . plot () 2021-03-15T09:32:52.987873 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Scikit Learn Compatible Imputers. \u00b6 You can also use any other scikit-learn compatible imputer as an imputing strategy. e.g. feature engine library provides a host of other imputing stratgies as well. You can pass them for comparision as well.","title":"Imputation Strategy Comparison"},{"location":"tutorials/nb_imputation_comparison.html#imputation-comparison","text":"This notebook explains how the ImputationSelector class works in probatus . With ImputationSelector you can compare multiple imputation strategies and choose a strategy which works the best for a given model and a dataset. Currently ImputationSelector supports any scikit-learn compatible imputation strategy. For categorical variables the missing values are replaced by a missing token and OneHotEncoder is applied. The user-supplied imputation strategies are applied to numerical columns only. Support for user-supplied imputation strategies for categorical columns can be added in the future releases. Let us look at an example and start by importing all the required classes and methods. ###Install the packages #%%capture #!pip install probatus #!pip install lightgbm % matplotlib inline % load_ext autoreload % autoreload 2 import pandas as pd import numpy as np import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.max_row' , 500 ) pd . set_option ( 'display.max_colwidth' , 200 ) from probatus.missing_values.imputation import ImputationSelector from probatus.utils.missing_helpers import generate_MCAR import lightgbm as lgb from sklearn.linear_model import LogisticRegression from sklearn.experimental import enable_iterative_imputer from sklearn.impute import KNNImputer , SimpleImputer , IterativeImputer from sklearn.datasets import make_classification Let's create a classification dataset to apply the various imputation strategies. We'll use the probatus.utils.missing_helpers.generate_MCAR function to randomly add missing values to the dataset. n_features = 20 X , y = make_classification ( n_samples = 2000 , n_features = n_features , random_state = 123 , class_sep = 0.3 ) X = pd . DataFrame ( X , columns = [ \"f_\" + str ( i ) for i in range ( 0 , n_features )]) print ( f \"Shape of X,y : { X . shape } , { y . shape } \" ) Shape of X,y : (2000, 20),(2000,) X_missing = generate_MCAR ( X , missing = 0.2 ) missing_stats = pd . DataFrame ( X_missing . isnull () . mean ()) . T missing_stats . index = [ 'Missing %' ] missing_stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f_0 f_1 f_2 f_3 f_4 f_5 f_6 f_7 f_8 f_9 f_10 f_11 f_12 f_13 f_14 f_15 f_16 f_17 f_18 f_19 Missing % 0.226 0.1985 0.186 0.213 0.201 0.2045 0.196 0.1995 0.2095 0.195 0.2015 0.201 0.1835 0.176 0.199 0.193 0.1965 0.212 0.202 0.21 The data has approximately 20% missing values in each feature.","title":"Imputation Comparison"},{"location":"tutorials/nb_imputation_comparison.html#imputation-strategies","text":"Create a dictionary with all the strategies to compare. Also, create a classifier to use for evaluating various strategies. If the model supports handling of missing features by default then the model performance on an unimputed dataset is calculated. You can indicate that the model supports handling missing values by setting the parameter model_na_support=True . The model performance against the unimputed dataset can be found in No Imputation results. strategies = { 'KNN Imputer' : KNNImputer ( n_neighbors = 3 ), 'Median Imputer' : SimpleImputer ( strategy = 'median' , add_indicator = True ), 'Iterative Imputer' : IterativeImputer ( add_indicator = True , n_nearest_features = 5 , sample_posterior = True ) } clf = lgb . LGBMClassifier ( n_estimators = 2 ) cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 , random_state = 45 , model_na_support = True ) cmp . fit_compute ( X_missing , y ) result_plot = cmp . plot () 2021-03-15T09:32:06.660519 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} However if the model does not support missing values by default (e.g. LogisticRegression ), results for only the imputation strategies are calculated. clf = LogisticRegression () cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 ) cmp . fit_compute ( X_missing , y ) result_plot = cmp . plot () 2021-03-15T09:32:25.260269 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} You can also pass a sklearn pipeline instead of a classifier. from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline steps = [( 'scaler' , StandardScaler ()), ( 'LR' , LogisticRegression ())] clf = Pipeline ( steps ) cmp = ImputationSelector ( clf = clf , strategies = strategies , cv = 5 , model_na_support = False ) cmp . fit_compute ( X_missing , y ) result_plot = cmp . plot () 2021-03-15T09:32:52.987873 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Imputation Strategies"},{"location":"tutorials/nb_imputation_comparison.html#scikit-learn-compatible-imputers","text":"You can also use any other scikit-learn compatible imputer as an imputing strategy. e.g. feature engine library provides a host of other imputing stratgies as well. You can pass them for comparision as well.","title":"Scikit Learn Compatible Imputers."},{"location":"tutorials/nb_metric_volatility.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Metric Volatility Estimation \u00b6 The estimation of AUC of your model could be influenced by, for instance, how you split your data. If another random seed was used, your AUC could be 3% lower. In order to understand how stable your model evaluation is, and what performance you can expect on average from your model, you can use the metric_volatility module. Setup \u00b6 %% capture ! pip install probatus from probatus.metric_volatility import TrainTestVolatility , SplitSeedVolatility , BootstrappedVolatility from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier X , y = make_classification ( n_samples = 1000 , n_features = 10 , random_state = 1 ) clf = RandomForestClassifier ( n_estimators = 2 , max_depth = 2 , random_state = 0 ) TrainTestVolatility \u00b6 The class that provides a wide functionality for experimentation with metric volatility is TrainTestVolatility. Please refer to the API reference for full description of available parameters. By default, the class performs a simple experiment, in which it computes the metrics on data split into train and test set with a different random seed at each iteration. Having computed the mean and standard deviation of the metrics, you can analyse the impact of random seed setting on your results and get a better estimation of performance on this dataset. When you run the fit() and compute() or fit_compute() , the experiment described above is performed and the report is returned. The train_mean and and test_mean show an averaged performance of the model, and delta_mean indicates on average how much the model overfits on the data. By looking at train_std , test_std , delta_std , you can assess the stability of these scores overall. High volatility on some of the splits may indicate the need to change the sizes of these splits or make changes to the model. # Basic functionality volatility = TrainTestVolatility ( clf , iterations = 50 ) volatility . fit_compute ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.831818 0.036407 0.816538 0.043732 0.01528 0.027516 The results above show quite unstable results, due to high train_std and test_std . However, the delta_mean is relatively, which indicates that the model might underfit and increasing the complexity of the model could bring improvements to the results. One can also present the distributions of train, test and deltas for each metric. The plots allows for a sensitivity analysis. axs = volatility . plot () In order to simplify the use of this class for the user, two convenience classes have been created to perform the main types of analyses with less parameters needed to be set by the user. SplitSeedVolatility \u00b6 The estimation of volatility is done in the same way as the default analysis described in TrainTestVolatility. The main advantage of using that class is a lower number of parameters to set. volatility = SplitSeedVolatility ( clf , iterations = 50 , test_prc = 0.5 ) volatility . fit_compute ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.827796 0.039356 0.804926 0.040501 0.02287 0.019264 BootstrappedVolatility \u00b6 This class allows to perform a different experiment. At each iteration, the train-test split is the same, however, the samples in both splits are bootstrapped (sampled with replacement). Thus, some of the samples might be omitted, and some will be used multiple times in a given run. With this experiment, you can estimate an average performance for a specific train-test split, as well as indicate how volatile the scores are towards certain samples within your splits. Moreover, you can experiment with the amount of data sampled in each split, to tweak the test split size. volatility = BootstrappedVolatility ( clf , iterations = 50 , scoring = [ 'accuracy' , 'roc_auc' ]) volatility . fit_compute ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std accuracy 0.823200 0.031567 0.765120 0.049303 0.058080 0.034091 roc_auc 0.852316 0.029762 0.785378 0.053647 0.066938 0.038386","title":"Model Metrics Volatility"},{"location":"tutorials/nb_metric_volatility.html#metric-volatility-estimation","text":"The estimation of AUC of your model could be influenced by, for instance, how you split your data. If another random seed was used, your AUC could be 3% lower. In order to understand how stable your model evaluation is, and what performance you can expect on average from your model, you can use the metric_volatility module.","title":"Metric Volatility Estimation"},{"location":"tutorials/nb_metric_volatility.html#setup","text":"%% capture ! pip install probatus from probatus.metric_volatility import TrainTestVolatility , SplitSeedVolatility , BootstrappedVolatility from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier X , y = make_classification ( n_samples = 1000 , n_features = 10 , random_state = 1 ) clf = RandomForestClassifier ( n_estimators = 2 , max_depth = 2 , random_state = 0 )","title":"Setup"},{"location":"tutorials/nb_metric_volatility.html#traintestvolatility","text":"The class that provides a wide functionality for experimentation with metric volatility is TrainTestVolatility. Please refer to the API reference for full description of available parameters. By default, the class performs a simple experiment, in which it computes the metrics on data split into train and test set with a different random seed at each iteration. Having computed the mean and standard deviation of the metrics, you can analyse the impact of random seed setting on your results and get a better estimation of performance on this dataset. When you run the fit() and compute() or fit_compute() , the experiment described above is performed and the report is returned. The train_mean and and test_mean show an averaged performance of the model, and delta_mean indicates on average how much the model overfits on the data. By looking at train_std , test_std , delta_std , you can assess the stability of these scores overall. High volatility on some of the splits may indicate the need to change the sizes of these splits or make changes to the model. # Basic functionality volatility = TrainTestVolatility ( clf , iterations = 50 ) volatility . fit_compute ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.831818 0.036407 0.816538 0.043732 0.01528 0.027516 The results above show quite unstable results, due to high train_std and test_std . However, the delta_mean is relatively, which indicates that the model might underfit and increasing the complexity of the model could bring improvements to the results. One can also present the distributions of train, test and deltas for each metric. The plots allows for a sensitivity analysis. axs = volatility . plot () In order to simplify the use of this class for the user, two convenience classes have been created to perform the main types of analyses with less parameters needed to be set by the user.","title":"TrainTestVolatility"},{"location":"tutorials/nb_metric_volatility.html#splitseedvolatility","text":"The estimation of volatility is done in the same way as the default analysis described in TrainTestVolatility. The main advantage of using that class is a lower number of parameters to set. volatility = SplitSeedVolatility ( clf , iterations = 50 , test_prc = 0.5 ) volatility . fit_compute ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.827796 0.039356 0.804926 0.040501 0.02287 0.019264","title":"SplitSeedVolatility"},{"location":"tutorials/nb_metric_volatility.html#bootstrappedvolatility","text":"This class allows to perform a different experiment. At each iteration, the train-test split is the same, however, the samples in both splits are bootstrapped (sampled with replacement). Thus, some of the samples might be omitted, and some will be used multiple times in a given run. With this experiment, you can estimate an average performance for a specific train-test split, as well as indicate how volatile the scores are towards certain samples within your splits. Moreover, you can experiment with the amount of data sampled in each split, to tweak the test split size. volatility = BootstrappedVolatility ( clf , iterations = 50 , scoring = [ 'accuracy' , 'roc_auc' ]) volatility . fit_compute ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std accuracy 0.823200 0.031567 0.765120 0.049303 0.058080 0.034091 roc_auc 0.852316 0.029762 0.785378 0.053647 0.066938 0.038386","title":"BootstrappedVolatility"},{"location":"tutorials/nb_sample_similarity.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Sample Similarity \u00b6 The goal of a Resemblance Model is understanding how different two samples are from a multivariate perspective. For instance, if you suspect that your Out-Of-Time Test set may have a different distribution than the In-Time Train set, you can detect that using the Resemblance Model. Having two samples X1 and X2 with the same set of features, one can analyse how well a model can recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to X1 dataset, and label 1 to X2 . Then, the data is shuffled and split into a Train split ( X_train , y_train ) and a Test split ( X_test , y_test ). The user provides a binary classifier that is then fitted on the Train split and evaluated on both Train and Test. Interpreting such a model allows us to understand which features and interactions between them differ between these two samples. It is crucial that the model does not overfit or underfit, because the interpretation of such a model will lead to the wrong conclusions. Therefore, you should try fitting the model with a couple of different hyperparameter settings, and subsequently make sure that the Train AUC is not significantly higher than the Test AUC Once you have the final Resemblance Model, a Test AUC significantly above 0.5 indicates the predictive power of the model, as well as the change in the distribution between X1 and X2 . The higher the Test AUC , the larger the difference between two datasets. You can then further inspect the model, in order to understand the patterns that the model has learned. There are two classes in probatus that allow you to analyse which features have changed between two samples: SHAPImportanceResemblance (Recommended) - Trains a Resemblance model based on a tree classifier , then it uses SHAP library to analyse the differences in features between the two samples. The main advantage of using this method is its high speed, better understanding of the relations in the data and handling of categorical features and missing values. PermutationImportanceResemblance - Trains a Resemblance model for any provided classifier , and uses Permutation Importance to analyse, which features the model relies on. It is significantly slower, and requires preprocessing of the data before training the resemblance model. Setup \u00b6 %% capture ! pip install probatus from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import pandas as pd # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X1 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 )[ 0 ], columns = feature_names ) X2 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , shift = 0.5 , random_state = 0 )[ 0 ], columns = feature_names ) # Prepare model clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 ) SHAP Importance Resemblance Model for Tree models \u00b6 Below you can see an example of how to use the model: from probatus.sample_similarity import SHAPImportanceResemblance rm = SHAPImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = rm . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) display ( feature_importance ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value mean_shap_value f1 0.074271 0.007526 f3 0.057329 -0.001271 f4 0.027532 0.000174 f2 0.022310 0.000958 By looking into the above results, one can conclude that the two samples significantly differ, since the Test AUC of the model is very high. The table shows the mean absolute shap values and mean shap values for the model's features: Mean Absolute SHAP Values provide insights about overall SHAP feature importance . Mean SHAP Values show in which direction on average the feature influences the prediction. Negative value indicates 0 class, and positive indicates 1 class. Below, the SHAP feature importance is plotted ax = rm . plot () In order to get more insights of the change in underlying relations in the data, let's plot a dot summary plot. ax = rm . plot ( plot_type = 'dot' ) We can see that the second sample have higher values in all the features. Permutation Importance Resemblance Model \u00b6 Below we show the example on how to use the PermutationImportanceResemblance from probatus.sample_similarity import PermutationImportanceResemblance perm = PermutationImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = perm . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) display ( feature_importance ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_importance std_importance f3 0.123734 0.011815 f1 0.100127 0.015754 f2 0.038549 0.011208 f4 0.018376 0.007511 Same as before, we can get more insights into the importance of the features. However, now we can also analyse the standard deviation of the permutation importance. A high standard deviation might indicate that the permutation of this feature has a higher or lower impact only in part of the available samples, while a low standard deviation indicates a consistent effect. ax = perm . plot () Visualize the difference in the most important feature \u00b6 We can also use the utils to provide more insights into the feature distribution difference in the two samples. from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ X1 [ 'f3' ], X2 [ 'f3' ]] plot_distributions_of_feature ( feature_distributions , plot_perc_outliers_removed = 0.01 )","title":"Multivariate Sample Similarity"},{"location":"tutorials/nb_sample_similarity.html#sample-similarity","text":"The goal of a Resemblance Model is understanding how different two samples are from a multivariate perspective. For instance, if you suspect that your Out-Of-Time Test set may have a different distribution than the In-Time Train set, you can detect that using the Resemblance Model. Having two samples X1 and X2 with the same set of features, one can analyse how well a model can recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to X1 dataset, and label 1 to X2 . Then, the data is shuffled and split into a Train split ( X_train , y_train ) and a Test split ( X_test , y_test ). The user provides a binary classifier that is then fitted on the Train split and evaluated on both Train and Test. Interpreting such a model allows us to understand which features and interactions between them differ between these two samples. It is crucial that the model does not overfit or underfit, because the interpretation of such a model will lead to the wrong conclusions. Therefore, you should try fitting the model with a couple of different hyperparameter settings, and subsequently make sure that the Train AUC is not significantly higher than the Test AUC Once you have the final Resemblance Model, a Test AUC significantly above 0.5 indicates the predictive power of the model, as well as the change in the distribution between X1 and X2 . The higher the Test AUC , the larger the difference between two datasets. You can then further inspect the model, in order to understand the patterns that the model has learned. There are two classes in probatus that allow you to analyse which features have changed between two samples: SHAPImportanceResemblance (Recommended) - Trains a Resemblance model based on a tree classifier , then it uses SHAP library to analyse the differences in features between the two samples. The main advantage of using this method is its high speed, better understanding of the relations in the data and handling of categorical features and missing values. PermutationImportanceResemblance - Trains a Resemblance model for any provided classifier , and uses Permutation Importance to analyse, which features the model relies on. It is significantly slower, and requires preprocessing of the data before training the resemblance model.","title":"Sample Similarity"},{"location":"tutorials/nb_sample_similarity.html#setup","text":"%% capture ! pip install probatus from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import pandas as pd # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X1 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 )[ 0 ], columns = feature_names ) X2 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , shift = 0.5 , random_state = 0 )[ 0 ], columns = feature_names ) # Prepare model clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 )","title":"Setup"},{"location":"tutorials/nb_sample_similarity.html#shap-importance-resemblance-model-for-tree-models","text":"Below you can see an example of how to use the model: from probatus.sample_similarity import SHAPImportanceResemblance rm = SHAPImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = rm . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) display ( feature_importance ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value mean_shap_value f1 0.074271 0.007526 f3 0.057329 -0.001271 f4 0.027532 0.000174 f2 0.022310 0.000958 By looking into the above results, one can conclude that the two samples significantly differ, since the Test AUC of the model is very high. The table shows the mean absolute shap values and mean shap values for the model's features: Mean Absolute SHAP Values provide insights about overall SHAP feature importance . Mean SHAP Values show in which direction on average the feature influences the prediction. Negative value indicates 0 class, and positive indicates 1 class. Below, the SHAP feature importance is plotted ax = rm . plot () In order to get more insights of the change in underlying relations in the data, let's plot a dot summary plot. ax = rm . plot ( plot_type = 'dot' ) We can see that the second sample have higher values in all the features.","title":"SHAP Importance Resemblance Model for Tree models"},{"location":"tutorials/nb_sample_similarity.html#permutation-importance-resemblance-model","text":"Below we show the example on how to use the PermutationImportanceResemblance from probatus.sample_similarity import PermutationImportanceResemblance perm = PermutationImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = perm . fit_compute ( X1 , X2 , column_names = feature_names , return_scores = True ) display ( feature_importance ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_importance std_importance f3 0.123734 0.011815 f1 0.100127 0.015754 f2 0.038549 0.011208 f4 0.018376 0.007511 Same as before, we can get more insights into the importance of the features. However, now we can also analyse the standard deviation of the permutation importance. A high standard deviation might indicate that the permutation of this feature has a higher or lower impact only in part of the available samples, while a low standard deviation indicates a consistent effect. ax = perm . plot ()","title":"Permutation Importance Resemblance Model"},{"location":"tutorials/nb_sample_similarity.html#visualize-the-difference-in-the-most-important-feature","text":"We can also use the utils to provide more insights into the feature distribution difference in the two samples. from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ X1 [ 'f3' ], X2 [ 'f3' ]] plot_distributions_of_feature ( feature_distributions , plot_perc_outliers_removed = 0.01 )","title":"Visualize the difference in the most important feature"},{"location":"tutorials/nb_shap_dependence.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Shap dependence \u00b6 This notebook illustrates the use of the interpret.shap_dependence module. The shap_dependence module can be used to explain the module using shap values. Imports \u00b6 First let's import some dependencies and set some settings: %% capture ! pip install probatus import warnings import pandas as pd from sklearn.ensemble import RandomForestClassifier from probatus.interpret.shap_dependence import DependencePlotter warnings . filterwarnings ( 'ignore' ) Data preparation \u00b6 Now let's load a sample dataset # Download and read the dataset iris = pd . read_csv ( 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv' ) # Change the problem in a binary classification problem: 'setosa' or not. iris [ 'species' ] = iris [ 'species' ] . apply ( lambda x : 1 if x == 'setosa' else 0 ) iris = iris . rename ( columns = { 'species' : 'setosa' }) # Extract the relevant features features = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ] X = iris [ features ] y = iris [ 'setosa' ] # Show the dataset iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width setosa 0 5.1 3.5 1.4 0.2 1 1 4.9 3.0 1.4 0.2 1 2 4.7 3.2 1.3 0.2 1 3 4.6 3.1 1.5 0.2 1 4 5.0 3.6 1.4 0.2 1 and let's train a random forest classifier: clf = RandomForestClassifier () clf = clf . fit ( X , y ) Shap dependence \u00b6 First, we fit the DependencePlotter tdp = DependencePlotter ( clf ) . fit ( X , y ) Now, we can plot the shap dependence for a specific feature. The top plot shows the shap values for target_names = [ 'Not setosa' , 'Setosa' ] fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 10 , 7 )) 2021-02-10T10:24:48.530300 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Plotting with outliers \u00b6 Say we alter the dataset artificially to include an outlier. The plotting will be disturbed, but we can still plot by setting the plotter's parameters. # Add an outlier X . at [ 0 , 'sepal_length' ] = 25 # Retrain the classifier and fit the plotter clf = RandomForestClassifier () . fit ( X , y ) tdp = DependencePlotter ( clf ) . fit ( X , y ) # Plot the dependence plot. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 )) 2021-02-10T10:24:49.695246 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} As we can see, the plot is not readable, and the histogram makes little sense. Both issues can be solved: Removing the outliers \u00b6 The outliers can be removed by specifying the min_q and max_q parameters. The min_q parameter is the minimum quantile after which data points are considered, and max_q is the maximum quantile before which they are considered. In this case (with one large outlier), we can set the max_q parameter to 0.99 to only remove the largest point. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), max_q = 0.99 ) 2021-02-10T10:24:50.741256 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Working with skewed distributions \u00b6 The binning functionality of probatus can be used to plot a sensible histogram under different distributions of feature data. For example, using the 'quantile' setting (without removing the outlier, produces the following histogram. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), type_binning = 'agglomerative' ) 2021-02-10T10:24:51.899052 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Nb shap dependence"},{"location":"tutorials/nb_shap_dependence.html#shap-dependence","text":"This notebook illustrates the use of the interpret.shap_dependence module. The shap_dependence module can be used to explain the module using shap values.","title":"Shap dependence"},{"location":"tutorials/nb_shap_dependence.html#imports","text":"First let's import some dependencies and set some settings: %% capture ! pip install probatus import warnings import pandas as pd from sklearn.ensemble import RandomForestClassifier from probatus.interpret.shap_dependence import DependencePlotter warnings . filterwarnings ( 'ignore' )","title":"Imports"},{"location":"tutorials/nb_shap_dependence.html#data-preparation","text":"Now let's load a sample dataset # Download and read the dataset iris = pd . read_csv ( 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv' ) # Change the problem in a binary classification problem: 'setosa' or not. iris [ 'species' ] = iris [ 'species' ] . apply ( lambda x : 1 if x == 'setosa' else 0 ) iris = iris . rename ( columns = { 'species' : 'setosa' }) # Extract the relevant features features = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ] X = iris [ features ] y = iris [ 'setosa' ] # Show the dataset iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width setosa 0 5.1 3.5 1.4 0.2 1 1 4.9 3.0 1.4 0.2 1 2 4.7 3.2 1.3 0.2 1 3 4.6 3.1 1.5 0.2 1 4 5.0 3.6 1.4 0.2 1 and let's train a random forest classifier: clf = RandomForestClassifier () clf = clf . fit ( X , y )","title":"Data preparation"},{"location":"tutorials/nb_shap_dependence.html#shap-dependence_1","text":"First, we fit the DependencePlotter tdp = DependencePlotter ( clf ) . fit ( X , y ) Now, we can plot the shap dependence for a specific feature. The top plot shows the shap values for target_names = [ 'Not setosa' , 'Setosa' ] fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 10 , 7 )) 2021-02-10T10:24:48.530300 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Shap dependence"},{"location":"tutorials/nb_shap_dependence.html#plotting-with-outliers","text":"Say we alter the dataset artificially to include an outlier. The plotting will be disturbed, but we can still plot by setting the plotter's parameters. # Add an outlier X . at [ 0 , 'sepal_length' ] = 25 # Retrain the classifier and fit the plotter clf = RandomForestClassifier () . fit ( X , y ) tdp = DependencePlotter ( clf ) . fit ( X , y ) # Plot the dependence plot. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 )) 2021-02-10T10:24:49.695246 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} As we can see, the plot is not readable, and the histogram makes little sense. Both issues can be solved:","title":"Plotting with outliers"},{"location":"tutorials/nb_shap_dependence.html#removing-the-outliers","text":"The outliers can be removed by specifying the min_q and max_q parameters. The min_q parameter is the minimum quantile after which data points are considered, and max_q is the maximum quantile before which they are considered. In this case (with one large outlier), we can set the max_q parameter to 0.99 to only remove the largest point. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), max_q = 0.99 ) 2021-02-10T10:24:50.741256 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Removing the outliers"},{"location":"tutorials/nb_shap_dependence.html#working-with-skewed-distributions","text":"The binning functionality of probatus can be used to plot a sensible histogram under different distributions of feature data. For example, using the 'quantile' setting (without removing the outlier, produces the following histogram. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), type_binning = 'agglomerative' ) 2021-02-10T10:24:51.899052 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Working with skewed distributions"},{"location":"tutorials/nb_shap_feature_elimination.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); ShapRFECV - Recursive Feature Elimination using SHAP importance \u00b6 Recursive Feature Elimination allows you to efficiently reduce the number of features in your dataset, without losing the predictive power of the model. probatus implements the following feature elimination routine for tree-based & linear models : While any features left , iterate : 1 . ( Optional ) Tune hyperparameters , in case sklearn compatible search CV e . g . `GridSearchCV` or `RandomizedSearchCV` or `BayesSearchCV` are passed as clf , 2 . Calculate SHAP feature importance using Cross - Validation , 3 . Remove `step` lowest importance features . The functionality is similar to RFECV , yet it removes the lowest importance features, based on SHAP features importance. It also supports the use of any hyperparameter search schema that is consistent with sklearn API e.g. GridSearchCV , RandomizedSearchCV and BayesSearchCV passed as a clf , thanks to which you can perform hyperparameter optimization at each step of the search. hyperparameters of the model at each round, to tune the model for each features set. Lastly, it supports categorical features ( object and category dtype) and missing values in the data, as long as the model supports them. The main advantages of using this routine are: It uses a tree-based or a linear model to detect the complex relations between features and the target. It uses SHAP importance, which is one of the most reliable ways to estimate features importance. Unlike many other techniques, it works with missing values and categorical variables. Supports the use of sklearn compatible hyperparameter search schemas e.g. GridSearchCV , RandomizedSearchCV and BayesSearchCV , in order to optimize hyperparameters at each iteration. This way you can assess if the removal of a given feature reduces the predictive power, or simply requires additional tuning of the model. You can also provide a list of features that should not be eliminated e.g. incase of prior knowledge. The disadvantages are: Removing lowest SHAP importance feature does not always translate to choosing the feature with the lowest impact on a model's performance. Shap importance illustrates how strongly a given feature affects the output of the model, while disregarding correctness of this prediction. Currently, the functionality only supports tree-based & linear binary classifiers, in the future the scope might be extended. For large datasets, performing hyperparameter optimization can be very computationally expensive. For gradient boosted tree models, one alternative is to use early stopping of the training step. For this, see EarlyStoppingShapRFECV Setup the dataset \u00b6 In order to use the functionality, let's set up an example dataset with: 18 numerical features 1 static feature 1 static feature 1 feature with missing values %% capture ! pip install probatus ! pip install lightgbm from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np import pandas as pd import lightgbm from sklearn.model_selection import RandomizedSearchCV feature_names = [ 'f1' , 'f2_missing' , 'f3_static' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) X [ 'f2_missing' ] = X [ 'f2_missing' ] . apply ( lambda x : x if np . random . rand () < 0.8 else np . nan ) X [ 'f3_static' ] = 0 #First 5 rows of first 5 columns X [ feature_names [: 5 ]] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f1 f2_missing f3_static f4 f5 0 3.399287 -3.902230 0 0.037207 -0.211075 1 -2.480698 0.772855 0 0.302824 0.729950 2 -0.690014 1.350847 0 1.837895 -0.745689 3 -5.291164 4.559465 0 -1.277930 3.688404 4 -1.028435 1.505766 0 -0.576209 -0.790525 Set up the model and model tuning \u00b6 You need to set up the model that you would like to use in the feature elimination. probatus requires a tree-based or linear binary classifier in order to speed up the computation of SHAP feature importance at each step. We recommend using LGBMClassifier , which by default handles missing values and categorical features. The example below applies randomized search in order to optimize the hyperparameters of the model at each iteration of the search. clf = lightgbm . LGBMClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'num_leaves' : [ 3 , 5 , 7 , 10 ], } search = RandomizedSearchCV ( clf , param_grid ) Apply ShapRFECV \u00b6 Now let's apply the ShapRFECV . shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) At the end of the process, you can investigate the results for each iteration. #First 5 rows of first 5 columns report [[ 'num_features' , 'features_set' , 'val_metric_mean' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_features features_set val_metric_mean 1 20 [f1, f2_missing, f3_static, f4, f5, f6, f7, f8... 0.922 2 16 [f17, f19, f20, f8, f1, f9, f4, f18, f16, f14,... 0.922 3 13 [f20, f19, f8, f1, f9, f18, f16, f14, f11, f5,... 0.923 4 11 [f18, f20, f19, f16, f8, f14, f11, f5, f9, f10... 0.923 5 9 [f20, f19, f16, f8, f14, f11, f5, f9, f15] 0.904 6 8 [f20, f19, f16, f8, f14, f5, f9, f15] 0.910 7 7 [f20, f19, f16, f8, f14, f9, f15] 0.905 8 6 [f20, f19, f16, f14, f9, f15] 0.917 9 5 [f19, f16, f14, f9, f15] 0.889 10 4 [f19, f9, f16, f14] 0.877 11 3 [f19, f16, f9] 0.867 12 2 [f19, f16] 0.818 13 1 [f16] 0.720 Once the process is completed, you can visualize the results. \u00b6 Let's investigate the performance plot. In this case, the Validation AUC score has the highest Validation AUC at 11 features and a peak at 6 features. performance_plot = shap_elimination . plot () Let's see the final feature set: shap_elimination . get_reduced_features_set ( num_features = 6 ) ['f20', 'f19', 'f16', 'f14', 'f9', 'f15'] You can also provide a list of features that should not be eliminated. Say based on your prior knowledge you know that the features f10,f19,f15 are important and should not be eliminated. This can be done by providing a list of columns to columns_to_keep parameter in the fit() function. shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , n_jobs = 3 , min_features_to_select = 4 ) report = shap_elimination . fit_compute ( X , y , columns_to_keep = [ 'f10' , 'f15' , 'f19' ]) performance_plot = shap_elimination . plot () 2021-04-14T13:15:30.176929 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Let's see the final feature set: shap_elimination . get_reduced_features_set ( num_features = 4 ) ['f15', 'f16', 'f10', 'f19'] EarlyStoppingShapRFECV \u00b6 Early stopping is a type of regularization, common in gradient boosted trees , such as LightGBM and XGBoost . It consists of measuring how well the model performs after each base learner is added to the ensemble tree, using a relevant scoring metric. If this metric does not improve after a certain number of training steps, the training can be stopped before the maximum number of base learners is reached. Early stopping is thus a way of mitigating overfitting in a relatively cheaply, without having to find the ideal regularization hyperparameters. It is particularly useful for handling large datasets, since it reduces the number of training steps which can decrease the modelling time. EarlyStoppingShapRFECV is a child of ShapRFECV with limited support for early stopping and the example below shows how to use it with LightGBM. from probatus.feature_elimination import EarlyStoppingShapRFECV clf = lightgbm . LGBMClassifier ( n_estimators = 200 , max_depth = 3 ) # Run feature elimination shap_elimination = EarlyStoppingShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , eval_metric = 'auc' , early_stopping_rounds = 5 , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) # Make plots performance_plot = shap_elimination . plot () # Get final feature set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 9 ) 2021-04-14T13:15:41.651975 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} As it is hinted in the example above, with large datasets and simple base learners, early stopping can be a much faster alternative to hyperparameter optimization of the ideal number of trees. Note that although EarlyStoppingShapRFECV supports hyperparameter search models as input, early stopping is used only during the Shapley value estimation step, and not during hyperparameter search. For this reason, if you are not using early stopping, you should use the parent class, ShapRFECV , instead of EarlyStoppingShapRFECV .","title":"ShapRFECV - Recursive Feature Elimination using SHAP importance"},{"location":"tutorials/nb_shap_feature_elimination.html#shaprfecv-recursive-feature-elimination-using-shap-importance","text":"Recursive Feature Elimination allows you to efficiently reduce the number of features in your dataset, without losing the predictive power of the model. probatus implements the following feature elimination routine for tree-based & linear models : While any features left , iterate : 1 . ( Optional ) Tune hyperparameters , in case sklearn compatible search CV e . g . `GridSearchCV` or `RandomizedSearchCV` or `BayesSearchCV` are passed as clf , 2 . Calculate SHAP feature importance using Cross - Validation , 3 . Remove `step` lowest importance features . The functionality is similar to RFECV , yet it removes the lowest importance features, based on SHAP features importance. It also supports the use of any hyperparameter search schema that is consistent with sklearn API e.g. GridSearchCV , RandomizedSearchCV and BayesSearchCV passed as a clf , thanks to which you can perform hyperparameter optimization at each step of the search. hyperparameters of the model at each round, to tune the model for each features set. Lastly, it supports categorical features ( object and category dtype) and missing values in the data, as long as the model supports them. The main advantages of using this routine are: It uses a tree-based or a linear model to detect the complex relations between features and the target. It uses SHAP importance, which is one of the most reliable ways to estimate features importance. Unlike many other techniques, it works with missing values and categorical variables. Supports the use of sklearn compatible hyperparameter search schemas e.g. GridSearchCV , RandomizedSearchCV and BayesSearchCV , in order to optimize hyperparameters at each iteration. This way you can assess if the removal of a given feature reduces the predictive power, or simply requires additional tuning of the model. You can also provide a list of features that should not be eliminated e.g. incase of prior knowledge. The disadvantages are: Removing lowest SHAP importance feature does not always translate to choosing the feature with the lowest impact on a model's performance. Shap importance illustrates how strongly a given feature affects the output of the model, while disregarding correctness of this prediction. Currently, the functionality only supports tree-based & linear binary classifiers, in the future the scope might be extended. For large datasets, performing hyperparameter optimization can be very computationally expensive. For gradient boosted tree models, one alternative is to use early stopping of the training step. For this, see EarlyStoppingShapRFECV","title":"ShapRFECV - Recursive Feature Elimination using SHAP importance"},{"location":"tutorials/nb_shap_feature_elimination.html#setup-the-dataset","text":"In order to use the functionality, let's set up an example dataset with: 18 numerical features 1 static feature 1 static feature 1 feature with missing values %% capture ! pip install probatus ! pip install lightgbm from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np import pandas as pd import lightgbm from sklearn.model_selection import RandomizedSearchCV feature_names = [ 'f1' , 'f2_missing' , 'f3_static' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) X [ 'f2_missing' ] = X [ 'f2_missing' ] . apply ( lambda x : x if np . random . rand () < 0.8 else np . nan ) X [ 'f3_static' ] = 0 #First 5 rows of first 5 columns X [ feature_names [: 5 ]] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f1 f2_missing f3_static f4 f5 0 3.399287 -3.902230 0 0.037207 -0.211075 1 -2.480698 0.772855 0 0.302824 0.729950 2 -0.690014 1.350847 0 1.837895 -0.745689 3 -5.291164 4.559465 0 -1.277930 3.688404 4 -1.028435 1.505766 0 -0.576209 -0.790525","title":"Setup the dataset"},{"location":"tutorials/nb_shap_feature_elimination.html#set-up-the-model-and-model-tuning","text":"You need to set up the model that you would like to use in the feature elimination. probatus requires a tree-based or linear binary classifier in order to speed up the computation of SHAP feature importance at each step. We recommend using LGBMClassifier , which by default handles missing values and categorical features. The example below applies randomized search in order to optimize the hyperparameters of the model at each iteration of the search. clf = lightgbm . LGBMClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'num_leaves' : [ 3 , 5 , 7 , 10 ], } search = RandomizedSearchCV ( clf , param_grid )","title":"Set up the model and model tuning"},{"location":"tutorials/nb_shap_feature_elimination.html#apply-shaprfecv","text":"Now let's apply the ShapRFECV . shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) At the end of the process, you can investigate the results for each iteration. #First 5 rows of first 5 columns report [[ 'num_features' , 'features_set' , 'val_metric_mean' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_features features_set val_metric_mean 1 20 [f1, f2_missing, f3_static, f4, f5, f6, f7, f8... 0.922 2 16 [f17, f19, f20, f8, f1, f9, f4, f18, f16, f14,... 0.922 3 13 [f20, f19, f8, f1, f9, f18, f16, f14, f11, f5,... 0.923 4 11 [f18, f20, f19, f16, f8, f14, f11, f5, f9, f10... 0.923 5 9 [f20, f19, f16, f8, f14, f11, f5, f9, f15] 0.904 6 8 [f20, f19, f16, f8, f14, f5, f9, f15] 0.910 7 7 [f20, f19, f16, f8, f14, f9, f15] 0.905 8 6 [f20, f19, f16, f14, f9, f15] 0.917 9 5 [f19, f16, f14, f9, f15] 0.889 10 4 [f19, f9, f16, f14] 0.877 11 3 [f19, f16, f9] 0.867 12 2 [f19, f16] 0.818 13 1 [f16] 0.720","title":"Apply ShapRFECV"},{"location":"tutorials/nb_shap_feature_elimination.html#once-the-process-is-completed-you-can-visualize-the-results","text":"Let's investigate the performance plot. In this case, the Validation AUC score has the highest Validation AUC at 11 features and a peak at 6 features. performance_plot = shap_elimination . plot () Let's see the final feature set: shap_elimination . get_reduced_features_set ( num_features = 6 ) ['f20', 'f19', 'f16', 'f14', 'f9', 'f15'] You can also provide a list of features that should not be eliminated. Say based on your prior knowledge you know that the features f10,f19,f15 are important and should not be eliminated. This can be done by providing a list of columns to columns_to_keep parameter in the fit() function. shap_elimination = ShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , n_jobs = 3 , min_features_to_select = 4 ) report = shap_elimination . fit_compute ( X , y , columns_to_keep = [ 'f10' , 'f15' , 'f19' ]) performance_plot = shap_elimination . plot () 2021-04-14T13:15:30.176929 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Let's see the final feature set: shap_elimination . get_reduced_features_set ( num_features = 4 ) ['f15', 'f16', 'f10', 'f19']","title":"Once the process is completed, you can visualize the results."},{"location":"tutorials/nb_shap_feature_elimination.html#earlystoppingshaprfecv","text":"Early stopping is a type of regularization, common in gradient boosted trees , such as LightGBM and XGBoost . It consists of measuring how well the model performs after each base learner is added to the ensemble tree, using a relevant scoring metric. If this metric does not improve after a certain number of training steps, the training can be stopped before the maximum number of base learners is reached. Early stopping is thus a way of mitigating overfitting in a relatively cheaply, without having to find the ideal regularization hyperparameters. It is particularly useful for handling large datasets, since it reduces the number of training steps which can decrease the modelling time. EarlyStoppingShapRFECV is a child of ShapRFECV with limited support for early stopping and the example below shows how to use it with LightGBM. from probatus.feature_elimination import EarlyStoppingShapRFECV clf = lightgbm . LGBMClassifier ( n_estimators = 200 , max_depth = 3 ) # Run feature elimination shap_elimination = EarlyStoppingShapRFECV ( clf = search , step = 0.2 , cv = 10 , scoring = 'roc_auc' , eval_metric = 'auc' , early_stopping_rounds = 5 , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) # Make plots performance_plot = shap_elimination . plot () # Get final feature set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 9 ) 2021-04-14T13:15:41.651975 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} As it is hinted in the example above, with large datasets and simple base learners, early stopping can be a much faster alternative to hyperparameter optimization of the ideal number of trees. Note that although EarlyStoppingShapRFECV supports hyperparameter search models as input, early stopping is used only during the Shapley value estimation step, and not during hyperparameter search. For this reason, if you are not using early stopping, you should use the parent class, ShapRFECV , instead of EarlyStoppingShapRFECV .","title":"EarlyStoppingShapRFECV"},{"location":"tutorials/nb_shap_model_interpreter.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Tree Model Interpretation using SHAP \u00b6 There are many techniques, each with advantages and disadvantages that can be suitable for different situations. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model, and is well-suited for exploring feature importances. Pros: Mathematical theory behind explanation of the model. Very wide application and ease of use. Explanations on single sample and global level, and a number of graphs that can be very easily computed and understood. Feature interactions taken into account by the method. High computation speed, especially for the tree based models. Cons: Documentation is often lacking. Different API when you use sklearn models e.g. RandomForestClassifier. Slow computation for some explainers e.g. KernelExplainer. Let's assume we want to analyse the following model: %% capture ! pip install probatus from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split import numpy as np import pandas as pd from probatus.interpret import ShapModelInterpreter import warnings warnings . filterwarnings ( 'ignore' ) feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf = clf . fit ( X_train , y_train ) ShapModelInterpreter \u00b6 The ShapModelInterpreter class in Probatus is a convenience wrapper class that allows us to easily interpret the ML models. Currently it supports only tree-based & linear models . Feature importance \u00b6 Firstly, lets compute the report presenting various properties of the model: mean_abs_shap_value_test - SHAP feature importance computed on the test set. It is an unbiased measurement of feature importance of the model on unseen data. mean_abs_shap_value_train - SHAP feature importance computed on the test set. It is a biased measurement, because the model has used this data to train. However, the a significant difference between this metric, and the mean_abs_shap_value_test might indicate a shift in the data distribution, target distribution, or overfitting of the model. mean_shap_value_test - This metric presents how strongly values of a given feature in the test set push the prediction towards one class or the other. A positive value indicates that this feature increases the probability of the positive class, and negative indicates that it decreases it. In the balanced setting it is typically around 0, while for imbalanced the value it has is relative to the majority class. It is crucial to compare it with mean_shap_value_train - if it differs significantly, there is possibly a shift in data or target distribution in the test set. mean_shap_value_train - This metric presents how strongly the values of a given feature in the train set push the prediction towards one class or the other, similarly to mean_shap_value_test . shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test , approximate = False ) feature_importance .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value_test mean_abs_shap_value_train mean_shap_value_test mean_shap_value_train f1 0.344057 0.329813 0.344057 0.329813 f4 0.118455 0.126920 0.118455 0.126920 f3 0.035574 0.024811 -0.029377 -0.016373 f2 0.007389 0.009072 0.007389 0.009072 Run the following command to plot the SHAP feature importance. ax = shap_interpreter . plot ( 'importance' ) 2021-02-11T16:04:50.793625 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} The AUC on train and test sets is illustrated in each plot, to indicate if the model overfits. If you see that Test AUC is significantly lower than Train AUC, this is a sign that the model might be overfitting. In such cases, the interpretation of the model might be misleading. In these situations we recommend retraining the model with more regularization. Summary plot \u00b6 Summary plot gives you more insights into how different feature values affect the predictions made. This is a very crucial plot to make for every model. Each dot on the X-axis represents a sample in the data, and how strongly it affected the prediction (together with predictions direction). The colours of the dots present the values of that feature. For each model try to analyse this plot with Subject Matter Expert, in order to make sure that the relations that the model has learned make sense. ax = shap_interpreter . plot ( 'summary' ) 2021-02-11T16:04:53.255478 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Dependence Plot \u00b6 This plot allows you to understand how the model reacts for different feature values. You can plot it for each feature in your model, or at least the top 10 features. This can provide you with further insights on how the model uses each of the features. Moreover, one can detect anomalies, as well as the effect of the outliers on the model. As an addition, the bottom plot presents the feature distribution histogram, and the target rate for different buckets within that feature values. This allows you to further analyse how the feature correlates with the target variable. ax = shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' ]) 2021-02-11T16:04:54.775424 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Sample explanation \u00b6 In order to explain predictions for specific samples from your test set, you can use a sample plot. For a given sample, the plot presents the force and direction of the prediction shift that each feature value causes. ax = shap_interpreter . plot ( 'sample' , samples_index = [ 521 , 78 ]) 2021-02-11T16:04:56.377013 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} 2021-02-11T16:04:56.909876 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Detecting Data or Target Distribution Shift \u00b6 Let's assume that there is a shift between the train and test data: X_test [ 'f1' ] = X_test [ 'f1' ] - 5 X_test [ 'f4' ] = X_test [ 'f4' ] + 5 Now, we can look into how it affects the resutls: shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test , approximate = False ) feature_importance .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value_test mean_abs_shap_value_train mean_shap_value_test mean_shap_value_train f3 0.041311 0.024811 -0.039222 -0.016373 f2 0.007557 0.009072 0.006605 0.009072 f1 0.000000 0.329813 0.000000 0.329813 f4 0.000000 0.126920 0.000000 0.126920 In case of feature f1 and f4 the shift is indicated by the differences between train and test in mean absolute shap values, and mean shap values. We can visualize the summary plot for these two sets: ax_test = shap_interpreter . plot ( 'summary' , target_set = 'test' ) ax_train = shap_interpreter . plot ( 'summary' , target_set = 'train' ) 2021-02-11T16:04:59.045097 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} 2021-02-11T16:04:59.520565 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Tips for using the interpreter \u00b6 Before using the ShapModelInterpreter consider the following tips: Make sure you do not underfit or overfit the model. Underfitting will cause only the most important relations in the data to be visible, while overfitting will present relationships that do not generalize. Perform a feature selection process before fitting the final model. This way, it will be easier to interpret the explanation. Moreover, highly-correlated features will affect the explanation less. Preferably use a model that handles NaNs e.g. LightGBM or impute them beforehand using SHAP. When imputing also extract a MissingIndicator to get insights into when NaNs are meaningful for the model. For categorical features either use a model that handles them e.g. LightGBM, or apply One-hot encoding. Keep in mind that with One-hot encoding the importance of a categorical feature might be spread over multiple encoded features.","title":"Tree-based & Linear Model Interpretation with SHAP"},{"location":"tutorials/nb_shap_model_interpreter.html#tree-model-interpretation-using-shap","text":"There are many techniques, each with advantages and disadvantages that can be suitable for different situations. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model, and is well-suited for exploring feature importances. Pros: Mathematical theory behind explanation of the model. Very wide application and ease of use. Explanations on single sample and global level, and a number of graphs that can be very easily computed and understood. Feature interactions taken into account by the method. High computation speed, especially for the tree based models. Cons: Documentation is often lacking. Different API when you use sklearn models e.g. RandomForestClassifier. Slow computation for some explainers e.g. KernelExplainer. Let's assume we want to analyse the following model: %% capture ! pip install probatus from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split import numpy as np import pandas as pd from probatus.interpret import ShapModelInterpreter import warnings warnings . filterwarnings ( 'ignore' ) feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf = clf . fit ( X_train , y_train )","title":"Tree Model Interpretation using SHAP"},{"location":"tutorials/nb_shap_model_interpreter.html#shapmodelinterpreter","text":"The ShapModelInterpreter class in Probatus is a convenience wrapper class that allows us to easily interpret the ML models. Currently it supports only tree-based & linear models .","title":"ShapModelInterpreter"},{"location":"tutorials/nb_shap_model_interpreter.html#feature-importance","text":"Firstly, lets compute the report presenting various properties of the model: mean_abs_shap_value_test - SHAP feature importance computed on the test set. It is an unbiased measurement of feature importance of the model on unseen data. mean_abs_shap_value_train - SHAP feature importance computed on the test set. It is a biased measurement, because the model has used this data to train. However, the a significant difference between this metric, and the mean_abs_shap_value_test might indicate a shift in the data distribution, target distribution, or overfitting of the model. mean_shap_value_test - This metric presents how strongly values of a given feature in the test set push the prediction towards one class or the other. A positive value indicates that this feature increases the probability of the positive class, and negative indicates that it decreases it. In the balanced setting it is typically around 0, while for imbalanced the value it has is relative to the majority class. It is crucial to compare it with mean_shap_value_train - if it differs significantly, there is possibly a shift in data or target distribution in the test set. mean_shap_value_train - This metric presents how strongly the values of a given feature in the train set push the prediction towards one class or the other, similarly to mean_shap_value_test . shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test , approximate = False ) feature_importance .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value_test mean_abs_shap_value_train mean_shap_value_test mean_shap_value_train f1 0.344057 0.329813 0.344057 0.329813 f4 0.118455 0.126920 0.118455 0.126920 f3 0.035574 0.024811 -0.029377 -0.016373 f2 0.007389 0.009072 0.007389 0.009072 Run the following command to plot the SHAP feature importance. ax = shap_interpreter . plot ( 'importance' ) 2021-02-11T16:04:50.793625 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} The AUC on train and test sets is illustrated in each plot, to indicate if the model overfits. If you see that Test AUC is significantly lower than Train AUC, this is a sign that the model might be overfitting. In such cases, the interpretation of the model might be misleading. In these situations we recommend retraining the model with more regularization.","title":"Feature importance"},{"location":"tutorials/nb_shap_model_interpreter.html#summary-plot","text":"Summary plot gives you more insights into how different feature values affect the predictions made. This is a very crucial plot to make for every model. Each dot on the X-axis represents a sample in the data, and how strongly it affected the prediction (together with predictions direction). The colours of the dots present the values of that feature. For each model try to analyse this plot with Subject Matter Expert, in order to make sure that the relations that the model has learned make sense. ax = shap_interpreter . plot ( 'summary' ) 2021-02-11T16:04:53.255478 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Summary plot"},{"location":"tutorials/nb_shap_model_interpreter.html#dependence-plot","text":"This plot allows you to understand how the model reacts for different feature values. You can plot it for each feature in your model, or at least the top 10 features. This can provide you with further insights on how the model uses each of the features. Moreover, one can detect anomalies, as well as the effect of the outliers on the model. As an addition, the bottom plot presents the feature distribution histogram, and the target rate for different buckets within that feature values. This allows you to further analyse how the feature correlates with the target variable. ax = shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' ]) 2021-02-11T16:04:54.775424 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Dependence Plot"},{"location":"tutorials/nb_shap_model_interpreter.html#sample-explanation","text":"In order to explain predictions for specific samples from your test set, you can use a sample plot. For a given sample, the plot presents the force and direction of the prediction shift that each feature value causes. ax = shap_interpreter . plot ( 'sample' , samples_index = [ 521 , 78 ]) 2021-02-11T16:04:56.377013 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} 2021-02-11T16:04:56.909876 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Sample explanation"},{"location":"tutorials/nb_shap_model_interpreter.html#detecting-data-or-target-distribution-shift","text":"Let's assume that there is a shift between the train and test data: X_test [ 'f1' ] = X_test [ 'f1' ] - 5 X_test [ 'f4' ] = X_test [ 'f4' ] + 5 Now, we can look into how it affects the resutls: shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test , approximate = False ) feature_importance .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value_test mean_abs_shap_value_train mean_shap_value_test mean_shap_value_train f3 0.041311 0.024811 -0.039222 -0.016373 f2 0.007557 0.009072 0.006605 0.009072 f1 0.000000 0.329813 0.000000 0.329813 f4 0.000000 0.126920 0.000000 0.126920 In case of feature f1 and f4 the shift is indicated by the differences between train and test in mean absolute shap values, and mean shap values. We can visualize the summary plot for these two sets: ax_test = shap_interpreter . plot ( 'summary' , target_set = 'test' ) ax_train = shap_interpreter . plot ( 'summary' , target_set = 'train' ) 2021-02-11T16:04:59.045097 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} 2021-02-11T16:04:59.520565 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","title":"Detecting Data or Target Distribution Shift"},{"location":"tutorials/nb_shap_model_interpreter.html#tips-for-using-the-interpreter","text":"Before using the ShapModelInterpreter consider the following tips: Make sure you do not underfit or overfit the model. Underfitting will cause only the most important relations in the data to be visible, while overfitting will present relationships that do not generalize. Perform a feature selection process before fitting the final model. This way, it will be easier to interpret the explanation. Moreover, highly-correlated features will affect the explanation less. Preferably use a model that handles NaNs e.g. LightGBM or impute them beforehand using SHAP. When imputing also extract a MissingIndicator to get insights into when NaNs are meaningful for the model. For categorical features either use a model that handles them e.g. LightGBM, or apply One-hot encoding. Keep in mind that with One-hot encoding the importance of a categorical feature might be spread over multiple encoded features.","title":"Tips for using the interpreter"}]}