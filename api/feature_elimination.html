
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Validation of binary classifiers and data used to develop them">
      
      
      
        <meta name="author" content="ING Bank N. V.">
      
      
        <link rel="canonical" href="https://ing-bank.github.io/probatus/api/feature_elimination.html">
      
      <link rel="icon" href="../img/Probatus_P_white.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.1.1">
    
    
      
        <title>probatus.feature_elimination - Probatus Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.9299cb39.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ef6f36e2.min.css">
        
          
          
          <meta name="theme-color" content="#ff6e42">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Ubuntu";--md-code-font-family:"Ubuntu Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../css/pandas-dataframe.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-orange" data-md-color-accent="indigo">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#features-elimination" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Probatus Docs" class="md-header__button md-logo" aria-label="Probatus Docs" data-md-component="logo">
      
  <img src="../img/Probatus_P_white.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Probatus Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              probatus.feature_elimination
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/ing-bank/probatus/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../index.html" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../tutorials/nb_shap_feature_elimination.html" class="md-tabs__link">
        Tutorials
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../howto/reproducibility.html" class="md-tabs__link">
        HowTo
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="feature_elimination.html" class="md-tabs__link md-tabs__link--active">
        API
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../discussion/vision.html" class="md-tabs__link">
        Discussion
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Probatus Docs" class="md-nav__button md-logo" aria-label="Probatus Docs" data-md-component="logo">
      
  <img src="../img/Probatus_P_white.png" alt="logo">

    </a>
    Probatus Docs
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ing-bank/probatus/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        Tutorials
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Tutorials" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_shap_feature_elimination.html" class="md-nav__link">
        ShapRFECV - Recursive Feature Elimination using SHAP importance
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_shap_model_interpreter.html" class="md-nav__link">
        Tree-based & Linear Model Interpretation with SHAP
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_imputation_comparison.html" class="md-nav__link">
        Imputation Strategy Comparison
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_metric_volatility.html" class="md-nav__link">
        Model Metrics Volatility
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_sample_similarity.html" class="md-nav__link">
        Multivariate Sample Similarity
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_distribution_statistics.html" class="md-nav__link">
        Univariate Sample Similarity
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_custom_scoring.html" class="md-nav__link">
        Custom Scoring Metrics
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        HowTo
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="HowTo" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          HowTo
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../howto/reproducibility.html" class="md-nav__link">
        Reproducibility of the results
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      <label class="md-nav__link" for="__nav_4">
        API
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          probatus.feature_elimination
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="feature_elimination.html" class="md-nav__link md-nav__link--active">
        probatus.feature_elimination
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV" class="md-nav__link">
    EarlyStoppingShapRFECV
  </a>
  
    <nav class="md-nav" aria-label="EarlyStoppingShapRFECV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.compute" class="md-nav__link">
    compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit_compute" class="md-nav__link">
    fit_compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.get_reduced_features_set" class="md-nav__link">
    get_reduced_features_set()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.plot" class="md-nav__link">
    plot()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV" class="md-nav__link">
    ShapRFECV
  </a>
  
    <nav class="md-nav" aria-label="ShapRFECV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.compute" class="md-nav__link">
    compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute" class="md-nav__link">
    fit_compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set" class="md-nav__link">
    get_reduced_features_set()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.plot" class="md-nav__link">
    plot()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="model_interpret.html" class="md-nav__link">
        probatus.interpret
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="metric_volatility.html" class="md-nav__link">
        probatus.metric_volatility
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="imputation_selector.html" class="md-nav__link">
        probatus.missing_values
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="sample_similarity.html" class="md-nav__link">
        probatus.sample_similarity
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="stat_tests.html" class="md-nav__link">
        probatus.stat_tests
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="utils.html" class="md-nav__link">
        probatus.utils
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      <label class="md-nav__link" for="__nav_5">
        Discussion
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Discussion" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Discussion
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../discussion/vision.html" class="md-nav__link">
        Vision
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../discussion/nb_rfecv_vs_shaprfecv.html" class="md-nav__link">
        ShapRFECV vs sklearn RFECV
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../discussion/contributing.html" class="md-nav__link">
        Contributing
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV" class="md-nav__link">
    EarlyStoppingShapRFECV
  </a>
  
    <nav class="md-nav" aria-label="EarlyStoppingShapRFECV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.compute" class="md-nav__link">
    compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit_compute" class="md-nav__link">
    fit_compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.get_reduced_features_set" class="md-nav__link">
    get_reduced_features_set()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.plot" class="md-nav__link">
    plot()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV" class="md-nav__link">
    ShapRFECV
  </a>
  
    <nav class="md-nav" aria-label="ShapRFECV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.compute" class="md-nav__link">
    compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute" class="md-nav__link">
    fit_compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set" class="md-nav__link">
    get_reduced_features_set()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.plot" class="md-nav__link">
    plot()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ing-bank/probatus/edit/master/docs/api/feature_elimination.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="features-elimination">Features Elimination<a class="headerlink" href="#features-elimination" title="Permanent link">&para;</a></h1>
<p>This module focuses on feature elimination and it contains two classes:</p>
<ul>
<li><code>ShapRFECV</code>: Perform Backwards Recursive Feature Elimination, using SHAP feature importance. It supports binary classification models and hyperparameter optimization at every feature elimination step.</li>
<li><code>EarlyStoppingShapRFECV</code>: adds support to early stopping of the model fitting process. It can be an alternative regularization technique to hyperparameter optimization of the number of base trees in gradient boosted tree models. Particularly useful when dealing with large datasets.</li>
</ul>


  <div class="doc doc-object doc-module">


    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV">
        <code>EarlyStoppingShapRFECV</code>



<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>This class performs Backwards Recursive Feature Elimination, using SHAP feature importance.</p>
<p>This is a child of ShapRFECV which allows early stopping of the training step, available in models such as
    XGBoost and LightGBM. If you are not using early stopping, you should use the parent class,
    ShapRFECV, instead of EarlyStoppingShapRFECV.</p>
<p><a href="https://en.wikipedia.org/wiki/Early_stopping">Early stopping</a> is a type of
    regularization technique in which the model is trained until the scoring metric, measured on a validation set,
    stops improving after a number of early_stopping_rounds. In boosted tree models, this technique can increase
    the training speed, by skipping the training of trees that do not improve the scoring metric any further,
    which is particularly useful when the training dataset is large.</p>
<p>Note that if the classifier is a hyperparameter search model is used, the early stopping parameter is passed only
    to the fit method of the model duiring the Shapley values estimation step, and not for the hyperparameter
    search step.
    Early stopping can be seen as a type of regularization of the optimal number of trees. Therefore you can use
    it directly with a LightGBM or XGBoost model, as an alternative to a hyperparameter search model.</p>
<p>At each round, for a
    given feature set, starting from all available features, the following steps are applied:</p>
<ol>
<li>(Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g.
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html">GridSearchCV</a>,
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV">RandomizedSearchCV</a>, or
    <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>.
    Note that during this step the model does not use early stopping.</li>
<li>Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV
    iteration, the model is fitted on the train folds, and applied on the validation fold to estimate
    SHAP feature importance. The model is trained until the scoring metric eval_metric, measured on the
    validation fold, stops improving after a number of early_stopping_rounds.</li>
<li>Remove <code>step</code> lowest SHAP importance features from the dataset.</li>
</ol>
<p>At the end of the process, the user can plot the performance of the model for each iteration, and select the
    optimal number of features and the features set.</p>
<p>We recommend using <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">LGBMClassifier</a>,
    because by default it handles missing values and categorical features. In case of other models, make sure to
    handle these issues for your dataset and consider impact it might have on features importance.</p>

<p><strong>Examples:</strong></p>
    
      <p><div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">probatus.feature_elimination</span> <span class="kn">import</span> <span class="n">EarlyStoppingShapRFECV</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="s1">&#39;f2&#39;</span><span class="p">,</span> <span class="s1">&#39;f3&#39;</span><span class="p">,</span> <span class="s1">&#39;f4&#39;</span><span class="p">,</span> <span class="s1">&#39;f5&#39;</span><span class="p">,</span> <span class="s1">&#39;f6&#39;</span><span class="p">,</span> <span class="s1">&#39;f7&#39;</span><span class="p">,</span>
    <span class="s1">&#39;f8&#39;</span><span class="p">,</span> <span class="s1">&#39;f9&#39;</span><span class="p">,</span> <span class="s1">&#39;f10&#39;</span><span class="p">,</span> <span class="s1">&#39;f11&#39;</span><span class="p">,</span> <span class="s1">&#39;f12&#39;</span><span class="p">,</span> <span class="s1">&#39;f13&#39;</span><span class="p">,</span>
    <span class="s1">&#39;f14&#39;</span><span class="p">,</span> <span class="s1">&#39;f15&#39;</span><span class="p">,</span> <span class="s1">&#39;f16&#39;</span><span class="p">,</span> <span class="s1">&#39;f17&#39;</span><span class="p">,</span> <span class="s1">&#39;f18&#39;</span><span class="p">,</span> <span class="s1">&#39;f19&#39;</span><span class="p">,</span> <span class="s1">&#39;f20&#39;</span><span class="p">]</span>

<span class="c1"># Prepare two samples</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>

<span class="c1"># Prepare model</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Run feature elimination</span>
<span class="n">shap_elimination</span> <span class="o">=</span> <span class="n">EarlyStoppingShapRFECV</span><span class="p">(</span>
    <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">shap_elimination</span><span class="o">.</span><span class="n">fit_compute</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Make plots</span>
<span class="n">performance_plot</span> <span class="o">=</span> <span class="n">shap_elimination</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="c1"># Get final feature set</span>
<span class="n">final_features_set</span> <span class="o">=</span> <span class="n">shap_elimination</span><span class="o">.</span><span class="n">get_reduced_features_set</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>
<img src="../img/earlystoppingshaprfecv.png" width="500" /></p>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.__init__">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;auc&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.__init__" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>This method initializes the class.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>clf</code></td>
        <td><code>binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV</code></td>
        <td><p>A model that will be optimized and trained at each round of features elimination. The model must
support early stopping of training, which is the case for XGBoost and LightGBM, for example. The
recommended model is <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">LGBMClassifier</a>,
because it by default handles the missing values and categorical variables. This parameter also supports
any hyperparameter search schema that is consistent with the sklearn API e.g.
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV">BayesSearchCV</a>.
Note that if a hyperparemeter search model is used, the hyperparameters are tuned without early
stopping. Early stopping is applied only during the Shapley values estimation for feature
elimination. We recommend simply passing the model without hyperparameter optimization, or using
ShapRFECV without early stopping.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>step</code></td>
        <td><code>int or float</code></td>
        <td><p>Number of lowest importance features removed each round. If it is an int, then each round such number of
features is discarded. If float, such percentage of remaining features (rounded down) is removed each
iteration. It is recommended to use float, since it is faster for a large number of features, and slows
down and becomes more precise towards less features. Note: the last round may remove fewer features in
order to reach min_features_to_select.
If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after
keeping those columns.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>min_features_to_select</code></td>
        <td><code>int</code></td>
        <td><p>Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By
default the process stops when one feature is left. If columns_to_keep is specified in the fit method,
it may overide this parameter to the maximum between length of columns_to_keep the two.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>cv</code></td>
        <td><code>int, cross-validation generator or an iterable</code></td>
        <td><p>Determines the cross-validation splitting strategy. Compatible with sklearn
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html">cv parameter</a>.
If None, then cv of 5 is used.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>scoring</code></td>
        <td><code>string or probatus.utils.Scorer</code></td>
        <td><p>Metric for which the model performance is calculated. It can be either a metric name  aligned with predefined
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html">classification scorers names in sklearn</a>.
Another option is using probatus.utils.Scorer to define a custom metric.</p></td>
        <td><code>&#39;roc_auc&#39;</code></td>
      </tr>
      <tr>
        <td><code>n_jobs</code></td>
        <td><code>int</code></td>
        <td><p>Number of cores to run in parallel while fitting across folds. None means 1 unless in a
<code>joblib.parallel_backend</code> context. -1 means using all processors.</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>verbose</code></td>
        <td><code>int</code></td>
        <td><p>Controls verbosity of the output:</p>
<ul>
<li>0 - nether prints nor warnings are shown</li>
<li>1 - 50 - only most important warnings</li>
<li>51 - 100 - shows other warnings and prints</li>
<li>above 100 - presents all prints and all warnings (including SHAP warnings).</li>
</ul></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>random_state</code></td>
        <td><code>int</code></td>
        <td><p>Random state set at each round of feature elimination. If it is None, the results will not be
reproducible and in random search at each iteration a different hyperparameters might be tested. For
reproducible results set it to integer.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>early_stopping_rounds</code></td>
        <td><code>int</code></td>
        <td><p>Number of rounds with constant performance after which the model fitting stops. This is passed to the
fit method of the model for Shapley values estimation, but not for hyperparameter search. Only
supported by some models, such as XGBoost and LightGBM.</p></td>
        <td><code>5</code></td>
      </tr>
      <tr>
        <td><code>eval_metric</code></td>
        <td><code>str</code></td>
        <td><p>Metric for scoring fitting rounds and activating early stopping. This is passed to the
fit method of the model for Shapley values estimation, but not for hyperparameter search. Only
supported by some models, such as <a href="https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters">XGBoost</a>
 and <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters">LightGBM</a>.
Note that <code>eval_metric</code> is an argument of the model's fit method and it is different from <code>scoring</code>.</p></td>
        <td><code>&#39;auc&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">min_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">eval_metric</span><span class="o">=</span><span class="s2">&quot;auc&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method initializes the class.</span>

<span class="sd">    Args:</span>
<span class="sd">        clf (binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV):</span>
<span class="sd">            A model that will be optimized and trained at each round of features elimination. The model must</span>
<span class="sd">            support early stopping of training, which is the case for XGBoost and LightGBM, for example. The</span>
<span class="sd">            recommended model is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html),</span>
<span class="sd">            because it by default handles the missing values and categorical variables. This parameter also supports</span>
<span class="sd">            any hyperparameter search schema that is consistent with the sklearn API e.g.</span>
<span class="sd">            [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">            [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">            or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV).</span>
<span class="sd">            Note that if a hyperparemeter search model is used, the hyperparameters are tuned without early</span>
<span class="sd">            stopping. Early stopping is applied only during the Shapley values estimation for feature</span>
<span class="sd">            elimination. We recommend simply passing the model without hyperparameter optimization, or using</span>
<span class="sd">            ShapRFECV without early stopping.</span>


<span class="sd">        step (int or float, optional):</span>
<span class="sd">            Number of lowest importance features removed each round. If it is an int, then each round such number of</span>
<span class="sd">            features is discarded. If float, such percentage of remaining features (rounded down) is removed each</span>
<span class="sd">            iteration. It is recommended to use float, since it is faster for a large number of features, and slows</span>
<span class="sd">            down and becomes more precise towards less features. Note: the last round may remove fewer features in</span>
<span class="sd">            order to reach min_features_to_select.</span>
<span class="sd">            If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after</span>
<span class="sd">            keeping those columns.</span>

<span class="sd">        min_features_to_select (int, optional):</span>
<span class="sd">            Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By</span>
<span class="sd">            default the process stops when one feature is left. If columns_to_keep is specified in the fit method,</span>
<span class="sd">            it may overide this parameter to the maximum between length of columns_to_keep the two.</span>

<span class="sd">        cv (int, cross-validation generator or an iterable, optional):</span>
<span class="sd">            Determines the cross-validation splitting strategy. Compatible with sklearn</span>
<span class="sd">            [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).</span>
<span class="sd">            If None, then cv of 5 is used.</span>

<span class="sd">        scoring (string or probatus.utils.Scorer, optional):</span>
<span class="sd">            Metric for which the model performance is calculated. It can be either a metric name  aligned with predefined</span>
<span class="sd">            [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html).</span>
<span class="sd">            Another option is using probatus.utils.Scorer to define a custom metric.</span>

<span class="sd">        n_jobs (int, optional):</span>
<span class="sd">            Number of cores to run in parallel while fitting across folds. None means 1 unless in a</span>
<span class="sd">            `joblib.parallel_backend` context. -1 means using all processors.</span>

<span class="sd">        verbose (int, optional):</span>
<span class="sd">            Controls verbosity of the output:</span>

<span class="sd">            - 0 - nether prints nor warnings are shown</span>
<span class="sd">            - 1 - 50 - only most important warnings</span>
<span class="sd">            - 51 - 100 - shows other warnings and prints</span>
<span class="sd">            - above 100 - presents all prints and all warnings (including SHAP warnings).</span>

<span class="sd">        random_state (int, optional):</span>
<span class="sd">            Random state set at each round of feature elimination. If it is None, the results will not be</span>
<span class="sd">            reproducible and in random search at each iteration a different hyperparameters might be tested. For</span>
<span class="sd">            reproducible results set it to integer.</span>

<span class="sd">        early_stopping_rounds (int, optional):</span>
<span class="sd">            Number of rounds with constant performance after which the model fitting stops. This is passed to the</span>
<span class="sd">            fit method of the model for Shapley values estimation, but not for hyperparameter search. Only</span>
<span class="sd">            supported by some models, such as XGBoost and LightGBM.</span>

<span class="sd">        eval_metric (str, optional):</span>
<span class="sd">            Metric for scoring fitting rounds and activating early stopping. This is passed to the</span>
<span class="sd">            fit method of the model for Shapley values estimation, but not for hyperparameter search. Only</span>
<span class="sd">            supported by some models, such as [XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters)</span>
<span class="sd">             and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters).</span>
<span class="sd">            Note that `eval_metric` is an argument of the model&#39;s fit method and it is different from `scoring`.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">EarlyStoppingShapRFECV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">clf</span><span class="p">,</span>
        <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>
        <span class="n">min_features_to_select</span><span class="o">=</span><span class="n">min_features_to_select</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
        <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_clf</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Early stopping will be used only during Shapley value estimation step, and not for hyperparameter&quot;</span>
                <span class="s2">&quot;optimization.&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">early_stopping_rounds</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">early_stopping_rounds</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span> <span class="o">=</span> <span class="n">early_stopping_rounds</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="p">(</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The current value of early_stopping_rounds = </span><span class="si">{</span><span class="n">early_stopping_rounds</span><span class="si">}</span><span class="s2"> is not allowed. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;It needs to be a positive integer.&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">eval_metric</span> <span class="o">=</span> <span class="n">eval_metric</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.compute">
<code class="highlight language-python"><span class="n">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.compute" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Checks if fit() method has been run.</p>
<p>and computes the DataFrame with results of feature elimintation for each round.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(pd.DataFrame)</code></td>
      <td><p>DataFrame with results of feature elimination for each round.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if fit() method has been run.</span>

<span class="sd">    and computes the DataFrame with results of feature elimintation for each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (pd.DataFrame):</span>
<span class="sd">            DataFrame with results of feature elimination for each round.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit">
<code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Fits the object with the provided data.</p>
<p>The algorithm starts with the entire dataset, and then sequentially
     eliminates features. If sklearn compatible search CV is passed as clf e.g.
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
     or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,
     the hyperparameter optimization is applied at each step of the elimination.
     Then, the SHAP feature importance is calculated using Cross-Validation,
     and <code>step</code> lowest importance features are removed.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>X</code></td>
        <td><code>pd.DataFrame</code></td>
        <td><p>Provided dataset.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>y</code></td>
        <td><code>pd.Series</code></td>
        <td><p>Binary labels for X.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>columns_to_keep</code></td>
        <td><code>list of str</code></td>
        <td><p>List of column names to keep. If given,
these columns will not be eliminated by the feature elimination process.
However, these feature will used for the calculation of the SHAP values.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>column_names</code></td>
        <td><code>list of str</code></td>
        <td><p>List of feature names of the provided samples. If provided it will be used to overwrite the existing
feature names. If not provided the existing feature names are used or default feature names are
generated.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**shap_kwargs</code></td>
        <td><code></code></td>
        <td><p>keyword arguments passed to
<a href="https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer">shap.Explainer</a>.
It also enables <code>approximate</code> and <code>check_additivity</code> parameters, passed while calculating SHAP values.
The <code>approximate=True</code> causes less accurate, but faster SHAP values calculation, while
<code>check_additivity=False</code> disables the additivity check inside SHAP.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(ShapRFECV)</code></td>
      <td><p>Fitted object.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fits the object with the provided data.</span>

<span class="sd">    The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">         eliminates features. If sklearn compatible search CV is passed as clf e.g.</span>
<span class="sd">         [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">         [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">         or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">         the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">         Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">         and `step` lowest importance features are removed.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (pd.DataFrame):</span>
<span class="sd">            Provided dataset.</span>

<span class="sd">        y (pd.Series):</span>
<span class="sd">            Binary labels for X.</span>

<span class="sd">        columns_to_keep (list of str, optional):</span>
<span class="sd">            List of column names to keep. If given,</span>
<span class="sd">            these columns will not be eliminated by the feature elimination process.</span>
<span class="sd">            However, these feature will used for the calculation of the SHAP values.</span>

<span class="sd">        column_names (list of str, optional):</span>
<span class="sd">            List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">            feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">            generated.</span>

<span class="sd">        **shap_kwargs:</span>
<span class="sd">            keyword arguments passed to</span>
<span class="sd">            [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">            It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">            The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">            `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (ShapRFECV): Fitted object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set seed for results reproducibility</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># If to columns_to_keep is not provided, then initialise it by an empty string.</span>
    <span class="c1"># If provided check if all the elements in columns_to_keep are of type string.</span>
    <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">columns_to_keep</span><span class="p">):</span>
            <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns_to_keep</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The current values of columns_to_keep are not allowed.All the elements should be strings.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="c1"># If the columns_to_keep parameter is provided, check if they match the column names in the X.</span>
    <span class="k">if</span> <span class="n">column_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="ow">in</span> <span class="n">column_names</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The column names in parameter columns_to_keep and column_names are not macthing.&quot;</span><span class="p">))</span>

    <span class="c1"># Check that the total number of columns to select is less than total number of columns in the data.</span>
    <span class="c1"># only when both parameters are provided.</span>
    <span class="k">if</span> <span class="n">column_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">+</span> <span class="n">len_columns_to_keep</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">column_names</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Minimum features to select is greater than number of features.&quot;</span>
                <span class="s2">&quot;Lower the value for min_features_to_select or number of columns in columns_to_keep&quot;</span>
            <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">preprocess_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">))</span>

    <span class="n">remaining_features</span> <span class="o">=</span> <span class="n">current_features_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span>
    <span class="n">round_number</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Stop when stopping criteria is met.</span>
    <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span><span class="p">,</span> <span class="n">len_columns_to_keep</span><span class="p">])</span>

    <span class="c1"># Setting up the min_features_to_select parameter.</span>
    <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># This ensures that, if columns_to_keep is provided ,</span>
        <span class="c1"># the last features remaining are only the columns_to_keep.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum features to select : </span><span class="si">{</span><span class="n">stopping_criteria</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">stopping_criteria</span><span class="p">:</span>
        <span class="n">round_number</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Get current dataset info</span>
        <span class="n">current_features_set</span> <span class="o">=</span> <span class="n">remaining_features</span>
        <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">remaining_removeable_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">remaining_removeable_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">columns_to_keep</span><span class="p">))</span>
        <span class="n">current_X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">remaining_removeable_features</span><span class="p">]</span>

        <span class="c1"># Set seed for results reproducibility</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># Optimize parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_clf</span><span class="p">:</span>
            <span class="n">current_search_clf</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
            <span class="n">current_clf</span> <span class="o">=</span> <span class="n">current_search_clf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">current_search_clf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_clf</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">)</span>

        <span class="c1"># Perform CV to estimate feature importance with SHAP</span>
        <span class="n">results_per_fold</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_shap_values_per_fold</span><span class="p">)(</span>
                <span class="n">X</span><span class="o">=</span><span class="n">current_X</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                <span class="n">clf</span><span class="o">=</span><span class="n">current_clf</span><span class="p">,</span>
                <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">current_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">])</span>
        <span class="n">scores_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>
        <span class="n">scores_val</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>

        <span class="c1"># Calculate the shap features with remaining features and features to keep.</span>

        <span class="n">shap_importance_df</span> <span class="o">=</span> <span class="n">calculate_shap_importance</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">remaining_removeable_features</span><span class="p">)</span>

        <span class="c1"># Get features to remove</span>
        <span class="n">features_to_remove</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_current_features_to_remove</span><span class="p">(</span>
            <span class="n">shap_importance_df</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="n">columns_to_keep</span>
        <span class="p">)</span>
        <span class="n">remaining_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">features_to_remove</span><span class="p">))</span>

        <span class="c1"># Report results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_report_current_results</span><span class="p">(</span>
            <span class="n">round_number</span><span class="o">=</span><span class="n">round_number</span><span class="p">,</span>
            <span class="n">current_features_set</span><span class="o">=</span><span class="n">current_features_set</span><span class="p">,</span>
            <span class="n">features_to_remove</span><span class="o">=</span><span class="n">features_to_remove</span><span class="p">,</span>
            <span class="n">train_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
            <span class="n">train_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
            <span class="n">val_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
            <span class="n">val_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Round: </span><span class="si">{</span><span class="n">round_number</span><span class="si">}</span><span class="s2">, Current number of features: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s1">&#39;Current performance: Train </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">, CV Validation &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">. </span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="sa">f</span><span class="s2">&quot;Features left: </span><span class="si">{</span><span class="n">remaining_features</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Removed features at the end of the round: </span><span class="si">{</span><span class="n">features_to_remove</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fitted</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit_compute">
<code class="highlight language-python"><span class="n">fit_compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.fit_compute" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Fits the object with the provided data.</p>
<p>The algorithm starts with the entire dataset, and then sequentially
     eliminates features. If sklearn compatible search CV is passed as clf e.g.
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
     or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,
     the hyperparameter optimization is applied at each step of the elimination.
     Then, the SHAP feature importance is calculated using Cross-Validation,
     and <code>step</code> lowest importance features are removed. At the end, the
     report containing results from each iteration is computed and returned to the user.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>X</code></td>
        <td><code>pd.DataFrame</code></td>
        <td><p>Provided dataset.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>y</code></td>
        <td><code>pd.Series</code></td>
        <td><p>Binary labels for X.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>columns_to_keep</code></td>
        <td><code>list of str</code></td>
        <td><p>List of columns to keep. If given, these columns will not be eliminated.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>column_names</code></td>
        <td><code>list of str</code></td>
        <td><p>List of feature names of the provided samples. If provided it will be used to overwrite the existing
feature names. If not provided the existing feature names are used or default feature names are
generated.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**shap_kwargs</code></td>
        <td><code></code></td>
        <td><p>keyword arguments passed to
<a href="https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer">shap.Explainer</a>.
It also enables <code>approximate</code> and <code>check_additivity</code> parameters, passed while calculating SHAP values.
The <code>approximate=True</code> causes less accurate, but faster SHAP values calculation, while
<code>check_additivity=False</code> disables the additivity check inside SHAP.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(pd.DataFrame)</code></td>
      <td><p>DataFrame containing results of feature elimination from each iteration.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fit_compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fits the object with the provided data.</span>

<span class="sd">    The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">         eliminates features. If sklearn compatible search CV is passed as clf e.g.</span>
<span class="sd">         [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">         [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">         or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">         the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">         Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">         and `step` lowest importance features are removed. At the end, the</span>
<span class="sd">         report containing results from each iteration is computed and returned to the user.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (pd.DataFrame):</span>
<span class="sd">            Provided dataset.</span>

<span class="sd">        y (pd.Series):</span>
<span class="sd">            Binary labels for X.</span>

<span class="sd">        columns_to_keep (list of str, optional):</span>
<span class="sd">            List of columns to keep. If given, these columns will not be eliminated.</span>

<span class="sd">        column_names (list of str, optional):</span>
<span class="sd">            List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">            feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">            generated.</span>

<span class="sd">        **shap_kwargs:</span>
<span class="sd">            keyword arguments passed to</span>
<span class="sd">            [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">            It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">            The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">            `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (pd.DataFrame):</span>
<span class="sd">            DataFrame containing results of feature elimination from each iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="n">columns_to_keep</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.get_reduced_features_set">
<code class="highlight language-python"><span class="n">get_reduced_features_set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.get_reduced_features_set" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Gets the features set after the feature elimination process, for a given number of features.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_features</code></td>
        <td><code>int</code></td>
        <td><p>Number of features in the reduced features set.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(list of str)</code></td>
      <td><p>Reduced features set.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_reduced_features_set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the features set after the feature elimination process, for a given number of features.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_features (int):</span>
<span class="sd">            Number of features in the reduced features set.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (list of str):</span>
<span class="sd">            Reduced features set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">num_features</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
        <span class="k">raise</span> <span class="p">(</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The provided number of features has not been achieved at any stage of the process. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;You can select one of the following: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span> <span class="o">==</span> <span class="n">num_features</span><span class="p">][</span><span class="s2">&quot;features_set&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.plot">
<code class="highlight language-python"><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.EarlyStoppingShapRFECV.plot" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Generates plot of the model performance for each iteration of feature elimination.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>show</code></td>
        <td><code>bool</code></td>
        <td><p>If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful,
when you want to edit the returned axis, before showing it.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>**figure_kwargs</code></td>
        <td><code></code></td>
        <td><p>Keyword arguments that are passed to the plt.figure, at its initialization.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(plt.axis)</code></td>
      <td><p>Axis containing the performance plot.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates plot of the model performance for each iteration of feature elimination.</span>

<span class="sd">    Args:</span>
<span class="sd">        show (bool, optional):</span>
<span class="sd">            If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful,</span>
<span class="sd">            when you want to edit the returned axis, before showing it.</span>

<span class="sd">        **figure_kwargs:</span>
<span class="sd">            Keyword arguments that are passed to the plt.figure, at its initialization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (plt.axis):</span>
<span class="sd">            Axis containing the performance plot.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_ticks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train Score&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;coerce&quot;</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Score&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;coerce&quot;</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of features&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Performance </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Backwards Feature Elimination using SHAP &amp; CV&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">invert_xaxis</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ax</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.ShapRFECV">
        <code>ShapRFECV</code>



<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.ShapRFECV" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>This class performs Backwards Recursive Feature Elimination, using SHAP feature importance.</p>
<p>At each round, for a
    given feature set, starting from all available features, the following steps are applied:</p>
<ol>
<li>(Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g.
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html">GridSearchCV</a>,
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV">RandomizedSearchCV</a>, or
    <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,</li>
<li>Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV
    iteration, the model is fitted on the train folds, and applied on the validation fold to estimate
    SHAP feature importance.</li>
<li>Remove <code>step</code> lowest SHAP importance features from the dataset.</li>
</ol>
<p>At the end of the process, the user can plot the performance of the model for each iteration, and select the
    optimal number of features and the features set.</p>
<p>The functionality is
    similar to <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html">RFECV</a>.
    The main difference is removing the lowest importance features based on SHAP features importance. It also
    supports the use of sklearn compatible search CV for hyperparameter optimization e.g.
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html">GridSearchCV</a>,
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV">RandomizedSearchCV</a>, or
    <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>, which
    needs to be passed as the <code>clf</code>. Thanks to this you can perform hyperparameter optimization at each step of
    the feature elimination. Lastly, it supports categorical features (object and category dtype) and missing values
    in the data, as long as the model supports them.</p>
<p>We recommend using <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">LGBMClassifier</a>,
    because by default it handles missing values and categorical features. In case of other models, make sure to
    handle these issues for your dataset and consider impact it might have on features importance.</p>

<p><strong>Examples:</strong></p>
    
      <p><div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">probatus.feature_elimination</span> <span class="kn">import</span> <span class="n">ShapRFECV</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="s1">&#39;f2&#39;</span><span class="p">,</span> <span class="s1">&#39;f3&#39;</span><span class="p">,</span> <span class="s1">&#39;f4&#39;</span><span class="p">,</span> <span class="s1">&#39;f5&#39;</span><span class="p">,</span> <span class="s1">&#39;f6&#39;</span><span class="p">,</span> <span class="s1">&#39;f7&#39;</span><span class="p">,</span>
    <span class="s1">&#39;f8&#39;</span><span class="p">,</span> <span class="s1">&#39;f9&#39;</span><span class="p">,</span> <span class="s1">&#39;f10&#39;</span><span class="p">,</span> <span class="s1">&#39;f11&#39;</span><span class="p">,</span> <span class="s1">&#39;f12&#39;</span><span class="p">,</span> <span class="s1">&#39;f13&#39;</span><span class="p">,</span>
    <span class="s1">&#39;f14&#39;</span><span class="p">,</span> <span class="s1">&#39;f15&#39;</span><span class="p">,</span> <span class="s1">&#39;f16&#39;</span><span class="p">,</span> <span class="s1">&#39;f17&#39;</span><span class="p">,</span> <span class="s1">&#39;f18&#39;</span><span class="p">,</span> <span class="s1">&#39;f19&#39;</span><span class="p">,</span> <span class="s1">&#39;f20&#39;</span><span class="p">]</span>

<span class="c1"># Prepare two samples</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>


<span class="c1"># Prepare model and parameter search space</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">)</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">)</span>


<span class="c1"># Run feature elimination</span>
<span class="n">shap_elimination</span> <span class="o">=</span> <span class="n">ShapRFECV</span><span class="p">(</span>
    <span class="n">clf</span><span class="o">=</span><span class="n">search</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">shap_elimination</span><span class="o">.</span><span class="n">fit_compute</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Make plots</span>
<span class="n">performance_plot</span> <span class="o">=</span> <span class="n">shap_elimination</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="c1"># Get final feature set</span>
<span class="n">final_features_set</span> <span class="o">=</span> <span class="n">shap_elimination</span><span class="o">.</span><span class="n">get_reduced_features_set</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>
<img src="../img/shaprfecv.png" width="500" /></p>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.ShapRFECV.__init__">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.ShapRFECV.__init__" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>This method initializes the class.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>clf</code></td>
        <td><code>binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV</code></td>
        <td><p>A model that will be optimized and trained at each round of feature elimination. The recommended model
is <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">LGBMClassifier</a>,
because it by default handles the missing values and categorical variables. This parameter also supports
any hyperparameter search schema that is consistent with the sklearn API e.g.
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV">BayesSearchCV</a>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>step</code></td>
        <td><code>int or float</code></td>
        <td><p>Number of lowest importance features removed each round. If it is an int, then each round such a number of
features are discarded. If float, such a percentage of remaining features (rounded down) is removed each
iteration. It is recommended to use float, since it is faster for a large number of features, and slows
down and becomes more precise with fewer features. Note: the last round may remove fewer features in
order to reach min_features_to_select.
If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after
keeping those columns.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>min_features_to_select</code></td>
        <td><code>int</code></td>
        <td><p>Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By
default the process stops when one feature is left. If columns_to_keep is specified in the fit method,
it may overide this parameter to the maximum between length of columns_to_keep the two.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>cv</code></td>
        <td><code>int, cross-validation generator or an iterable</code></td>
        <td><p>Determines the cross-validation splitting strategy. Compatible with sklearn
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html">cv parameter</a>.
If None, then cv of 5 is used.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>scoring</code></td>
        <td><code>string or probatus.utils.Scorer</code></td>
        <td><p>Metric for which the model performance is calculated. It can be either a metric name aligned with predefined
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html">classification scorers names in sklearn</a>.
Another option is using probatus.utils.Scorer to define a custom metric.</p></td>
        <td><code>&#39;roc_auc&#39;</code></td>
      </tr>
      <tr>
        <td><code>n_jobs</code></td>
        <td><code>int</code></td>
        <td><p>Number of cores to run in parallel while fitting across folds. None means 1 unless in a
<code>joblib.parallel_backend</code> context. -1 means using all processors.</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>verbose</code></td>
        <td><code>int</code></td>
        <td><p>Controls verbosity of the output:</p>
<ul>
<li>0 - neither prints nor warnings are shown</li>
<li>1 - 50 - only most important warnings</li>
<li>51 - 100 - shows other warnings and prints</li>
<li>above 100 - presents all prints and all warnings (including SHAP warnings).</li>
</ul></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>random_state</code></td>
        <td><code>int</code></td>
        <td><p>Random state set at each round of feature elimination. If it is None, the results will not be
reproducible and in random search at each iteration a different hyperparameters might be tested. For
reproducible results set it to an integer.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">min_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method initializes the class.</span>

<span class="sd">    Args:</span>
<span class="sd">        clf (binary classifier, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV):</span>
<span class="sd">            A model that will be optimized and trained at each round of feature elimination. The recommended model</span>
<span class="sd">            is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html),</span>
<span class="sd">            because it by default handles the missing values and categorical variables. This parameter also supports</span>
<span class="sd">            any hyperparameter search schema that is consistent with the sklearn API e.g.</span>
<span class="sd">            [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">            [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">            or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV).</span>

<span class="sd">        step (int or float, optional):</span>
<span class="sd">            Number of lowest importance features removed each round. If it is an int, then each round such a number of</span>
<span class="sd">            features are discarded. If float, such a percentage of remaining features (rounded down) is removed each</span>
<span class="sd">            iteration. It is recommended to use float, since it is faster for a large number of features, and slows</span>
<span class="sd">            down and becomes more precise with fewer features. Note: the last round may remove fewer features in</span>
<span class="sd">            order to reach min_features_to_select.</span>
<span class="sd">            If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after</span>
<span class="sd">            keeping those columns.</span>

<span class="sd">        min_features_to_select (int, optional):</span>
<span class="sd">            Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By</span>
<span class="sd">            default the process stops when one feature is left. If columns_to_keep is specified in the fit method,</span>
<span class="sd">            it may overide this parameter to the maximum between length of columns_to_keep the two.</span>

<span class="sd">        cv (int, cross-validation generator or an iterable, optional):</span>
<span class="sd">            Determines the cross-validation splitting strategy. Compatible with sklearn</span>
<span class="sd">            [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).</span>
<span class="sd">            If None, then cv of 5 is used.</span>

<span class="sd">        scoring (string or probatus.utils.Scorer, optional):</span>
<span class="sd">            Metric for which the model performance is calculated. It can be either a metric name aligned with predefined</span>
<span class="sd">            [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html).</span>
<span class="sd">            Another option is using probatus.utils.Scorer to define a custom metric.</span>

<span class="sd">        n_jobs (int, optional):</span>
<span class="sd">            Number of cores to run in parallel while fitting across folds. None means 1 unless in a</span>
<span class="sd">            `joblib.parallel_backend` context. -1 means using all processors.</span>

<span class="sd">        verbose (int, optional):</span>
<span class="sd">            Controls verbosity of the output:</span>

<span class="sd">            - 0 - neither prints nor warnings are shown</span>
<span class="sd">            - 1 - 50 - only most important warnings</span>
<span class="sd">            - 51 - 100 - shows other warnings and prints</span>
<span class="sd">            - above 100 - presents all prints and all warnings (including SHAP warnings).</span>

<span class="sd">        random_state (int, optional):</span>
<span class="sd">            Random state set at each round of feature elimination. If it is None, the results will not be</span>
<span class="sd">            reproducible and in random search at each iteration a different hyperparameters might be tested. For</span>
<span class="sd">            reproducible results set it to an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">,</span> <span class="n">BaseSearchCV</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">search_clf</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">search_clf</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="n">step</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="p">(</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The current value of step = </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> is not allowed. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;It needs to be a positive integer or positive float.&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_features_to_select</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">min_features_to_select</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">=</span> <span class="n">min_features_to_select</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="p">(</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The current value of min_features_to_select = </span><span class="si">{</span><span class="n">min_features_to_select</span><span class="si">}</span><span class="s2"> is not allowed. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;It needs to be a greater than or equal to 0.&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span> <span class="o">=</span> <span class="n">get_single_scorer</span><span class="p">(</span><span class="n">scoring</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.ShapRFECV.compute">
<code class="highlight language-python"><span class="n">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.ShapRFECV.compute" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Checks if fit() method has been run.</p>
<p>and computes the DataFrame with results of feature elimintation for each round.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(pd.DataFrame)</code></td>
      <td><p>DataFrame with results of feature elimination for each round.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if fit() method has been run.</span>

<span class="sd">    and computes the DataFrame with results of feature elimintation for each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (pd.DataFrame):</span>
<span class="sd">            DataFrame with results of feature elimination for each round.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.ShapRFECV.fit">
<code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Fits the object with the provided data.</p>
<p>The algorithm starts with the entire dataset, and then sequentially
     eliminates features. If sklearn compatible search CV is passed as clf e.g.
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
     or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,
     the hyperparameter optimization is applied at each step of the elimination.
     Then, the SHAP feature importance is calculated using Cross-Validation,
     and <code>step</code> lowest importance features are removed.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>X</code></td>
        <td><code>pd.DataFrame</code></td>
        <td><p>Provided dataset.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>y</code></td>
        <td><code>pd.Series</code></td>
        <td><p>Binary labels for X.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>columns_to_keep</code></td>
        <td><code>list of str</code></td>
        <td><p>List of column names to keep. If given,
these columns will not be eliminated by the feature elimination process.
However, these feature will used for the calculation of the SHAP values.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>column_names</code></td>
        <td><code>list of str</code></td>
        <td><p>List of feature names of the provided samples. If provided it will be used to overwrite the existing
feature names. If not provided the existing feature names are used or default feature names are
generated.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**shap_kwargs</code></td>
        <td><code></code></td>
        <td><p>keyword arguments passed to
<a href="https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer">shap.Explainer</a>.
It also enables <code>approximate</code> and <code>check_additivity</code> parameters, passed while calculating SHAP values.
The <code>approximate=True</code> causes less accurate, but faster SHAP values calculation, while
<code>check_additivity=False</code> disables the additivity check inside SHAP.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(ShapRFECV)</code></td>
      <td><p>Fitted object.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fits the object with the provided data.</span>

<span class="sd">    The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">         eliminates features. If sklearn compatible search CV is passed as clf e.g.</span>
<span class="sd">         [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">         [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">         or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">         the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">         Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">         and `step` lowest importance features are removed.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (pd.DataFrame):</span>
<span class="sd">            Provided dataset.</span>

<span class="sd">        y (pd.Series):</span>
<span class="sd">            Binary labels for X.</span>

<span class="sd">        columns_to_keep (list of str, optional):</span>
<span class="sd">            List of column names to keep. If given,</span>
<span class="sd">            these columns will not be eliminated by the feature elimination process.</span>
<span class="sd">            However, these feature will used for the calculation of the SHAP values.</span>

<span class="sd">        column_names (list of str, optional):</span>
<span class="sd">            List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">            feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">            generated.</span>

<span class="sd">        **shap_kwargs:</span>
<span class="sd">            keyword arguments passed to</span>
<span class="sd">            [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">            It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">            The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">            `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (ShapRFECV): Fitted object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set seed for results reproducibility</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># If to columns_to_keep is not provided, then initialise it by an empty string.</span>
    <span class="c1"># If provided check if all the elements in columns_to_keep are of type string.</span>
    <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">columns_to_keep</span><span class="p">):</span>
            <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns_to_keep</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The current values of columns_to_keep are not allowed.All the elements should be strings.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="c1"># If the columns_to_keep parameter is provided, check if they match the column names in the X.</span>
    <span class="k">if</span> <span class="n">column_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="ow">in</span> <span class="n">column_names</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The column names in parameter columns_to_keep and column_names are not macthing.&quot;</span><span class="p">))</span>

    <span class="c1"># Check that the total number of columns to select is less than total number of columns in the data.</span>
    <span class="c1"># only when both parameters are provided.</span>
    <span class="k">if</span> <span class="n">column_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">+</span> <span class="n">len_columns_to_keep</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">column_names</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Minimum features to select is greater than number of features.&quot;</span>
                <span class="s2">&quot;Lower the value for min_features_to_select or number of columns in columns_to_keep&quot;</span>
            <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">preprocess_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">))</span>

    <span class="n">remaining_features</span> <span class="o">=</span> <span class="n">current_features_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span>
    <span class="n">round_number</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Stop when stopping criteria is met.</span>
    <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span><span class="p">,</span> <span class="n">len_columns_to_keep</span><span class="p">])</span>

    <span class="c1"># Setting up the min_features_to_select parameter.</span>
    <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># This ensures that, if columns_to_keep is provided ,</span>
        <span class="c1"># the last features remaining are only the columns_to_keep.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum features to select : </span><span class="si">{</span><span class="n">stopping_criteria</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">stopping_criteria</span><span class="p">:</span>
        <span class="n">round_number</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Get current dataset info</span>
        <span class="n">current_features_set</span> <span class="o">=</span> <span class="n">remaining_features</span>
        <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">remaining_removeable_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">remaining_removeable_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">columns_to_keep</span><span class="p">))</span>
        <span class="n">current_X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">remaining_removeable_features</span><span class="p">]</span>

        <span class="c1"># Set seed for results reproducibility</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># Optimize parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_clf</span><span class="p">:</span>
            <span class="n">current_search_clf</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
            <span class="n">current_clf</span> <span class="o">=</span> <span class="n">current_search_clf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">current_search_clf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_clf</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">)</span>

        <span class="c1"># Perform CV to estimate feature importance with SHAP</span>
        <span class="n">results_per_fold</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_shap_values_per_fold</span><span class="p">)(</span>
                <span class="n">X</span><span class="o">=</span><span class="n">current_X</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                <span class="n">clf</span><span class="o">=</span><span class="n">current_clf</span><span class="p">,</span>
                <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">current_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">])</span>
        <span class="n">scores_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>
        <span class="n">scores_val</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>

        <span class="c1"># Calculate the shap features with remaining features and features to keep.</span>

        <span class="n">shap_importance_df</span> <span class="o">=</span> <span class="n">calculate_shap_importance</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">remaining_removeable_features</span><span class="p">)</span>

        <span class="c1"># Get features to remove</span>
        <span class="n">features_to_remove</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_current_features_to_remove</span><span class="p">(</span>
            <span class="n">shap_importance_df</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="n">columns_to_keep</span>
        <span class="p">)</span>
        <span class="n">remaining_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">features_to_remove</span><span class="p">))</span>

        <span class="c1"># Report results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_report_current_results</span><span class="p">(</span>
            <span class="n">round_number</span><span class="o">=</span><span class="n">round_number</span><span class="p">,</span>
            <span class="n">current_features_set</span><span class="o">=</span><span class="n">current_features_set</span><span class="p">,</span>
            <span class="n">features_to_remove</span><span class="o">=</span><span class="n">features_to_remove</span><span class="p">,</span>
            <span class="n">train_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
            <span class="n">train_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
            <span class="n">val_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
            <span class="n">val_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Round: </span><span class="si">{</span><span class="n">round_number</span><span class="si">}</span><span class="s2">, Current number of features: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s1">&#39;Current performance: Train </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">, CV Validation &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">. </span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="sa">f</span><span class="s2">&quot;Features left: </span><span class="si">{</span><span class="n">remaining_features</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Removed features at the end of the round: </span><span class="si">{</span><span class="n">features_to_remove</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fitted</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute">
<code class="highlight language-python"><span class="n">fit_compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Fits the object with the provided data.</p>
<p>The algorithm starts with the entire dataset, and then sequentially
     eliminates features. If sklearn compatible search CV is passed as clf e.g.
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
     or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,
     the hyperparameter optimization is applied at each step of the elimination.
     Then, the SHAP feature importance is calculated using Cross-Validation,
     and <code>step</code> lowest importance features are removed. At the end, the
     report containing results from each iteration is computed and returned to the user.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>X</code></td>
        <td><code>pd.DataFrame</code></td>
        <td><p>Provided dataset.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>y</code></td>
        <td><code>pd.Series</code></td>
        <td><p>Binary labels for X.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>columns_to_keep</code></td>
        <td><code>list of str</code></td>
        <td><p>List of columns to keep. If given, these columns will not be eliminated.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>column_names</code></td>
        <td><code>list of str</code></td>
        <td><p>List of feature names of the provided samples. If provided it will be used to overwrite the existing
feature names. If not provided the existing feature names are used or default feature names are
generated.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**shap_kwargs</code></td>
        <td><code></code></td>
        <td><p>keyword arguments passed to
<a href="https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer">shap.Explainer</a>.
It also enables <code>approximate</code> and <code>check_additivity</code> parameters, passed while calculating SHAP values.
The <code>approximate=True</code> causes less accurate, but faster SHAP values calculation, while
<code>check_additivity=False</code> disables the additivity check inside SHAP.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(pd.DataFrame)</code></td>
      <td><p>DataFrame containing results of feature elimination from each iteration.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fit_compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fits the object with the provided data.</span>

<span class="sd">    The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">         eliminates features. If sklearn compatible search CV is passed as clf e.g.</span>
<span class="sd">         [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">         [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">         or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">         the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">         Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">         and `step` lowest importance features are removed. At the end, the</span>
<span class="sd">         report containing results from each iteration is computed and returned to the user.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (pd.DataFrame):</span>
<span class="sd">            Provided dataset.</span>

<span class="sd">        y (pd.Series):</span>
<span class="sd">            Binary labels for X.</span>

<span class="sd">        columns_to_keep (list of str, optional):</span>
<span class="sd">            List of columns to keep. If given, these columns will not be eliminated.</span>

<span class="sd">        column_names (list of str, optional):</span>
<span class="sd">            List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">            feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">            generated.</span>

<span class="sd">        **shap_kwargs:</span>
<span class="sd">            keyword arguments passed to</span>
<span class="sd">            [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">            It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">            The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">            `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (pd.DataFrame):</span>
<span class="sd">            DataFrame containing results of feature elimination from each iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="n">columns_to_keep</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set">
<code class="highlight language-python"><span class="n">get_reduced_features_set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">)</span></code>


<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Gets the features set after the feature elimination process, for a given number of features.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_features</code></td>
        <td><code>int</code></td>
        <td><p>Number of features in the reduced features set.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(list of str)</code></td>
      <td><p>Reduced features set.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_reduced_features_set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the features set after the feature elimination process, for a given number of features.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_features (int):</span>
<span class="sd">            Number of features in the reduced features set.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (list of str):</span>
<span class="sd">            Reduced features set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">num_features</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
        <span class="k">raise</span> <span class="p">(</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The provided number of features has not been achieved at any stage of the process. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;You can select one of the following: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span> <span class="o">==</span> <span class="n">num_features</span><span class="p">][</span><span class="s2">&quot;features_set&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 class="doc doc-heading" id="probatus.feature_elimination.feature_elimination.ShapRFECV.plot">
<code class="highlight language-python"><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#probatus.feature_elimination.feature_elimination.ShapRFECV.plot" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Generates plot of the model performance for each iteration of feature elimination.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>show</code></td>
        <td><code>bool</code></td>
        <td><p>If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful,
when you want to edit the returned axis, before showing it.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>**figure_kwargs</code></td>
        <td><code></code></td>
        <td><p>Keyword arguments that are passed to the plt.figure, at its initialization.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(plt.axis)</code></td>
      <td><p>Axis containing the performance plot.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates plot of the model performance for each iteration of feature elimination.</span>

<span class="sd">    Args:</span>
<span class="sd">        show (bool, optional):</span>
<span class="sd">            If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful,</span>
<span class="sd">            when you want to edit the returned axis, before showing it.</span>

<span class="sd">        **figure_kwargs:</span>
<span class="sd">            Keyword arguments that are passed to the plt.figure, at its initialization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (plt.axis):</span>
<span class="sd">            Axis containing the performance plot.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_ticks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train Score&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;coerce&quot;</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Score&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;coerce&quot;</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of features&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Performance </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Backwards Feature Elimination using SHAP &amp; CV&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">invert_xaxis</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ax</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../howto/reproducibility.html" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Reproducibility of the results
            </div>
          </div>
        </a>
      
      
        <a href="model_interpret.html" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              probatus.interpret
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2020 ING Bank N.V.
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fe42c31b.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.7353b375.min.js"></script>
      
    
  </body>
</html>